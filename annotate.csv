,qid,docid,docno,query,authors,title,abstract,label
0,12,191618,2204.01848,COVID-19 and social media,"Ekagra Ranjan, Naman Poddar",Multilingual Abusiveness Identification on Code-Mixed Social Media Text,"  Social Media platforms have been seeing adoption and growth in their usage
over time. This growth has been further accelerated with the lockdown in the
past year when people's interaction, conversation, and expression were limited
physically. It is becoming increasingly important to keep the platform safe
from abusive content for better user experience. Much work has been done on
English social media content but text analysis on non-English social media is
relatively underexplored. Non-English social media content have the additional
challenges of code-mixing, transliteration and using different scripture in
same sentence. In this work, we propose an approach for abusiveness
identification on the multilingual Moj dataset which comprises of Indic
languages. Our approach tackles the common challenges of non-English social
media content and can be extended to other languages as well.
",1.0
1,18,57973,1709.07952,infomation retrieval time complexity,Julien Lavauzelle,Private Information Retrieval from Transversal Designs,"  Private information retrieval (PIR) protocols allow a user to retrieve
entries of a database without revealing the index of the desired item.
Information-theoretical privacy can be achieved by the use of several servers
and specific retrieval algorithms. Most of known PIR protocols focus on
decreasing the number of bits exchanged between the client and the server(s)
during the retrieval process. On another side, Fazeli et. al. introduced
so-called PIR codes in order to reduce the storage overhead on the servers.
However, only a few works address the issue of the computation complexity of
the servers.
  In this paper, we show that a specific encoding of the database provides PIR
protocols with reasonable communication complexity, low storage overhead and
optimal computational complexity for the servers. This encoding is based on
incidence matrices of transversal designs, from which a natural and efficient
recovering algorithm is derived. We also present instances of our construction,
making use of finite geometries and orthogonal arrays, and we finally give a
generalisation of our main construction for resisting collusions of servers.
",3.0
2,10,87914,1905.03836,web archive,"Mohamed Aturban, Michael L. Nelson, Michele C. Weigle, Martin Klein
  and Herbert Van de Sompel",Collecting 16K archived web pages from 17 public web archives,"  We document the creation of a data set of 16,627 archived web pages, or
mementos, of 3,698 unique live web URIs (Uniform Resource Identifiers) from 17
public web archives. We used four different methods to collect the dataset.
First, we used the Los Alamos National Laboratory (LANL) Memento Aggregator to
collect mementos of an initial set of URIs obtained from four sources: (a) the
Moz Top 500, (b) the dataset used in our previous study, (c) the HTTP Archive,
and (d) the Web Archives for Historical Research group. Second, we extracted
URIs from the HTML of already collected mementos. These URIs were then used to
look up mementos in LANL's aggregator. Third, we downloaded web archives'
published lists of URIs of both original pages and their associated mementos.
Fourth, we collected more mementos from archives that support the Memento
protocol by requesting TimeMaps directly from archives, not through the Memento
aggregator. Finally, we downsampled the collected mementos to 16,627 due to our
constraints of a maximum of 1,600 mementos per archive and being able to
download all mementos from each archive in less than 40 hours.
",5.0
3,18,27576,1407.1065,infomation retrieval time complexity,"Emmanuel Candes, Xiaodong Li, Mahdi Soltanolkotabi",Phase Retrieval via Wirtinger Flow: Theory and Algorithms,"  We study the problem of recovering the phase from magnitude measurements;
specifically, we wish to reconstruct a complex-valued signal x of C^n about
which we have phaseless samples of the form y_r = |< a_r,x >|^2, r = 1,2,...,m
(knowledge of the phase of these samples would yield a linear system). This
paper develops a non-convex formulation of the phase retrieval problem as well
as a concrete solution algorithm. In a nutshell, this algorithm starts with a
careful initialization obtained by means of a spectral method, and then refines
this initial estimate by iteratively applying novel update rules, which have
low computational complexity, much like in a gradient descent scheme. The main
contribution is that this algorithm is shown to rigorously allow the exact
retrieval of phase information from a nearly minimal number of random
measurements. Indeed, the sequence of successive iterates provably converges to
the solution at a geometric rate so that the proposed scheme is efficient both
in terms of computational and data resources. In theory, a variation on this
scheme leads to a near-linear time algorithm for a physically realizable model
based on coded diffraction patterns. We illustrate the effectiveness of our
methods with various experiments on image data. Underlying our analysis are
insights for the analysis of non-convex optimization schemes that may have
implications for computational problems beyond phase retrieval.
",0.0
4,5,28542,1409.0203,matrix completion,"Mohammad J. Taghizadeh, Reza Parhizkar, Philip N. Garner, Herve
  Bourlard, Afsaneh Asaei","Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix
  Completion Algorithm and Theoretical Guarantees","  This paper addresses the problem of ad hoc microphone array calibration where
only partial information about the distances between microphones is available.
We construct a matrix consisting of the pairwise distances and propose to
estimate the missing entries based on a novel Euclidean distance matrix
completion algorithm by alternative low-rank matrix completion and projection
onto the Euclidean distance space. This approach confines the recovered matrix
to the EDM cone at each iteration of the matrix completion algorithm. The
theoretical guarantees of the calibration performance are obtained considering
the random and locally structured missing entries as well as the measurement
noise on the known distances. This study elucidates the links between the
calibration error and the number of microphones along with the noise level and
the ratio of missing distances. Thorough experiments on real data recordings
and simulated setups are conducted to demonstrate these theoretical insights. A
significant improvement is achieved by the proposed Euclidean distance matrix
completion algorithm over the state-of-the-art techniques for ad hoc microphone
array calibration.
",3.0
5,15,59424,1711.00953,relevance feedback for imformation retrieval,"Bj\""orn Barz, Joachim Denzler",Automatic Query Image Disambiguation for Content-Based Image Retrieval,"  Query images presented to content-based image retrieval systems often have
various different interpretations, making it difficult to identify the search
objective pursued by the user. We propose a technique for overcoming this
ambiguity, while keeping the amount of required user interaction at a minimum.
To achieve this, the neighborhood of the query image is divided into coherent
clusters from which the user may choose the relevant ones. A novel feedback
integration technique is then employed to re-rank the entire database with
regard to both the user feedback and the original query. We evaluate our
approach on the publicly available MIRFLICKR-25K dataset, where it leads to a
relative improvement of average precision by 23% over the baseline retrieval,
which does not distinguish between different image senses.
",5.0
6,14,135760,2010.12854,text summarization model,Shih-Ting Lin and Ashish Sabharwal and Tushar Khot,ReadOnce Transformers: Reusable Representations of Text for Transformers,"  We present ReadOnce Transformers, an approach to convert a transformer-based
model into one that can build an information-capturing, task-independent, and
compressed representation of text. The resulting representation is reusable
across different examples and tasks, thereby requiring a document shared across
many examples or tasks to only be \emph{read once}. This leads to faster
training and evaluation of models. Additionally, we extend standard
text-to-text transformer models to Representation+Text-to-text models, and
evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and
long-document summarization. Our one-time computed representation results in a
2x-5x speedup compared to standard text-to-text models, while the compression
also allows existing language models to handle longer documents without the
need for designing new pre-trained models.
",3.0
7,16,103893,1911.09257,activation function in neutral networks,Andrew Hryniowski and Alexander Wong,"DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully
  Learnable Basis Functions","  From fully connected neural networks to convolutional neural networks, the
learned parameters within a neural network have been primarily relegated to the
linear parameters (e.g., convolutional filters). The non-linear functions
(e.g., activation functions) have largely remained, with few exceptions in
recent years, parameter-less, static throughout training, and seen limited
variation in design. Largely ignored by the deep learning community, radial
basis function (RBF) networks provide an interesting mechanism for learning
more complex non-linear activation functions in addition to the linear
parameters in a network. However, the interest in RBF networks has waned over
time due to the difficulty of integrating RBFs into more complex deep neural
network architectures in a tractable and stable manner. In this work, we
present a novel approach that enables end-to-end learning of deep RBF networks
with fully learnable activation basis functions in an automatic and tractable
manner. We demonstrate that our approach for enabling the use of learnable
activation basis functions in deep neural networks, which we will refer to as
DeepLABNet, is an effective tool for automated activation function learning
within complex network architectures.
",5.0
8,16,75285,1810.0612,activation function in neutral networks,"Yiwei Li, Enzhi Li",Variational Neural Networks: Every Layer and Neuron Can Be Unique,"  The choice of activation function can significantly influence the performance
of neural networks. The lack of guiding principles for the selection of
activation function is lamentable. We try to address this issue by introducing
our variational neural networks, where the activation function is represented
as a linear combination of possible candidate functions, and an optimal
activation is obtained via minimization of a loss function using gradient
descent method. The gradient formulae for the loss function with respect to
these expansion coefficients are central for the implementation of gradient
descent algorithm, and here we derive these gradient formulae.
",5.0
9,0,164943,2107.13327,learning to rank with partitioned preference,"Oriol Barbany Mayor, Vito Bellini, Alexander Buchholz, Giuseppe Di
  Benedetto, Diego Marco Granziol, Matteo Ruffini, Yannik Stein",Ranker-agnostic Contextual Position Bias Estimation,"  Learning-to-rank (LTR) algorithms are ubiquitous and necessary to explore the
extensive catalogs of media providers. To avoid the user examining all the
results, its preferences are used to provide a subset of relatively small size.
The user preferences can be inferred from the interactions with the presented
content if explicit ratings are unavailable. However, directly using implicit
feedback can lead to learning wrong relevance models and is known as biased
LTR. The mismatch between implicit feedback and true relevances is due to
various nuisances, with position bias one of the most relevant. Position bias
models consider that the lack of interaction with a presented item is not only
attributed to the item being irrelevant but because the item was not examined.
This paper introduces a method for modeling the probability of an item being
seen in different contexts, e.g., for different users, with a single estimator.
Our suggested method, denoted as contextual (EM)-based regression, is
ranker-agnostic and able to correctly learn the latent examination
probabilities while only using implicit feedback. Our empirical results
indicate that the method introduced in this paper outperforms other existing
position bias estimators in terms of relative error when the examination
probability varies across queries. Moreover, the estimated values provide a
ranking performance boost when used to debias the implicit ranking data even if
there is no context dependency on the examination probabilities.
",1.0
10,5,18416,1302.6768,matrix completion,"Gil Shabat, Yaniv Shmueli and Amir Averbuch",Missing Entries Matrix Approximation and Completion,"  We describe several algorithms for matrix completion and matrix approximation
when only some of its entries are known. The approximation constraint can be
any whose approximated solution is known for the full matrix. For low rank
approximations, similar algorithms appears recently in the literature under
different names. In this work, we introduce new theorems for matrix
approximation and show that these algorithms can be extended to handle
different constraints such as nuclear norm, spectral norm, orthogonality
constraints and more that are different than low rank approximations. As the
algorithms can be viewed from an optimization point of view, we discuss their
convergence to global solution for the convex case. We also discuss the optimal
step size and show that it is fixed in each iteration. In addition, the derived
matrix completion flow is robust and does not require any parameters. This
matrix completion flow is applicable to different spectral minimizations and
can be applied to physics, mathematics and electrical engineering problems such
as data reconstruction of images and data coming from PDEs such as Helmholtz
equation used for electromagnetic waves.
",5.0
11,2,20312,1305.4525,random forests,Miron B. Kursa,Robustness of Random Forest-based gene selection methods,"  Gene selection is an important part of microarray data analysis because it
provides information that can lead to a better mechanistic understanding of an
investigated phenomenon. At the same time, gene selection is very difficult
because of the noisy nature of microarray data. As a consequence, gene
selection is often performed with machine learning methods. The Random Forest
method is particularly well suited for this purpose. In this work, four
state-of-the-art Random Forest-based feature selection methods were compared in
a gene selection context. The analysis focused on the stability of selection
because, although it is necessary for determining the significance of results,
it is often ignored in similar studies.
  The comparison of post-selection accuracy in the validation of Random Forest
classifiers revealed that all investigated methods were equivalent in this
context. However, the methods substantially differed with respect to the number
of selected genes and the stability of selection. Of the analysed methods, the
Boruta algorithm predicted the most genes as potentially important.
  The post-selection classifier error rate, which is a frequently used measure,
was found to be a potentially deceptive measure of gene selection quality. When
the number of consistently selected genes was considered, the Boruta algorithm
was clearly the best. Although it was also the most computationally intensive
method, the Boruta algorithm's computational demands could be reduced to levels
comparable to those of other algorithms by replacing the Random Forest
importance with a comparable measure from Random Ferns (a similar but
simplified classifier). Despite their design assumptions, the minimal optimal
selection methods, were found to select a high fraction of false positives.
",3.0
12,4,201100,2206.11251,pre-trained language model,"Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya,
  Lerrel Pinto",Behavior Transformers: Cloning $k$ modes with one stone,"  While behavior learning has made impressive progress in recent times, it lags
behind computer vision and natural language processing due to its inability to
leverage large, human-generated datasets. Human behaviors have wide variance,
multiple modes, and human demonstrations typically do not come with reward
labels. These properties limit the applicability of current methods in Offline
RL and Behavioral Cloning to learn from large, pre-collected datasets. In this
work, we present Behavior Transformer (BeT), a new technique to model unlabeled
demonstration data with multiple modes. BeT retrofits standard transformer
architectures with action discretization coupled with a multi-task action
correction inspired by offset prediction in object detection. This allows us to
leverage the multi-modal modeling ability of modern transformers to predict
multi-modal continuous actions. We experimentally evaluate BeT on a variety of
robotic manipulation and self-driving behavior datasets. We show that BeT
significantly improves over prior state-of-the-art work on solving demonstrated
tasks while capturing the major modes present in the pre-collected datasets.
Finally, through an extensive ablation study, we analyze the importance of
every crucial component in BeT. Videos of behavior generated by BeT are
available at https://notmahi.github.io/bet
",1.0
13,1,37838,1511.05802,advanced search engine,"Nadine Hoechstoetter, Dirk Lewandowski",What Users See - Structures in Search Engine Results Pages,"  This paper investigates the composition of search engine results pages. We
define what elements the most popular web search engines use on their results
pages (e.g., organic results, advertisements, shortcuts) and to which degree
they are used for popular vs. rare queries. Therefore, we send 500 queries of
both types to the major search engines Google, Yahoo, Live.com and Ask. We
count how often the different elements are used by the individual engines. In
total, our study is based on 42,758 elements. Findings include that search
engines use quite different approaches to results pages composition and
therefore, the user gets to see quite different results sets depending on the
search engine and search query used. Organic results still play the major role
in the results pages, but different shortcuts are of some importance, too.
Regarding the frequency of certain host within the results sets, we find that
all search engines show Wikipedia results quite often, while other hosts shown
depend on the search engine used. Both Google and Yahoo prefer results from
their own offerings (such as YouTube or Yahoo Answers). Since we used the .com
interfaces of the search engines, results may not be valid for other
country-specific interfaces.
",3.0
14,10,165516,2108.01605,web archive,"Miguel Costa, Julien Masan\`es",Big Data Science Over the Past Web,"  Web archives preserve unique and historically valuable information. They hold
a record of past events and memories published by all kinds of people, such as
journalists, politicians and ordinary people who have shared their testimony
and opinion on multiple subjects. As a result, researchers such as historians
and sociologists have used web archives as a source of information to
understand the recent past since the early days of the World Wide Web. The
typical way to extract knowledge from a web archive is by using its search
functionalities to find and analyse historical content. This can be a slow and
superficial process when analysing complex topics, due to the huge amount of
data that web archives have been preserving over time. Big data science tools
can cope with this order of magnitude, enabling researchers to automatically
extract meaningful knowledge from the archived data. This knowledge helps not
only to explain the past but also to predict the future through the
computational modelling of events and behaviours. Currently, there is an
immense landscape of big data tools, machine learning frameworks and deep
learning algorithms that significantly increase the scalability and performance
of several computational tasks, especially over text, image and audio. Web
archives have been taking advantage of this panoply of technologies to provide
their users with more powerful tools to explore and exploit historical data.
This chapter presents several examples of these tools and gives an overview of
their application to support longitudinal studies over web archive collections.
",5.0
15,4,50312,1702.04521,pre-trained language model,"Micha{\l} Daniluk, Tim Rockt\""aschel, Johannes Welbl, Sebastian Riedel",Frustratingly Short Attention Spans in Neural Language Modeling,"  Neural language models predict the next token using a latent representation
of the immediate token history. Recently, various methods for augmenting neural
language models with an attention mechanism over a differentiable memory have
been proposed. For predicting the next token, these models query information
from a memory of the recent history which can facilitate learning mid- and
long-range dependencies. However, conventional attention mechanisms used in
memory-augmented neural language models produce a single output vector per time
step. This vector is used both for predicting the next token as well as for the
key and value of a differentiable memory of a token history. In this paper, we
propose a neural language model with a key-value attention mechanism that
outputs separate representations for the key and value of a differentiable
memory, as well as for encoding the next-word distribution. This model
outperforms existing memory-augmented neural language models on two corpora.
Yet, we found that our method mainly utilizes a memory of the five most recent
output representations. This led to the unexpected main finding that a much
simpler model based only on the concatenation of recent output representations
from previous time steps is on par with more sophisticated memory-augmented
neural language models.
",5.0
16,2,202457,2207.01678,random forests,"Chien-Ming Chi, Yingying Fan, Jinchi Lv",FACT: High-Dimensional Random Forests Inference,"  Random forests is one of the most widely used machine learning methods over
the past decade thanks to its outstanding empirical performance. Yet, because
of its black-box nature, the results by random forests can be hard to interpret
in many big data applications. Quantifying the usefulness of individual
features in random forests learning can greatly enhance its interpretability.
Existing studies have shown that some popularly used feature importance
measures for random forests suffer from the bias issue. In addition, there lack
comprehensive size and power analyses for most of these existing methods. In
this paper, we approach the problem via hypothesis testing, and suggest a
framework of the self-normalized feature-residual correlation test (FACT) for
evaluating the significance of a given feature in the random forests model with
bias-resistance property, where our null hypothesis concerns whether the
feature is conditionally independent of the response given all other features.
Such an endeavor on random forests inference is empowered by some recent
developments on high-dimensional random forests consistency. The vanilla
version of our FACT test can suffer from the bias issue in the presence of
feature dependency. We exploit the techniques of imbalancing and conditioning
for bias correction. We further incorporate the ensemble idea into the FACT
statistic through feature transformations for the enhanced power. Under a
fairly general high-dimensional nonparametric model setting with dependent
features, we formally establish that FACT can provide theoretically justified
random forests feature p-values and enjoy appealing power through nonasymptotic
analyses. The theoretical results and finite-sample advantages of the newly
suggested method are illustrated with several simulation examples and an
economic forecasting application in relation to COVID-19.
",4.0
17,9,81748,1901.11459,language model for long documents,"Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani","Funnelling: A New Ensemble Method for Heterogeneous Transfer Learning
  and its Application to Cross-Lingual Text Classification","  Cross-lingual Text Classification (CLC) consists of automatically
classifying, according to a common set C of classes, documents each written in
one of a set of languages L, and doing so more accurately than when naively
classifying each document via its corresponding language-specific classifier.
In order to obtain an increase in the classification accuracy for a given
language, the system thus needs to also leverage the training examples written
in the other languages. We tackle multilabel CLC via funnelling, a new ensemble
learning method that we propose here. Funnelling consists of generating a
two-tier classification system where all documents, irrespectively of language,
are classified by the same (2nd-tier) classifier. For this classifier all
documents are represented in a common, language-independent feature space
consisting of the posterior probabilities generated by 1st-tier,
language-dependent classifiers. This allows the classification of all test
documents, of any language, to benefit from the information present in all
training documents, of any language. We present substantial experiments, run on
publicly available multilingual text collections, in which funnelling is shown
to significantly outperform a number of state-of-the-art baselines. All code
and datasets (in vector form) are made publicly available.
",2.0
18,19,198890,2206.02794,artificial intelligence for low carbon,"R. Pradhan, A.P Joshi, M.R Sunny, and A. Sarkar","Machine learning models for determination of weldbead shape parameters
  for gas metal arc welded T-joints -- A comparative study","  The shape of a weld bead is critical in assessing the quality of the welded
joint. In particular, this has a major impact in the accuracy of the results
obtained from a numerical analysis. This study focuses on the statistical
design techniques and the artificial neural networks, to predict the weld bead
shape parameters of shielded Gas Metal Arc Welded (GMAW) fillet joints.
Extensive testing was carried out on low carbon mild steel plates of
thicknesses ranging from 3mm to 10mm. Welding voltage, welding current, and
moving heat source speed were considered as the welding parameters. Three types
of multiple linear regression models (MLR) were created to establish an
empirical equation for defining GMAW bead shape parameters considering
interactive and higher order terms. Additionally, artificial neural network
(ANN) models were created based on similar scheme, and the relevance of
specific features was investigated using SHapley Additive exPlanations (SHAP).
The results reveal that MLR-based approach performs better than the ANN based
models in terms of predictability and error assessment. This study shows the
usefulness of the predictive tools to aid numerical analysis of welding.
",1.0
19,0,14745,1208.0984,learning to rank with partitioned preference,"Riad Akrour (INRIA Saclay - Ile de France, LRI), Marc Schoenauer
  (INRIA Saclay - Ile de France, LRI), Mich\`ele Sebag (LRI)",APRIL: Active Preference-learning based Reinforcement Learning,"  This paper focuses on reinforcement learning (RL) with limited prior
knowledge. In the domain of swarm robotics for instance, the expert can hardly
design a reward function or demonstrate the target behavior, forbidding the use
of both standard RL and inverse reinforcement learning. Although with a limited
expertise, the human expert is still often able to emit preferences and rank
the agent demonstrations. Earlier work has presented an iterative
preference-based RL framework: expert preferences are exploited to learn an
approximate policy return, thus enabling the agent to achieve direct policy
search. Iteratively, the agent selects a new candidate policy and demonstrates
it; the expert ranks the new demonstration comparatively to the previous best
one; the expert's ranking feedback enables the agent to refine the approximate
policy return, and the process is iterated. In this paper, preference-based
reinforcement learning is combined with active ranking in order to decrease the
number of ranking queries to the expert needed to yield a satisfactory policy.
Experiments on the mountain car and the cancer treatment testbeds witness that
a couple of dozen rankings enable to learn a competent policy.
",0.0
20,4,192515,2204.06283,pre-trained language model,"Zeming Chen, Qiyue Gao","Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in
  Natural Language Understanding","  In the age of large transformer language models, linguistic evaluation play
an important role in diagnosing models' abilities and limitations on natural
language understanding. However, current evaluation methods show some
significant shortcomings. In particular, they do not provide insight into how
well a language model captures distinct linguistic skills essential for
language understanding and reasoning. Thus they fail to effectively map out the
aspects of language understanding that remain challenging to existing models,
which makes it hard to discover potential limitations in models and datasets.
In this paper, we introduce Curriculum as a new format of NLI benchmark for
evaluation of broad-coverage linguistic phenomena. Curriculum contains a
collection of datasets that covers 36 types of major linguistic phenomena and
an evaluation procedure for diagnosing how well a language model captures
reasoning skills for distinct types of linguistic phenomena. We show that this
linguistic-phenomena-driven benchmark can serve as an effective tool for
diagnosing model behavior and verifying model learning quality. In addition,
our experiments provide insight into the limitation of existing benchmark
datasets and state-of-the-art models that may encourage future research on
re-designing datasets, model architectures, and learning objectives.
",3.0
21,10,15187,1209.2664,web archive,Michael L. Nelson,"A Plan For Curating ""Obsolete Data or Resources""","  Our cultural discourse is increasingly carried in the web. With the initial
emergence of the web many years ago, there was a period where conventional
mediums (e.g., music, movies, books, scholarly publications) were primary and
the web was a supplementary channel. This has now changed, where the web is
often the primary channel, and other publishing mechanisms, if present at all,
supplement the web. Unfortunately, the technology for publishing information on
the web always outstrips our technology for preservation. My concern is less
that we will lose data of known importance (e.g., scientific data, census
data), but rather that we will lose data that we do not yet know is important.
In this paper I review some of the issues and, where appropriate, proposed
solutions for increasing the archivability of the web.
",2.0
22,14,129577,2008.11908,text summarization model,"Ensieh Davoodijam, Nasser Ghadiri, Maryam Lotfi Shahreza, Fabio
  Rinaldi",MultiGBS: A multi-layer graph approach to biomedical summarization,"  Automatic text summarization methods generate a shorter version of the input
text to assist the reader in gaining a quick yet informative gist. Existing
text summarization methods generally focus on a single aspect of text when
selecting sentences, causing the potential loss of essential information. In
this study, we propose a domain-specific method that models a document as a
multi-layer graph to enable multiple features of the text to be processed at
the same time. The features we used in this paper are word similarity, semantic
similarity, and co-reference similarity, which are modelled as three different
layers. The unsupervised method selects sentences from the multi-layer graph
based on the MultiRank algorithm and the number of concepts. The proposed
MultiGBS algorithm employs UMLS and extracts the concepts and relationships
using different tools such as SemRep, MetaMap, and OGER. Extensive evaluation
by ROUGE and BERTScore shows increased F-measure values.
",5.0
23,8,77630,1811.08019,node embedding for graph,George Berry,Role action embeddings: scalable representation of network positions,"  We consider the question of embedding nodes with similar local neighborhoods
together in embedding space, commonly referred to as ""role embeddings."" We
propose RAE, an unsupervised framework that learns role embeddings. It combines
a within-node loss function and a graph neural network (GNN) architecture to
place nodes with similar local neighborhoods close in embedding space. We also
propose a faster way of generating negative examples called neighbor shuffling,
which quickly creates negative examples directly within batches. These
techniques can be easily combined with existing GNN methods to create
unsupervised role embeddings at scale. We then explore role action embeddings,
which summarize the non-structural features in a node's neighborhood, leading
to better performance on node classification tasks. We find that the model
architecture proposed here provides strong performance on both graph and node
classification tasks, in some cases competitive with semi-supervised methods.
",5.0
24,8,130193,2009.01674,node embedding for graph,Yanqiao Zhu and Yichen Xu and Feng Yu and Shu Wu and Liang Wang,"CAGNN: Cluster-Aware Graph Neural Networks for Unsupervised Graph
  Representation Learning","  Unsupervised graph representation learning aims to learn low-dimensional node
embeddings without supervision while preserving graph topological structures
and node attributive features. Previous graph neural networks (GNN) require a
large number of labeled nodes, which may not be accessible in real-world graph
data. In this paper, we present a novel cluster-aware graph neural network
(CAGNN) model for unsupervised graph representation learning using
self-supervised techniques. In CAGNN, we perform clustering on the node
embeddings and update the model parameters by predicting the cluster
assignments. Moreover, we observe that graphs often contain inter-class edges,
which mislead the GNN model to aggregate noisy information from neighborhood
nodes. We further refine the graph topology by strengthening intra-class edges
and reducing node connections between different classes based on cluster
labels, which better preserves cluster structures in the embedding space. We
conduct comprehensive experiments on two benchmark tasks using real-world
datasets. The results demonstrate the superior performance of the proposed
model over existing baseline methods. Notably, our model gains over 7%
improvements in terms of accuracy on node clustering over state-of-the-arts.
",5.0
25,19,185097,2202.03393,artificial intelligence for low carbon,Francisco Valente,"Link Prediction of Artificial Intelligence Concepts using Low
  Computational Power","  This paper presents an approach proposed for the Science4cast 2021
competition, organized by the Institute of Advanced Research in Artificial
Intelligence, whose main goal was to predict the likelihood of future
associations between machine learning concepts in a semantic network. The
developed methodology corresponds to a solution for a scenario of availability
of low computational power only, exploiting the extraction of low order
topological features and its incorporation in an optimized classifier to
estimate the degree of future connections between the nodes. The reasons that
motivated the developed methodologies will be discussed, as well as some
results, limitations and suggestions of improvements.
",1.0
26,14,113981,2003.11173,text summarization model,"Haiyang Xu, Yahao He, Kun Han, Junwen Chen and Xiangang Li","Learning Syntactic and Dynamic Selective Encoding for Document
  Summarization","  Text summarization aims to generate a headline or a short summary consisting
of the major information of the source text. Recent studies employ the
sequence-to-sequence framework to encode the input with a neural network and
generate abstractive summary. However, most studies feed the encoder with the
semantic word embedding but ignore the syntactic information of the text.
Further, although previous studies proposed the selective gate to control the
information flow from the encoder to the decoder, it is static during the
decoding and cannot differentiate the information based on the decoder states.
In this paper, we propose a novel neural architecture for document
summarization. Our approach has the following contributions: first, we
incorporate syntactic information such as constituency parsing trees into the
encoding sequence to learn both the semantic and syntactic information from the
document, resulting in more accurate summary; second, we propose a dynamic gate
network to select the salient information based on the context of the decoder
state, which is essential to document summarization. The proposed model has
been evaluated on CNN/Daily Mail summarization datasets and the experimental
results show that the proposed approach outperforms baseline approaches.
",5.0
27,4,113756,2003.09833,pre-trained language model,"Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu and Jiwei
  Li","SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive
  Connection","  While the self-attention mechanism has been widely used in a wide variety of
tasks, it has the unfortunate property of a quadratic cost with respect to the
input length, which makes it difficult to deal with long inputs. In this paper,
we present a method for accelerating and structuring self-attentions: Sparse
Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and
attention operations are performed between linked nodes. In contrast with
previous self-attention models with pre-defined structures (edges), the model
learns to construct attention edges to improve task-specific performances. In
this way, the model is able to select the most salient nodes and reduce the
quadratic complexity regardless of the sequence length. Based on SAC, we show
that previous variants of self-attention models are its special cases. Through
extensive experiments on neural machine translation, language modeling, graph
representation learning and image classification, we demonstrate SAC is
competitive with state-of-the-art models while significantly reducing memory
cost.
",4.0
28,14,165182,2107.14691,text summarization model,"Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, Mohit Bansal",EmailSum: Abstractive Email Thread Summarization,"  Recent years have brought about an interest in the challenging task of
summarizing conversation threads (meetings, online discussions, etc.). Such
summaries help analysis of the long text to quickly catch up with the decisions
made and thus improve our work or communication efficiency. To spur research in
thread summarization, we have developed an abstractive Email Thread
Summarization (EmailSum) dataset, which contains human-annotated short (<30
words) and long (<100 words) summaries of 2549 email threads (each containing 3
to 10 emails) over a wide variety of topics. We perform a comprehensive
empirical study to explore different summarization techniques (including
extractive and abstractive methods, single-document and hierarchical models, as
well as transfer and semisupervised learning) and conduct human evaluations on
both short and long summary generation tasks. Our results reveal the key
challenges of current abstractive summarization models in this task, such as
understanding the sender's intent and identifying the roles of sender and
receiver. Furthermore, we find that widely used automatic evaluation metrics
(ROUGE, BERTScore) are weakly correlated with human judgments on this email
thread summarization task. Hence, we emphasize the importance of human
evaluation and the development of better metrics by the community. Our code and
summary data have been made available at:
https://github.com/ZhangShiyue/EmailSum
",3.0
29,6,1231,804.2057,query expansion for imformation retrieval,"Jos\'e R. P\'erez-Ag\""uera and Lourdes Araujo",Comparing and Combining Methods for Automatic Query Expansion,"  Query expansion is a well known method to improve the performance of
information retrieval systems. In this work we have tested different approaches
to extract the candidate query terms from the top ranked documents returned by
the first-pass retrieval.
  One of them is the cooccurrence approach, based on measures of cooccurrence
of the candidate and the query terms in the retrieved documents. The other one,
the probabilistic approach, is based on the probability distribution of terms
in the collection and in the top ranked set.
  We compare the retrieval improvement achieved by expanding the query with
terms obtained with different methods belonging to both approaches. Besides, we
have developed a na\""ive combination of both kinds of method, with which we
have obtained results that improve those obtained with any of them separately.
This result confirms that the information provided by each approach is of a
different nature and, therefore, can be used in a combined manner.
",5.0
30,10,40185,1602.06223,web archive,Shawn M. Jones and Harihar Shankar,Rules of Acquisition for Mementos and Their Content,"  Text extraction from web pages has many applications, including web crawling
optimization and document clustering. Though much has been written about the
acquisition of content from live web pages, content acquisition of archived web
pages, known as mementos, remains a relatively new enterprise. In the course of
conducting a study with almost 700,000 web pages, we encountered issues
acquiring mementos and extracting text from them. The acquisition of memento
content via HTTP is expected to be a relatively painless exercise, but we have
found cases to the contrary. We also find that the parsing of HTML, already
known to be problematic, can be more complex when one attempts to extract the
text of mementos across many web archives, due to issues involving different
memento presentation behaviors, as well as the age of the HTML in their
mementos. For the benefit of others acquiring mementos across many web
archives, we document those experiences here.
",4.0
31,19,181786,2201.0212,artificial intelligence for low carbon,"Thomas Anderson, Adam Belay, Mosharaf Chowdhury, Asaf Cidon, and Irene
  Zhang",Treehouse: A Case For Carbon-Aware Datacenter Software,"  The end of Dennard scaling and the slowing of Moore's Law has put the energy
use of datacenters on an unsustainable path. Datacenters are already a
significant fraction of worldwide electricity use, with application demand
scaling at a rapid rate. We argue that substantial reductions in the carbon
intensity of datacenter computing are possible with a software-centric
approach: by making energy and carbon visible to application developers on a
fine-grained basis, by modifying system APIs to make it possible to make
informed trade offs between performance and carbon emissions, and by raising
the level of application programming to allow for flexible use of more energy
efficient means of compute and storage. We also lay out a research agenda for
systems software to reduce the carbon footprint of datacenter computing.
",2.0
32,9,116830,2004.12297,language model for long documents,"Liu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, Marc Najork","Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical
  Encoder for Long-Form Document Matching","  Many natural language processing and information retrieval problems can be
formalized as the task of semantic matching. Existing work in this area has
been largely focused on matching between short texts (e.g., question
answering), or between a short and a long text (e.g., ad-hoc retrieval).
Semantic matching between long-form documents, which has many important
applications like news recommendation, related article recommendation and
document clustering, is relatively less explored and needs more research
effort. In recent years, self-attention based models like Transformers and BERT
have achieved state-of-the-art performance in the task of text matching. These
models, however, are still limited to short text like a few sentences or one
paragraph due to the quadratic computational complexity of self-attention with
respect to input text length. In this paper, we address the issue by proposing
the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for
long-form document matching. Our model contains several innovations to adapt
self-attention models for longer text input. In order to better capture
sentence level semantic relations within a document, we pre-train the model
with a novel masked sentence block language modeling task in addition to the
masked word language modeling task used by BERT. Our experimental results on
several benchmark datasets for long-form document matching show that our
proposed SMITH model outperforms the previous state-of-the-art models including
hierarchical attention, multi-depth attention-based hierarchical recurrent
neural network, and BERT. Comparing to BERT based baselines, our model is able
to increase maximum input text length from 512 to 2048. We will open source a
Wikipedia based benchmark dataset, code and a pre-trained checkpoint to
accelerate future research on long-form document matching.
",5.0
33,15,123071,2006.11821,relevance feedback for imformation retrieval,Subhadip Maji and Smarajit Bose,An Improved Relevance Feedback in CBIR,"  Relevance Feedback in Content-Based Image Retrieval is a method where the
feedback of the performance is being used to improve itself. Prior works use
feature re-weighting and classification techniques as the Relevance Feedback
methods. This paper shows a novel addition to the prior methods to further
improve the retrieval accuracy. In addition to all of these, the paper also
shows a novel idea to even improve the 0-th iteration retrieval accuracy from
the information of Relevance Feedback.
",5.0
34,15,150529,2103.10474,relevance feedback for imformation retrieval,"Onifade Olufade, Arise Abiola, Ogboo Chisom","Dynamic Model for Query-Document Expansion towards Improving Retrieval
  Relevance","  Getting relevant information from search engines has been the heart of
research works in information retrieval. Query expansion is a retrieval
technique that has been studied and proved to yield positive results in
relevance. Users are required to express their queries as a shortlist of words,
sentences, or questions. With this short format, a huge amount of information
is lost in the process of translating the information need from the actual
query size since the user cannot convey all his thoughts in a few words. This
mostly leads to poor query representation which contributes to undesired
retrieval effectiveness. This loss of information has made the study of query
expansion technique a strong area of study. This research work focuses on two
methods of retrieval for both tweet-length queries and sentence-length queries.
Two algorithms have been proposed and the implementation is expected to produce
a better relevance retrieval model than most state-the-art relevance models.
",5.0
35,5,121523,2006.06283,matrix completion,"Jianwen Huang, Wendong Wang, Feng Zhang, Jianjun Wang",The perturbation analysis of nonconvex low-rank matrix robust recovery,"  In this paper, we bring forward a completely perturbed nonconvex Schatten
$p$-minimization to address a model of completely perturbed low-rank matrix
recovery. The paper that based on the restricted isometry property generalizes
the investigation to a complete perturbation model thinking over not only noise
but also perturbation, gives the restricted isometry property condition that
guarantees the recovery of low-rank matrix and the corresponding reconstruction
error bound. In particular, the analysis of the result reveals that in the case
that $p$ decreases $0$ and $a>1$ for the complete perturbation and low-rank
matrix, the condition is the optimal sufficient condition $\delta_{2r}<1$
\cite{Recht et al 2010}. The numerical experiments are conducted to show better
performance, and provides outperformance of the nonconvex Schatten
$p$-minimization method comparing with the convex nuclear norm minimization
approach in the completely perturbed scenario.
",4.0
36,5,85743,1904.03779,matrix completion,"Chengkun Zhang. Junbin Gao, Stephen Lu",Cluster Developing 1-Bit Matrix Completion,"  Matrix completion has a long-time history of usage as the core technique of
recommender systems. In particular, 1-bit matrix completion, which considers
the prediction as a ``Recommended'' or ``Not Recommended'' question, has proved
its significance and validity in the field. However, while customers and
products aggregate into interacted clusters, state-of-the-art model-based 1-bit
recommender systems do not take the consideration of grouping bias. To tackle
the gap, this paper introduced Group-Specific 1-bit Matrix Completion (GS1MC)
by first-time consolidating group-specific effects into 1-bit recommender
systems under the low-rank latent variable framework. Additionally, to empower
GS1MC even when grouping information is unobtainable, Cluster Developing Matrix
Completion (CDMC) was proposed by integrating the sparse subspace clustering
technique into GS1MC. Namely, CDMC allows clustering users/items and to
leverage their group effects into matrix completion at the same time.
Experiments on synthetic and real-world data show that GS1MC outperforms the
current 1-bit matrix completion methods. Meanwhile, it is compelling that CDMC
can successfully capture items' genre features only based on sparse binary
user-item interactive data. Notably, GS1MC provides a new insight to
incorporate and evaluate the efficacy of clustering methods while CDMC can be
served as a new tool to explore unrevealed social behavior or market
phenomenon.
",5.0
37,1,124859,2007.03106,advanced search engine,Sarvesh Soni and Kirk Roberts,"An Evaluation of Two Commercial Deep Learning-Based Information
  Retrieval Systems for COVID-19 Literature","  The COVID-19 pandemic has resulted in a tremendous need for access to the
latest scientific information, primarily through the use of text mining and
search tools. This has led to both corpora for biomedical articles related to
COVID-19 (such as the CORD-19 corpus (Wang et al., 2020)) as well as search
engines to query such data. While most research in search engines is performed
in the academic field of information retrieval (IR), most academic search
engines$\unicode{x2013}$though rigorously evaluated$\unicode{x2013}$are
sparsely utilized, while major commercial web search engines (e.g., Google,
Bing) dominate. This relates to COVID-19 because it can be expected that
commercial search engines deployed for the pandemic will gain much higher
traction than those produced in academic labs, and thus leads to questions
about the empirical performance of these search tools. This paper seeks to
empirically evaluate two such commercial search engines for COVID-19, produced
by Google and Amazon, in comparison to the more academic prototypes evaluated
in the context of the TREC-COVID track (Roberts et al., 2020). We performed
several steps to reduce bias in the available manual judgments in order to
ensure a fair comparison of the two systems with those submitted to TREC-COVID.
We find that the top-performing system from TREC-COVID on bpref metric
performed the best among the different systems evaluated in this study on all
the metrics. This has implications for developing biomedical retrieval systems
for future health crises as well as trust in popular health search engines.
",4.0
38,6,79751,1812.10119,query expansion for imformation retrieval,Salah Zaiem and Fatiha Sadat,Sequence to Sequence Learning for Query Expansion,"  Using sequence to sequence algorithms for query expansion has not been
explored yet in Information Retrieval literature nor in Question-Answering's.
We tried to fill this gap in the literature with a custom Query Expansion
engine trained and tested on open datasets. Starting from open datasets, we
built a Query Expansion training set using sentence-embeddings-based Keyword
Extraction. We therefore assessed the ability of the Sequence to Sequence
neural networks to capture expanding relations in the words embeddings' space.
",5.0
39,4,206765,2208.08227,pre-trained language model,"Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna
  Phipps-Costin, Donald Pinckney, Ming Ho Yee, Yangtian Zi, Carolyn Jane
  Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg, Abhinav Jangda","A Scalable and Extensible Approach to Benchmarking NL2Code for 18
  Programming Languages","  Large language models have demonstrated the ability to condition on and
generate both natural language and programming language text. Such models open
up the possibility of multi-language code generation: could code generation
models generalize knowledge from one language to another? Although contemporary
code generation models can generate semantically correct Python code, little is
known about their abilities with other languages. We facilitate the exploration
of this topic by proposing MultiPL-E, the first multi-language parallel
benchmark for natural-language-to-code-generation.
  MultiPL-E extends the HumanEval benchmark (Chen et al, 2021) to support 18
more programming languages, encompassing a range of programming paradigms and
popularity. We evaluate two state-of-the-art code generation models on
MultiPL-E: Codex and InCoder. We find that on several languages, Codex matches
and even exceeds its performance on Python. The range of programming languages
represented in MultiPL-E allow us to explore the impact of language frequency
and language features on model performance. Finally, the MultiPL-E approach of
compiling code generation benchmarks to new programming languages is both
scalable and extensible. We describe a general approach for easily adding
support for new benchmarks and languages to MultiPL-E.
",4.0
40,11,29867,1411.3776,PageRank for web search,"Donggeng Xia, Shawn Mankad, George Michailidis","Measuring Influence in Twitter Ecosystems using a Counting Process
  Modeling Framework","  Data extracted from social media platforms, such as Twitter, are both large
in scale and complex in nature, since they contain both unstructured text, as
well as structured data, such as time stamps and interactions between users. A
key question for such platforms is to determine influential users, in the sense
that they generate interactions between members of the platform. Common
measures used both in the academic literature and by companies that provide
analytics services are variants of the popular web-search PageRank algorithm
applied to networks that capture connections between users. In this work, we
develop a modeling framework using multivariate interacting counting processes
to capture the detailed actions that users undertake on such platforms, namely
posting original content, reposting and/or mentioning other users' postings.
Based on the proposed model, we also derive a novel influence measure. We
discuss estimation of the model parameters through maximum likelihood and
establish their asymptotic properties. The proposed model and the accompanying
influence measure are illustrated on a data set covering a five year period of
the Twitter actions of the members of the US Senate, as well as mainstream news
organizations and media personalities.
",1.0
41,11,96847,1909.01132,PageRank for web search,"Loc Tran, Tho Quan, An Mai",PageRank algorithm for Directed Hypergraph,"  During the last two decades, we easilly see that the World Wide Web's link
structure is modeled as the directed graph. In this paper, we will model the
World Wide Web's link structure as the directed hypergraph. Moreover, we will
develop the PageRank algorithm for this directed hypergraph. Due to the lack of
the World Wide Web directed hypergraph datasets, we will apply the PageRank
algorithm to the metabolic network which is the directed hypergraph itself. The
experiments show that our novel PageRank algorithm is successfully applied to
this metabolic network.
",5.0
42,1,5238,1004.446,advanced search engine,"Sumalatha Ramachandran, Sharon Joseph, Sujaya Paulraj and Vetriselvi
  Ramaraj","Handling Overload Conditions In High Performance Trustworthy Information
  Retrieval Systems","  Web search engines retrieve a vast amount of information for a given search
query. But the user needs only trustworthy and high-quality information from
this vast retrieved data. The response time of the search engine must be a
minimum value in order to satisfy the user. An optimum level of response time
should be maintained even when the system is overloaded. This paper proposes an
optimal Load Shedding algorithm which is used to handle overload conditions in
real-time data stream applications and is adapted to the Information Retrieval
System of a web search engine. Experiment results show that the proposed
algorithm enables a web search engine to provide trustworthy search results to
the user within an optimum response time, even during overload conditions.
",5.0
43,18,170099,2109.10739,infomation retrieval time complexity,"Negar Arabzadeh, Xinyi Yan and Charles L. A. Clarke","Predicting Efficiency/Effectiveness Trade-offs for Dense vs. Sparse
  Retrieval Strategy Selection","  Over the last few years, contextualized pre-trained transformer models such
as BERT have provided substantial improvements on information retrieval tasks.
Recent approaches based on pre-trained transformer models such as BERT,
fine-tune dense low-dimensional contextualized representations of queries and
documents in embedding space. While these dense retrievers enjoy substantial
retrieval effectiveness improvements compared to sparse retrievers, they are
computationally intensive, requiring substantial GPU resources, and dense
retrievers are known to be more expensive from both time and resource
perspectives. In addition, sparse retrievers have been shown to retrieve
complementary information with respect to dense retrievers, leading to
proposals for hybrid retrievers. These hybrid retrievers leverage low-cost,
exact-matching based sparse retrievers along with dense retrievers to bridge
the semantic gaps between query and documents. In this work, we address this
trade-off between the cost and utility of sparse vs dense retrievers by
proposing a classifier to select a suitable retrieval strategy (i.e., sparse
vs. dense vs. hybrid) for individual queries. Leveraging sparse retrievers for
queries which can be answered with sparse retrievers decreases the number of
calls to GPUs. Consequently, while utility is maintained, query latency
decreases. Although we use less computational resources and spend less time, we
still achieve improved performance. Our classifier can select between sparse
and dense retrieval strategies based on the query alone. We conduct experiments
on the MS MARCO passage dataset demonstrating an improved range of
efficiency/effectiveness trade-offs between purely sparse, purely dense or
hybrid retrieval strategies, allowing an appropriate strategy to be selected
based on a target latency and resource budget.
",4.0
44,9,180992,2112.13432,language model for long documents,"Suchismit Mahapatra, Vladimir Blagojevic, Pablo Bertorello, Prasanna
  Kumar",New Methods & Metrics for LFQA tasks,"  Long-form question answering (LFQA) tasks require retrieving the documents
pertinent to a query, using them to form a paragraph-length answer. Despite
considerable progress in LFQA modeling, fundamental issues impede its progress:
i) train/validation/test dataset overlap, ii) absence of automatic metrics and
iii) generated answers not being ""grounded"" in retrieved documents. This work
addresses every one these critical bottlenecks, contributing natural language
inference/generation (NLI/NLG) methods and metrics that make significant
strides to their alleviation.
",4.0
45,8,106140,1912.08808,node embedding for graph,"Artem Lutov, Dingqi Yang and Philippe Cudr\'e-Mauroux","Bridging the Gap between Community and Node Representations: Graph
  Embedding via Community Detection","  Graph embedding has become a key component of many data mining and analysis
systems. Current graph embedding approaches either sample a large number of
node pairs from a graph to learn node embeddings via stochastic optimization or
factorize a high-order proximity/adjacency matrix of the graph via
computationally expensive matrix factorization techniques. These approaches
typically require significant resources for the learning process and rely on
multiple parameters, which limits their applicability in practice. Moreover,
most of the existing graph embedding techniques operate effectively in one
specific metric space only (e.g., the one produced with cosine similarity), do
not preserve higher-order structural features of the input graph and cannot
automatically determine a meaningful number of embedding dimensions. Typically,
the produced embeddings are not easily interpretable, which complicates further
analyses and limits their applicability. To address these issues, we propose
DAOR, a highly efficient and parameter-free graph embedding technique producing
metric space-robust, compact and interpretable embeddings without any manual
tuning. Compared to a dozen state-of-the-art graph embedding algorithms, DAOR
yields competitive results on both node classification (which benefits form
high-order proximity) and link prediction (which relies on low-order proximity
mostly). Unlike existing techniques, however, DAOR does not require any
parameter tuning and improves the embeddings generation speed by several orders
of magnitude. Our approach has hence the ambition to greatly simplify and speed
up data analysis tasks involving graph representation learning.
",5.0
46,3,20960,1307.0191,database management system,A B M Moniruzzaman and Syed Akhter Hossain,"NoSQL Database: New Era of Databases for Big data Analytics -
  Classification, Characteristics and Comparison","  Digital world is growing very fast and become more complex in the volume
(terabyte to petabyte), variety (structured and un-structured and hybrid),
velocity (high speed in growth) in nature. This refers to as Big Data that is a
global phenomenon. This is typically considered to be a data collection that
has grown so large it can not be effectively managed or exploited using
conventional data management tools: e.g., classic relational database
management systems (RDBMS) or conventional search engines. To handle this
problem, traditional RDBMS are complemented by specifically designed a rich set
of alternative DBMS; such as - NoSQL, NewSQL and Search-based systems. This
paper motivation is to provide - classification, characteristics and evaluation
of NoSQL databases in Big Data Analytics. This report is intended to help
users, especially to the organizations to obtain an independent understanding
of the strengths and weaknesses of various NoSQL database approaches to
supporting applications that process huge volumes of data.
",3.0
47,2,84576,1903.07799,random forests,"Mohan Shi, Zhihai Wang, Jodong Yuan and Haiyang Liu",Random Pairwise Shapelets Forest,"  Shapelet is a discriminative subsequence of time series. An advanced
shapelet-based method is to embed shapelet into accurate and fast random
forest. However, it shows several limitations. First, random shapelet forest
requires a large training cost for split threshold searching. Second, a single
shapelet provides limited information for only one branch of the decision tree,
resulting in insufficient accuracy and interpretability. Third, randomized
ensemble causes interpretability declining. For that, this paper presents
Random Pairwise Shapelets Forest (RPSF). RPSF combines a pair of shapelets from
different classes to construct random forest. It omits threshold searching to
be more efficient, includes more information for each node of the forest to be
more effective. Moreover, a discriminability metric, Decomposed Mean Decrease
Impurity (DMDI), is proposed to identify influential region for every class.
Extensive experiments show RPSF improves the accuracy and training speed of
shapelet-based forest. Case studies demonstrate the interpretability of our
method.
",5.0
48,19,131544,2009.09083,artificial intelligence for low carbon,Martin Molina,What is an intelligent system?,"  The concept of intelligent system has emerged in information technology as a
type of system derived from successful applications of artificial intelligence.
The goal of this paper is to give a general description of an intelligent
system, which integrates previous approaches and takes into account recent
advances in artificial intelligence. The paper describes an intelligent system
in a generic way, identifying its main properties and functional components,
and presents some common categories. The presented description follows a
practical approach to be used by system engineers. Its generality and its use
is illustrated with real-world system examples and related with artificial
intelligence methods.
",2.0
49,8,119269,2005.10039,node embedding for graph,"Tobias Schumacher, Hinrikus Wolf, Martin Ritzert, Florian Lemmerich,
  Jan Bachmann, Florian Frantzen, Max Klabunde, Martin Grohe, Markus Strohmaier",The Effects of Randomness on the Stability of Node Embeddings,"  We systematically evaluate the (in-)stability of state-of-the-art node
embedding algorithms due to randomness, i.e., the random variation of their
outcomes given identical algorithms and graphs. We apply five node embeddings
algorithms---HOPE, LINE, node2vec, SDNE, and GraphSAGE---to synthetic and
empirical graphs and assess their stability under randomness with respect to
(i) the geometry of embedding spaces as well as (ii) their performance in
downstream tasks. We find significant instabilities in the geometry of
embedding spaces independent of the centrality of a node. In the evaluation of
downstream tasks, we find that the accuracy of node classification seems to be
unaffected by random seeding while the actual classification of nodes can vary
significantly. This suggests that instability effects need to be taken into
account when working with node embeddings. Our work is relevant for researchers
and engineers interested in the effectiveness, reliability, and reproducibility
of node embedding approaches.
",4.0
50,2,203431,2207.06355,random forests,"Tommaso Aldinucci and Enrico Civitelli and Leonardo di Gangi and
  Alessandro Sestini",Contextual Decision Trees,"  Focusing on Random Forests, we propose a multi-armed contextual bandit
recommendation framework for feature-based selection of a single shallow tree
of the learned ensemble. The trained system, which works on top of the Random
Forest, dynamically identifies a base predictor that is responsible for
providing the final output. In this way, we obtain local interpretations by
observing the rules of the recommended tree. The carried out experiments reveal
that our dynamic method is superior to an independent fitted CART decision tree
and comparable to the whole black-box Random Forest in terms of predictive
performances.
",3.0
51,12,114437,2003.13783,COVID-19 and social media,"Yijun Tian, Rumi Chunara","Quasi-experimental Designs for Assessing Response on Social Media to
  Policy Changes","  Regulation of tobacco products is rapidly evolving. Understanding public
sentiment in response to changes is very important as authorities assess how to
effectively protect population health. Social media systems are widely
recognized to be useful for collecting data about human preferences and
perceptions. However, how social media data may be used, in rapid policy change
settings, given challenges of narrow time periods and specific locations and
non-representative the population using social media is an open question. In
this paper we apply quasi-experimental designs, which have been used previously
in observational data such as social media, to control for time and location
confounders on social media, and then use content analysis of Twitter and
Reddit posts to illustrate the content of reactions to tobacco flavor bans and
the effect of taxation on e-cigarettes. Conclusions distill the potential role
of social media in settings of rapidly changing regulation, in complement to
what is learned by traditional denominator-based representative surveys.
",2.0
52,16,115817,2004.06632,activation function in neutral networks,Leonid Datta,"A Survey on Activation Functions and their relation with Xavier and He
  Normal Initialization","  In artificial neural network, the activation function and the weight
initialization method play important roles in training and performance of a
neural network. The question arises is what properties of a function are
important/necessary for being a well-performing activation function. Also, the
most widely used weight initialization methods - Xavier and He normal
initialization have fundamental connection with activation function. This
survey discusses the important/necessary properties of activation function and
the most widely used activation functions (sigmoid, tanh, ReLU, LReLU and
PReLU). This survey also explores the relationship between these activation
functions and the two weight initialization methods - Xavier and He normal
initialization.
",5.0
53,5,25050,1402.4225,matrix completion,Changho Suh,Information Theory of Matrix Completion,"  Matrix completion is a fundamental problem that comes up in a variety of
applications like the Netflix problem, collaborative filtering, computer
vision, and crowdsourcing. The goal of the problem is to recover a k-by-n
unknown matrix from a subset of its noiseless (or noisy) entries. We define an
information-theoretic notion of completion capacity C that quantifies the
maximum number of entries that one observation of an entry can resolve. This
number provides the minimum number m of entries required for reliable
reconstruction: m=kn/C. Translating the problem into a distributed joint
source-channel coding problem with encoder restriction, we characterize the
completion capacity for a wide class of stochastic models of the unknown matrix
and the observation process. Our achievability proof is inspired by that of the
Slepian-Wolf theorem. For an arbitrary stochastic matrix, we derive an upper
bound on the completion capacity.
",4.0
54,12,23611,1312.3532,COVID-19 and social media,Dedi Rianto Rahadi and Leon Andretti Abdillah,"The utilization of social networking as promotion media (Case study:
  Handicraft business in Palembang)","  Nowadays social media (Twitter, Facebook, etc.), not only simply as
communication media, but also for promotion. Social networking media offers
many business benefits for companies and organizations. Research purposes is to
determine the model of social network media utilization as a promotional media
for handicraft business in Palembang city. Qualitative and quantitative
research design are used to know how handicraft business in Palembang city
utilizing social media networking as a promotional media. The research results
show 35% craft businesses already utilizing social media as a promotional
media. The social media used are blog development 15%, facebook 46%, and
twitter etc. are 39%. The reasons they use social media such as, 1) minimal
cost, 2) easily recognizable, 3) global distribution areas. Social media
emphasis on direct engagement with customers better. So that the marketing
method could be more personal through direct communication with customers.
",1.0
55,11,14790,1208.1926,PageRank for web search,Laxmi Choudhary and Bhawani Shankar Burdak,Role of Ranking Algorithms for Information Retrieval,"  As the use of web is increasing more day by day, the web users get easily
lost in the web's rich hyper structure. The main aim of the owner of the
website is to give the relevant information according their needs to the users.
We explained the Web mining is used to categorize users and pages by analyzing
user's behavior, the content of pages and then describe Web Structure mining.
This paper includes different Page Ranking algorithms and compares those
algorithms used for Information Retrieval. Different Page Rank based algorithms
like Page Rank (PR), WPR (Weighted Page Rank), HITS (Hyperlink Induced Topic
Selection), Distance Rank and EigenRumor algorithms are discussed and compared.
Simulation Interface has been designed for PageRank algorithm and Weighted
PageRank algorithm but PageRank is the only ranking algorithm on which Google
search engine works.
",4.0
56,14,158091,2105.15176,text summarization model,"Tianyang Xu, Chunyun Zhang","Reinforced Generative Adversarial Network for Abstractive Text
  Summarization","  Sequence-to-sequence models provide a viable new approach to generative
summarization, allowing models that are no longer limited to simply selecting
and recombining sentences from the original text. However, these models have
three drawbacks: their grasp of the details of the original text is often
inaccurate, and the text generated by such models often has repetitions, while
it is difficult to handle words that are beyond the word list. In this paper,
we propose a new architecture that combines reinforcement learning and
adversarial generative networks to enhance the sequence-to-sequence attention
model. First, we use a hybrid pointer-generator network that copies words
directly from the source text, contributing to accurate reproduction of
information without sacrificing the ability of generators to generate new
words. Second, we use both intra-temporal and intra-decoder attention to
penalize summarized content and thus discourage repetition. We apply our model
to our own proposed COVID-19 paper title summarization task and achieve close
approximations to the current model on ROUEG, while bringing better
readability.
",5.0
57,10,107915,2001.05399,web archive,"Nick Ruest, Jimmy Lin, Ian Milligan, and Samantha Fritz","The Archives Unleashed Project: Technology, Process, and Community to
  Improve Scholarly Access to Web Archives","  The Archives Unleashed project aims to improve scholarly access to web
archives through a multi-pronged strategy involving tool creation, process
modeling, and community building - all proceeding concurrently in
mutually-reinforcing efforts. As we near the end of our initially-conceived
three-year project, we report on our progress and share lessons learned along
the way. The main contribution articulated in this paper is a process model
that decomposes scholarly inquiries into four main activities: filter, extract,
aggregate, and visualize. Based on the insight that these activities can be
disaggregated across time, space, and tools, it is possible to generate
""derivative products"", using our Archives Unleashed Toolkit, that serve as
useful starting points for scholarly inquiry. Scholars can download these
products from the Archives Unleashed Cloud and manipulate them just like any
other dataset, thus providing access to web archives without requiring any
specialized knowledge. Over the past few years, our platform has processed over
a thousand different collections from about two hundred users, totaling over
280 terabytes of web archives.
",5.0
58,9,51465,1703.05706,language model for long documents,"Myungha Jang, Jinho D. Choi, James Allan",Improving Document Clustering by Eliminating Unnatural Language,"  Technical documents contain a fair amount of unnatural language, such as
tables, formulas, pseudo-codes, etc. Unnatural language can be an important
factor of confusing existing NLP tools. This paper presents an effective method
of distinguishing unnatural language from natural language, and evaluates the
impact of unnatural language detection on NLP tasks such as document
clustering. We view this problem as an information extraction task and build a
multiclass classification model identifying unnatural language components into
four categories. First, we create a new annotated corpus by collecting slides
and papers in various formats, PPT, PDF, and HTML, where unnatural language
components are annotated into four categories. We then explore features
available from plain text to build a statistical model that can handle any
format as long as it is converted into plain text. Our experiments show that
removing unnatural language components gives an absolute improvement in
document clustering up to 15%. Our corpus and tool are publicly available.
",2.0
59,19,164562,2107.1132,artificial intelligence for low carbon,"Gyri Reiersen, David Dao, Bj\""orn L\""utjens, Konstantin Klemmer,
  Xiaoxiang Zhu, and Ce Zhang","Tackling the Overestimation of Forest Carbon with Deep Learning and
  Aerial Imagery","  Forest carbon offsets are increasingly popular and can play a significant
role in financing climate mitigation, forest conservation, and reforestation.
Measuring how much carbon is stored in forests is, however, still largely done
via expensive, time-consuming, and sometimes unaccountable field measurements.
To overcome these limitations, many verification bodies are leveraging machine
learning (ML) algorithms to estimate forest carbon from satellite or aerial
imagery. Aerial imagery allows for tree species or family classification, which
improves the satellite imagery-based forest type classification. However,
aerial imagery is significantly more expensive to collect and it is unclear by
how much the higher resolution improves the forest carbon estimation. This
proposal paper describes the first systematic comparison of forest carbon
estimation from aerial imagery, satellite imagery, and ground-truth field
measurements via deep learning-based algorithms for a tropical reforestation
project. Our initial results show that forest carbon estimates from satellite
imagery can overestimate above-ground biomass by up to 10-times for tropical
reforestation projects. The significant difference between aerial and
satellite-derived forest carbon measurements shows the potential for aerial
imagery-based ML algorithms and raises the importance to extend this study to a
global benchmark between options for carbon measurements.
",5.0
60,0,84662,1903.08504,learning to rank with partitioned preference,"Cl\'audio Rebelo de S\'a and Paulo Azevedo and Carlos Soares and
  Al\'ipio M\'ario Jorge and Arno Knobbe","Preference rules for label ranking: Mining patterns in multi-target
  relations","  In this paper we investigate two variants of association rules for preference
data, Label Ranking Association Rules and Pairwise Association Rules. Label
Ranking Association Rules (LRAR) are the equivalent of Class Association Rules
(CAR) for the Label Ranking task. In CAR, the consequent is a single class, to
which the example is expected to belong to. In LRAR, the consequent is a
ranking of the labels. The generation of LRAR requires special support and
confidence measures to assess the similarity of rankings. In this work, we
carry out a sensitivity analysis of these similarity-based measures. We want to
understand which datasets benefit more from such measures and which parameters
have more influence in the accuracy of the model. Furthermore, we propose an
alternative type of rules, the Pairwise Association Rules (PAR), which are
defined as association rules with a set of pairwise preferences in the
consequent. While PAR can be used both as descriptive and predictive models,
they are essentially descriptive models. Experimental results show the
potential of both approaches.
",1.0
61,18,193171,2204.09412,infomation retrieval time complexity,Meng Huang and Zhiqiang Xu,Strong convexity of affine phase retrieval,"  The recovery of a signal from the intensity measurements with some entries
being known in advance is termed as {\em affine phase retrieval}. In this
paper, we prove that a natural least squares formulation for the affine phase
retrieval is strongly convex on the entire space under some mild conditions,
provided the measurements are complex Gaussian random vecotrs and the
measurement number $m \gtrsim d \log d$ where $d$ is the dimension of signals.
Based on the result, we prove that the simple gradient descent method for the
affine phase retrieval converges linearly to the target solution with high
probability from an arbitrary initial point. These results show an essential
difference between the affine phase retrieval and the classical phase
retrieval, where the least squares formulations for the classical phase
retrieval are non-convex.
",0.0
62,12,71912,1808.02191,COVID-19 and social media,"Ghazaleh Beigi, Huan Liu","Privacy in Social Media: Identification, Mitigation and Applications","  The increasing popularity of social media has attracted a huge number of
people to participate in numerous activities on a daily basis. This results in
tremendous amounts of rich user-generated data. This data provides
opportunities for researchers and service providers to study and better
understand users' behaviors and further improve the quality of the personalized
services. Publishing user-generated data risks exposing individuals' privacy.
Users privacy in social media is an emerging task and has attracted increasing
attention in recent years. These works study privacy issues in social media
from the two different points of views: identification of vulnerabilities, and
mitigation of privacy risks. Recent research has shown the vulnerability of
user-generated data against the two general types of attacks, identity
disclosure and attribute disclosure. These privacy issues mandate social media
data publishers to protect users' privacy by sanitizing user-generated data
before publishing it. Consequently, various protection techniques have been
proposed to anonymize user-generated social media data. There is a vast
literature on privacy of users in social media from many perspectives. In this
survey, we review the key achievements of user privacy in social media. In
particular, we review and compare the state-of-the-art algorithms in terms of
the privacy leakage attacks and anonymization algorithms. We overview the
privacy risks from different aspects of social media and categorize the
relevant works into five groups 1) graph data anonymization and
de-anonymization, 2) author identification, 3) profile attribute disclosure, 4)
user location and privacy, and 5) recommender systems and privacy issues. We
also discuss open problems and future research directions for user privacy
issues in social media.
",2.0
63,0,37434,1511.01282,learning to rank with partitioned preference,Phong Nguyen and Jun Wang and Alexandros Kalousis,Factorizing LambdaMART for cold start recommendations,"  Recommendation systems often rely on point-wise loss metrics such as the mean
squared error. However, in real recommendation settings only few items are
presented to a user. This observation has recently encouraged the use of
rank-based metrics. LambdaMART is the state-of-the-art algorithm in learning to
rank which relies on such a metric. Despite its success it does not have a
principled regularization mechanism relying in empirical approaches to control
model complexity leaving it thus prone to overfitting.
  Motivated by the fact that very often the users' and items' descriptions as
well as the preference behavior can be well summarized by a small number of
hidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization
(LambdaMART-MF), that learns a low rank latent representation of users and
items using gradient boosted trees. The algorithm factorizes lambdaMART by
defining relevance scores as the inner product of the learned representations
of the users and items. The low rank is essentially a model complexity
controller; on top of it we propose additional regularizers to constraint the
learned latent representations that reflect the user and item manifolds as
these are defined by their original feature based descriptors and the
preference behavior. Finally we also propose to use a weighted variant of NDCG
to reduce the penalty for similar items with large rating discrepancy.
  We experiment on two very different recommendation datasets, meta-mining and
movies-users, and evaluate the performance of LambdaMART-MF, with and without
regularization, in the cold start setting as well as in the simpler matrix
completion setting. In both cases it outperforms in a significant manner
current state of the art algorithms.
",1.0
64,10,209744,2209.08649,web archive,"Himarsha R. Jayanetti, Shawn M. Jones, Martin Klein, Alex Osbourne,
  Paul Koerbin, Michael L. Nelson, Michele C. Weigle","Creating Structure in Web Archives With Collections: Different Concepts
  From Web Archivists","  As web archives' holdings grow, archivists subdivide them into collections so
they are easier to understand and manage. In this work, we review the
collection structures of eight web archive platforms: : Archive-It, Conifer,
the Croatian Web Archive (HAW), the Internet Archive's user account web
archives, Library of Congress (LC), PANDORA, Trove, and the UK Web Archive
(UKWA). We note a plethora of different approaches to web archive collection
structures. Some web archive collections support sub-collections and some
permit embargoes. Curatorial decisions may be attributed to a single
organization or many. Archived web pages are known by many names: mementos,
copies, captures, or snapshots. Some platforms restrict a memento to a single
collection and others allow mementos to cross collections. Knowledge of
collection structures has implications for many different applications and
users. Visitors will need to understand how to navigate collections. Future
archivists will need to understand what options are available for designing
collections. Platform designers need it to know what possibilities exist. The
developers of tools that consume collections need to understand collection
structures so they can meet the needs of their users.
",5.0
65,15,85481,1904.02037,relevance feedback for imformation retrieval,"Sebasti\~ao Miranda and David Nogueira and Afonso Mendes and Andreas
  Vlachos and Andrew Secker and Rebecca Garrett and Jeff Mitchel and Zita
  Marinho",Automated Fact Checking in the News Room,"  Fact checking is an essential task in journalism; its importance has been
highlighted due to recently increased concerns and efforts in combating
misinformation. In this paper, we present an automated fact-checking platform
which given a claim, it retrieves relevant textual evidence from a document
collection, predicts whether each piece of evidence supports or refutes the
claim, and returns a final verdict. We describe the architecture of the system
and the user interface, focusing on the choices made to improve its
user-friendliness and transparency. We conduct a user study of the
fact-checking platform in a journalistic setting: we integrated it with a
collection of news articles and provide an evaluation of the platform using
feedback from journalists in their workflow. We found that the predictions of
our platform were correct 58\% of the time, and 59\% of the returned evidence
was relevant.
",1.0
66,11,172277,2110.04767,PageRank for web search,"Ikechukwu Onyenwe, Stanley Ogbonna, Ebele Onyedimma, Onyedikachukwu
  Ikechukwu-Onyenwe, Chidinma Nwafor",Developing Smart Web-Search Using RegEx,"  Due to the increasing storage data on Web Applications, it becomes very
difficult to use only keyword-based searches to provide comprehensive search
results, thus increasing the difficulty for web users to search information on
the web. In this paper, we proposed using a combined method of keyword-based
and Regular expressions (regEx) searches to perform a search using strings of
targeted items for optimal results even as the volume of data around the world
on the Internet continues to explode. The idea is to embed regEx patterns as
part of the search engine's algorithm in a web application project to provide
strings related to the targeted items for more comprehensive coverage of search
results. The user's search query is a string of characters guided by search
boundaries selected from the entry point. The results returned from the search
operation are different results within a category determined by the search
boundaries. This is designed to be beneficial to a user who has an obscure idea
about the information he/she wanted to search but knows the boundaries within
which to get the information. This technique can be applied to data processing
tasks such as information extraction and search refinement.
",1.0
67,8,148300,2102.12926,node embedding for graph,"Mustafa Hajij, Ghada Zamzmi, Xuanting Cai",Persistent Homology and Graphs Representation Learning,"  This article aims to study the topological invariant properties encoded in
node graph representational embeddings by utilizing tools available in
persistent homology. Specifically, given a node embedding representation
algorithm, we consider the case when these embeddings are real-valued. By
viewing these embeddings as scalar functions on a domain of interest, we can
utilize the tools available in persistent homology to study the topological
information encoded in these representations. Our construction effectively
defines a unique persistence-based graph descriptor, on both the graph and node
levels, for every node representation algorithm. To demonstrate the
effectiveness of the proposed method, we study the topological descriptors
induced by DeepWalk, Node2Vec and Diff2Vec.
",4.0
68,0,89996,1905.13612,learning to rank with partitioned preference,Dimitrios Rafailidis,Leveraging Trust and Distrust in Recommender Systems via Deep Learning,"  The data scarcity of user preferences and the cold-start problem often appear
in real-world applications and limit the recommendation accuracy of
collaborative filtering strategies. Leveraging the selections of social friends
and foes can efficiently face both problems. In this study, we propose a
strategy that performs social deep pairwise learning. Firstly, we design a
ranking loss function incorporating multiple ranking criteria based on the
choice in users, and the choice in their friends and foes to improve the
accuracy in the top-k recommendation task. We capture the nonlinear
correlations between user preferences and the social information of trust and
distrust relationships via a deep learning strategy. In each backpropagation
step, we follow a social negative sampling strategy to meet the multiple
ranking criteria of our ranking loss function. We conduct comprehensive
experiments on a benchmark dataset from Epinions, among the largest publicly
available that has been reported in the relevant literature. The experimental
results demonstrate that the proposed model beats other state-of-the art
methods, attaining an 11.49% average improvement over the most competitive
model. We show that our deep learning strategy plays an important role in
capturing the nonlinear correlations between user preferences and the social
information of trust and distrust relationships, and demonstrate the importance
of our social negative sampling strategy on the proposed model.
",1.0
69,3,30121,1411.7343,database management system,A B M Moniruzzaman,"NewSQL: Towards Next-Generation Scalable RDBMS for Online Transaction
  Processing (OLTP) for Big Data Management","  One of the key advances in resolving the big-data problem has been the
emergence of an alternative database technology. Today, classic RDBMS are
complemented by a rich set of alternative Data Management Systems (DMS)
specially designed to handle the volume, variety, velocity and variability
ofBig Data collections; these DMS include NoSQL, NewSQL and Search-based
systems. NewSQL is a class of modern relational database management systems
(RDBMS) that provide the same scalable performance of NoSQL systems for online
transaction processing (OLTP) read-write workloads while still maintaining the
ACID guarantees of a traditional database system. This paper discusses about
NewSQL data management system; and compares with NoSQL and with traditional
database system. This paper covers architecture, characteristics,
classification of NewSQL databases for online transaction processing (OLTP) for
Big data management. It also provides the list ofpopular NoSQL as well as
NewSQL databases in separate categorized tables. This paper compares SQL based
RDBMS, NoSQL and NewSQL databases with set of metrics; as well as, addressed
some research issues ofNoSQL and NewSQL.
",5.0
70,4,127391,2008.00032,pre-trained language model,"Cristina Zuheros, Eugenio Mart\'inez-C\'amara, Enrique Herrera-Viedma,
  and Francisco Herrera","Sentiment Analysis based Multi-person Multi-criteria Decision Making
  Methodology using Natural Language Processing and Deep Learning for Smarter
  Decision Aid. Case study of restaurant choice using TripAdvisor reviews","  Decision making models are constrained by taking the expert evaluations with
pre-defined numerical or linguistic terms. We claim that the use of sentiment
analysis will allow decision making models to consider expert evaluations in
natural language. Accordingly, we propose the Sentiment Analysis based
Multi-person Multi-criteria Decision Making (SA-MpMcDM) methodology for smarter
decision aid, which builds the expert evaluations from their natural language
reviews, and even from their numerical ratings if they are available. The
SA-MpMcDM methodology incorporates an end-to-end multi-task deep learning model
for aspect based sentiment analysis, named DOC-ABSADeepL model, able to
identify the aspect categories mentioned in an expert review, and to distill
their opinions and criteria. The individual evaluations are aggregated via the
procedure named criteria weighting through the attention of the experts. We
evaluate the methodology in a case study of restaurant choice using TripAdvisor
reviews, hence we build, manually annotate, and release the TripR-2020 dataset
of restaurant reviews. We analyze the SA-MpMcDM methodology in different
scenarios using and not using natural language and numerical evaluations. The
analysis shows that the combination of both sources of information results in a
higher quality preference vector.
",2.0
71,4,201222,2206.11862,pre-trained language model,"Syed Zain Abbas, Dr. Arif ur Rahman, Abdul Basit Mughal, Syed Mujtaba
  Haider","Urdu News Article Recommendation Model using Natural Language Processing
  Techniques","  There are several online newspapers in urdu but for the users it is difficult
to find the content they are looking for because these most of them contain
irrelevant data and most users did not get what they want to retrieve. Our
proposed framework will help to predict Urdu news in the interests of users and
reduce the users searching time for news. For this purpose, NLP techniques are
used for pre-processing, and then TF-IDF with cosine similarity is used for
gaining the highest similarity and recommended news on user preferences.
Moreover, the BERT language model is also used for similarity, and by using the
BERT model similarity increases as compared to TF-IDF so the approach works
better with the BERT language model and recommends news to the user on their
interest. The news is recommended when the similarity of the articles is above
60 percent.
",4.0
72,6,176720,2111.08229,query expansion for imformation retrieval,"Handong Ma, Jiawei Hou, Chenxu Zhu, Weinan Zhang, Ruiming Tang, Jincai
  Lai, Jieming Zhu, Xiuqiang He, and Yong Yu","QA4PRF: A Question Answering based Framework for Pseudo Relevance
  Feedback","  Pseudo relevance feedback (PRF) automatically performs query expansion based
on top-retrieved documents to better represent the user's information need so
as to improve the search results. Previous PRF methods mainly select expansion
terms with high occurrence frequency in top-retrieved documents or with high
semantic similarity with the original query. However, existing PRF methods
hardly try to understand the content of documents, which is very important in
performing effective query expansion to reveal the user's information need. In
this paper, we propose a QA-based framework for PRF called QA4PRF to utilize
contextual information in documents. In such a framework, we formulate PRF as a
QA task, where the query and each top-retrieved document play the roles of
question and context in the corresponding QA system, while the objective is to
find some proper terms to expand the original query by utilizing contextual
information, which are similar answers in QA task. Besides, an attention-based
pointer network is built on understanding the content of top-retrieved
documents and selecting the terms to represent the original query better. We
also show that incorporating the traditional supervised learning methods, such
as LambdaRank, to integrate PRF information will further improve the
performance of QA4PRF. Extensive experiments on three real-world datasets
demonstrate that QA4PRF significantly outperforms the state-of-the-art methods.
",3.0
73,15,18944,1303.525,relevance feedback for imformation retrieval,Marc Sloan and Jun Wang,Iterative Expectation for Multi Period Information Retrieval,"  Many Information Retrieval (IR) models make use of offline statistical
techniques to score documents for ranking over a single period, rather than use
an online, dynamic system that is responsive to users over time. In this paper,
we explicitly formulate a general Multi Period Information Retrieval problem,
where we consider retrieval as a stochastic yet controllable process. The
ranking action during the process continuously controls the retrieval system's
dynamics, and an optimal ranking policy is found in order to maximise the
overall users' satisfaction over the multiple periods as much as possible. Our
derivations show interesting properties about how the posterior probability of
the documents relevancy evolves from users feedbacks through clicks, and
provides a plug-in framework for incorporating different click models. Based on
the Multi-Armed Bandit theory, we propose a simple implementation of our
framework using a dynamic ranking rule that takes rank bias and exploration of
documents into account. We use TREC data to learn a suitable exploration
parameter for our model, and then analyse its performance and a number of
variants using a search log data set; the experiments suggest an ability to
explore document relevance dynamically over time using user feedback in a way
that can handle rank bias.
",1.0
74,6,72730,1808.09353,query expansion for imformation retrieval,"Morgan Gallant, Haruna Isah, Farhana Zulkernine, Shahzad Khan",Xu: An Automated Query Expansion and Optimization Tool,"  The exponential growth of information on the Internet is a big challenge for
information retrieval systems towards generating relevant results. Novel
approaches are required to reformat or expand user queries to generate a
satisfactory response and increase recall and precision. Query expansion (QE)
is a technique to broaden users' queries by introducing additional tokens or
phrases based on some semantic similarity metrics. The tradeoff is the added
computational complexity to find semantically similar words and a possible
increase in noise in information retrieval. Despite several research efforts on
this topic, QE has not yet been explored enough and more work is needed on
similarity matching and composition of query terms with an objective to
retrieve a small set of most appropriate responses. QE should be scalable,
fast, and robust in handling complex queries with a good response time and
noise ceiling. In this paper, we propose Xu, an automated QE technique, using
high dimensional clustering of word vectors and Datamuse API, an open source
query engine to find semantically similar words. We implemented Xu as a command
line tool and evaluated its performances using datasets containing news
articles and human-generated QEs. The evaluation results show that Xu was
better than Datamuse by achieving about 88% accuracy with reference to the
human-generated QE.
",5.0
75,10,39282,1601.05142,web archive,Justin F. Brunelle and Michele C. Weigle and Michael L. Nelson,"Adapting the Hypercube Model to Archive Deferred Representations and
  Their Descendants","  The web is today's primary publication medium, making web archiving an
important activity for historical and analytical purposes. Web pages are
increasingly interactive, resulting in pages that are increasingly difficult to
archive. Client-side technologies (e.g., JavaScript) enable interactions that
can potentially change the client-side state of a representation. We refer to
representations that load embedded resources via JavaScript as deferred
representations. It is difficult to archive all of the resources in deferred
representations and the result is archives with web pages that are either
incomplete or that erroneously load embedded resources from the live web.
  We propose a method of discovering and crawling deferred representations and
their descendants (representation states that are only reachable through
client-side events). We adapt the Dincturk et al. Hypercube model to construct
a model for archiving descendants, and we measure the number of descendants and
requisite embedded resources discovered in a proof-of-concept crawl. Our
approach identified an average of 38.5 descendants per seed URI crawled, 70.9%
of which are reached through an onclick event. This approach also added 15.6
times more embedded resources than Heritrix to the crawl frontier, but at a
rate that was 38.9 times slower than simply using Heritrix. We show that our
dataset has two levels of descendants. We conclude with proposed crawl policies
and an analysis of the storage requirements for archiving descendants.
",4.0
76,2,11588,1202.1121,random forests,Miron B. Kursa,"rFerns: An Implementation of the Random Ferns Method for General-Purpose
  Machine Learning","  In this paper I present an extended implementation of the Random ferns
algorithm contained in the R package rFerns. It differs from the original by
the ability of consuming categorical and numerical attributes instead of only
binary ones. Also, instead of using simple attribute subspace ensemble it
employs bagging and thus produce error approximation and variable importance
measure modelled after Random forest algorithm. I also present benchmarks'
results which show that although Random ferns' accuracy is mostly smaller than
achieved by Random forest, its speed and good quality of importance measure it
provides make rFerns a reasonable choice for a specific applications.
",2.0
77,1,26298,1404.7045,advanced search engine,"Enrique Orduna-Malea, Juan Manuel Ayllon, Alberto Martin-Martin,
  Emilio Delgado Lopez-Cozar","Empirical Evidences in Citation-Based Search Engines: Is Microsoft
  Academic Search dead?","  The goal of this working paper is to summarize the main empirical evidences
provided by the scientific community as regards the comparison between the two
main citation based academic search engines: Google Scholar and Microsoft
Academic Search, paying special attention to the following issues: coverage,
correlations between journal rankings, and usage of these academic search
engines. Additionally, selfelaborated data is offered, which are intended to
provide current evidence about the popularity of these tools on the Web, by
measuring the number of rich files PDF, PPT and DOC in which these tools are
mentioned, the amount of external links that both products receive, and the
search queries frequency from Google Trends. The poor results obtained by MAS
led us to an unexpected and unnoticed discovery: Microsoft Academic Search is
outdated since 2013. Therefore, the second part of the working paper aims at
advancing some data demonstrating this lack of update. For this purpose we
gathered the number of total records indexed by Microsoft Academic Search since
2000. The data shows an abrupt drop in the number of documents indexed from
2,346,228 in 2010 to 8,147 in 2013 and 802 in 2014. This decrease is offered
according to 15 thematic areas as well. In view of these problems it seems
logical not only that Microsoft Academic Searchwas poorly used to search for
articles by academics and students, who mostly use Google or Google Scholar,
but virtually ignored by bibliometricians
",3.0
78,13,29213,1410.2759,social network analysis with natrual language processing,"Johanna Hardin, Ghassan Sarkis, and P.C. Urc",Network Analysis with the Enron Email Corpus,"  We use the Enron email corpus to study relationships in a network by applying
six different measures of centrality. Our results came out of an in-semester
undergraduate research seminar. The Enron corpus is well suited to statistical
analyses at all levels of undergraduate education. Through this note's focus on
centrality, students can explore the dependence of statistical models on
initial assumptions and the interplay between centrality measures and
hierarchical ranking, and they can use completed studies as springboards for
future research. The Enron corpus also presents opportunities for research into
many other areas of analysis, including social networks, clustering, and
natural language processing.
",4.0
79,4,36844,1510.01562,pre-trained language model,Benjamin Piwowarski and Sylvain Lamprier and Nicolas Despres,Parameterized Neural Network Language Models for Information Retrieval,"  Information Retrieval (IR) models need to deal with two difficult issues,
vocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to
the difficulty of retrieving relevant documents that do not contain exact query
terms but semantically related terms. Term dependencies refers to the need of
considering the relationship between the words of the query when estimating the
relevance of a document. A multitude of solutions has been proposed to solve
each of these two problems, but no principled model solve both. In parallel, in
the last few years, language models based on neural networks have been used to
cope with complex natural language processing tasks like emotion and paraphrase
detection. Although they present good abilities to cope with both term
dependencies and vocabulary mismatch problems, thanks to the distributed
representation of words they are based upon, such models could not be used
readily in IR, where the estimation of one language model per document (or
query) is required. This is both computationally unfeasible and prone to
over-fitting. Based on a recent work that proposed to learn a generic language
model that can be modified through a set of document-specific parameters, we
explore use of new neural network models that are adapted to ad-hoc IR tasks.
Within the language model IR framework, we propose and study the use of a
generic language model as well as a document-specific language model. Both can
be used as a smoothing component, but the latter is more adapted to the
document at hand and has the potential of being used as a full document
language model. We experiment with such models and analyze their results on
TREC-1 to 8 datasets.
",4.0
80,18,88057,1905.04611,infomation retrieval time complexity,Tatsuki Sekino,"Data description and retrieval using periods represented by uncertain
  time intervals","  Time periods are frequently used to specify time in metadata and retrieval.
However, it is not easy to describe and retrieve information about periods,
because the temporal ranges represented by periods are often ambiguous. This is
because these temporal ranges do not have fixed beginning and end points. To
solve this problem, basic logics to describe and process uncertain time
intervals were developed in this study. An uncertain time interval is
represented as a set of time intervals that indicate states when the uncertain
time interval is determined. Based on this concept, a logic to retrieve
uncertain time intervals satisfying a given condition was established, and it
was revealed that retrieval results belong to three states: reliable,
impossible, and possible matches. Additionally, to describe data about
uncertain periods, an ontology (the HuTime Ontology) was constructed based on
the logic. This ontology is characterized by the fact that uncertain time
intervals can be defined recursively. It is expected that more data about time
periods will be created and released using the result of this study.
",1.0
81,14,152015,2104.00782,text summarization model,"Peter Jachim, Filipo Sharevski, Emma Pieroni","""TL;DR:"" Out-of-Context Adversarial Text Summarization and Hashtag
  Recommendation","  This paper presents Out-of-Context Summarizer, a tool that takes arbitrary
public news articles out of context by summarizing them to coherently fit
either a liberal- or conservative-leaning agenda. The Out-of-Context Summarizer
also suggests hashtag keywords to bolster the polarization of the summary, in
case one is inclined to take it to Twitter, Parler or other platforms for
trolling. Out-of-Context Summarizer achieved 79% precision and 99% recall when
summarizing COVID-19 articles, 93% precision and 93% recall when summarizing
politically-centered articles, and 87% precision and 88% recall when taking
liberally-biased articles out of context. Summarizing valid sources instead of
synthesizing fake text, the Out-of-Context Summarizer could fairly pass the
""adversarial disclosure"" test, but we didn't take this easy route in our paper.
Instead, we used the Out-of-Context Summarizer to push the debate of potential
misuse of automated text generation beyond the boilerplate text of responsible
disclosure of adversarial language models.
",4.0
82,18,151764,2103.16669,infomation retrieval time complexity,Koustav Rudra and Zeon Trevor Fernando and Avishek Anand,"An In-depth Analysis of Passage-Level Label Transfer for Contextual
  Document Ranking","  Recently introduced pre-trained contextualized autoregressive models like
BERT have shown improvements in document retrieval tasks. One of the major
limitations of the current approaches can be attributed to the manner they deal
with variable-size document lengths using a fixed input BERT model. Common
approaches either truncate or split longer documents into small
sentences/passages and subsequently label them - using the original document
label or from another externally trained model. In this paper, we conduct a
detailed study of the design decisions about splitting and label transfer on
retrieval effectiveness and efficiency. We find that direct transfer of
relevance labels from documents to passages introduces label noise that
strongly affects retrieval effectiveness for large training datasets. We also
find that query processing times are adversely affected by fine-grained
splitting schemes. As a remedy, we propose a careful passage level labelling
scheme using weak supervision that delivers improved performance (3-14% in
terms of nDCG score) over most of the recently proposed models for ad-hoc
retrieval while maintaining manageable computational complexity on four diverse
document retrieval datasets.
",3.0
83,19,177357,2111.11295,artificial intelligence for low carbon,"Yongmin Yoo, Dongjin Lim, Kyungsun Kim","Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","  Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems.
",1.0
84,13,42820,1605.0841,social network analysis with natrual language processing,"Yang Yang, Nitesh V. Chawla, Ryan N. Lichtenwalter, Yuxiao Dong","Influence Activation Model: A New Perspective in Social Influence
  Analysis and Social Network Evolution","  What drives the propensity for the social network dynamics? Social influence
is believed to drive both off-line and on-line human behavior, however it has
not been considered as a driver of social network evolution. Our analysis
suggest that, while the network structure affects the spread of influence in
social networks, the network is in turn shaped by social influence activity
(i.e., the process of social influence wherein one person's attitudes and
behaviors affect another's). To that end, we develop a novel model of network
evolution where the dynamics of network follow the mechanism of influence
propagation, which are not captured by the existing network evolution models.
Our experiments confirm the predictions of our model and demonstrate the
important role that social influence can play in the process of network
evolution. As well exploring the reason of social network evolution, different
genres of social influence have been spotted having different effects on the
network dynamics. These findings and methods are essential to both our
understanding of the mechanisms that drive network evolution and our knowledge
of the role of social influence in shaping the network structure.
",1.0
85,6,103466,1911.07317,query expansion for imformation retrieval,"Philippe Mulhem and Lorraine Goeuriot and Massih-Reza Amini and
  Nayanika Dogra","Quels corpus d'entra\^inement pour l'expansion de requ\^etes par
  plongement de mots : application \`a la recherche de microblogs culturels","  We describe here an experimental framework and the results obtained on
microblogs retrieval. We study the contribution one popular approach, i.e.,
words embeddings, and investigate the impact of the training set on the learned
embedding. We focus on query expansion for the retrieval of tweets on the CLEF
CMC 2016 corpus. Our results show that using embeddings trained on a corpus in
the same domain as the indexed documents did not necessarily lead to better
retrieval results.
",3.0
86,17,82715,1902.05983,robustness of neutral networks,"Ravi Mangal, Aditya V. Nori, Alessandro Orso",Robustness of Neural Networks: A Probabilistic and Practical Approach,"  Neural networks are becoming increasingly prevalent in software, and it is
therefore important to be able to verify their behavior. Because verifying the
correctness of neural networks is extremely challenging, it is common to focus
on the verification of other properties of these systems. One important
property, in particular, is robustness. Most existing definitions of
robustness, however, focus on the worst-case scenario where the inputs are
adversarial. Such notions of robustness are too strong, and unlikely to be
satisfied by-and verifiable for-practical neural networks. Observing that
real-world inputs to neural networks are drawn from non-adversarial probability
distributions, we propose a novel notion of robustness: probabilistic
robustness, which requires the neural network to be robust with at least $(1 -
\epsilon)$ probability with respect to the input distribution. This
probabilistic approach is practical and provides a principled way of estimating
the robustness of a neural network. We also present an algorithm, based on
abstract interpretation and importance sampling, for checking whether a neural
network is probabilistically robust. Our algorithm uses abstract interpretation
to approximate the behavior of a neural network and compute an
overapproximation of the input regions that violate robustness. It then uses
importance sampling to counter the effect of such overapproximation and compute
an accurate estimate of the probability that the neural network violates the
robustness property.
",5.0
87,3,70899,1807.05308,database management system,"Jeremy Kepner, Ron Brightwell, Alan Edelman, Vijay Gadepally, Hayden
  Jananthan, Michael Jones, Sam Madden, Peter Michaleas, Hamed Okhravi, Kevin
  Pedretti, Albert Reuther, Thomas Sterling, Mike Stonebraker","TabulaROSA: Tabular Operating System Architecture for Massively Parallel
  Heterogeneous Compute Engines","  The rise in computing hardware choices is driving a reevaluation of operating
systems. The traditional role of an operating system controlling the execution
of its own hardware is evolving toward a model whereby the controlling
processor is distinct from the compute engines that are performing most of the
computations. In this context, an operating system can be viewed as software
that brokers and tracks the resources of the compute engines and is akin to a
database management system. To explore the idea of using a database in an
operating system role, this work defines key operating system functions in
terms of rigorous mathematical semantics (associative array algebra) that are
directly translatable into database operations. These operations possess a
number of mathematical properties that are ideal for parallel operating systems
by guaranteeing correctness over a wide range of parallel operations. The
resulting operating system equations provide a mathematical specification for a
Tabular Operating System Architecture (TabulaROSA) that can be implemented on
any platform. Simulations of forking in TabularROSA are performed using an
associative array implementation and compared to Linux on a 32,000+ core
supercomputer. Using over 262,000 forkers managing over 68,000,000,000
processes, the simulations show that TabulaROSA has the potential to perform
operating system functions on a massively parallel scale. The TabulaROSA
simulations show 20x higher performance as compared to Linux while managing
2000x more processes in fully searchable tables.
",2.0
88,6,12645,1204.3471,query expansion for imformation retrieval,Arockia Anand Raj and T. Mala,Cloudpress 2.0: A MapReduce Approach for News Retrieval on the Cloud,"  In this era of the Internet, the amount of news articles added every minute
of everyday is humongous. As a result of this explosive amount of news
articles, news retrieval systems are required to process the news articles
frequently and intensively. The news retrieval systems that are in-use today
are not capable of coping up with these data-intensive computations. Cloudpress
2.0 presented here, is designed and implemented to be scalable, robust and
fault tolerant. It is designed in such a way that, all the processes involved
in news retrieval such as fetching, pre-processing, indexing, storing and
summarizing, exploit MapReduce paradigm and use the power of the Cloud
computing. It uses novel approaches for parallel processing, for storing the
news articles in a distributed database and for visualizing them as a 3D
visual. It uses Lucene-based indexing for efficient and faster retrieval. It
also includes a novel query expansion feature for searching the news articles.
Cloudpress 2.0 also allows on-the-fly, extractive summarization of news
articles based on the input query.
",1.0
89,7,137587,2011.04998,gradient boosting,"Allan Gr{\o}nlund, Lior Kamma, Kasper Green Larsen",Margins are Insufficient for Explaining Gradient Boosting,"  Boosting is one of the most successful ideas in machine learning, achieving
great practical performance with little fine-tuning. The success of boosted
classifiers is most often attributed to improvements in margins. The focus on
margin explanations was pioneered in the seminal work by Schapire et al. (1998)
and has culminated in the $k$'th margin generalization bound by Gao and Zhou
(2013), which was recently proved to be near-tight for some data distributions
(Gronlund et al. 2019). In this work, we first demonstrate that the $k$'th
margin bound is inadequate in explaining the performance of state-of-the-art
gradient boosters. We then explain the short comings of the $k$'th margin bound
and prove a stronger and more refined margin-based generalization bound for
boosted classifiers that indeed succeeds in explaining the performance of
modern gradient boosters. Finally, we improve upon the recent generalization
lower bound by Gr{\o}nlund et al. (2019).
",5.0
90,6,116585,2004.11093,query expansion for imformation retrieval,"Bhawani Selvaretnam, Mohammed Belkhatir","Natural language technology and query expansion: issues,
  state-of-the-art and perspectives","  The availability of an abundance of knowledge sources has spurred a large
amount of effort in the development and enhancement of Information Retrieval
techniques. Users information needs are expressed in natural language and
successful retrieval is very much dependent on the effective communication of
the intended purpose. Natural language queries consist of multiple linguistic
features which serve to represent the intended search goal. Linguistic
characteristics that cause semantic ambiguity and misinterpretation of queries
as well as additional factors such as the lack of familiarity with the search
environment affect the users ability to accurately represent their information
needs, coined by the concept intention gap. The latter directly affects the
relevance of the returned search results which may not be to the users
satisfaction and therefore is a major issue impacting the effectiveness of
information retrieval systems. Central to our discussion is the identification
of the significant constituents that characterize the query intent and their
enrichment through the addition of meaningful terms, phrases or even latent
representations, either manually or automatically to capture their intended
meaning. Specifically, we discuss techniques to achieve the enrichment and in
particular those utilizing the information gathered from statistical processing
of term dependencies within a document corpus or from external knowledge
sources such as ontologies. We lay down the anatomy of a generic linguistic
based query expansion framework and propose its module-based decomposition,
covering topical issues from query processing, information retrieval,
computational linguistics and ontology engineering. For each of the modules we
review state-of-the-art solutions in the literature categorized and analyzed
under the light of the techniques used.
",4.0
91,14,205102,2207.14687,text summarization model,"Daniel F. O. Onah, Elaine L. L. Pang, Mahmoud El-Haj","A Data-driven Latent Semantic Analysis for Automatic Text Summarization
  using LDA Topic Modelling","  With the advent and popularity of big data mining and huge text analysis in
modern times, automated text summarization became prominent for extracting and
retrieving important information from documents. This research investigates
aspects of automatic text summarization from the perspectives of single and
multiple documents. Summarization is a task of condensing huge text articles
into short, summarized versions. The text is reduced in size for summarization
purpose but preserving key vital information and retaining the meaning of the
original document. This study presents the Latent Dirichlet Allocation (LDA)
approach used to perform topic modelling from summarised medical science
journal articles with topics related to genes and diseases. In this study,
PyLDAvis web-based interactive visualization tool was used to visualise the
selected topics. The visualisation provides an overarching view of the main
topics while allowing and attributing deep meaning to the prevalence individual
topic. This study presents a novel approach to summarization of single and
multiple documents. The results suggest the terms ranked purely by considering
their probability of the topic prevalence within the processed document using
extractive summarization technique. PyLDAvis visualization describes the
flexibility of exploring the terms of the topics' association to the fitted LDA
model. The topic modelling result shows prevalence within topics 1 and 2. This
association reveals that there is similarity between the terms in topic 1 and 2
in this study. The efficacy of the LDA and the extractive summarization methods
were measured using Latent Semantic Analysis (LSA) and Recall-Oriented
Understudy for Gisting Evaluation (ROUGE) metrics to evaluate the reliability
and validity of the model.
",5.0
92,14,215028,2210.14606,text summarization model,"Frederic Kirstein, Jan Philip Wahle, Terry Ruas, Bela Gipp",Analyzing Multi-Task Learning for Abstractive Text Summarization,"  Despite the recent success of multi-task learning and pre-finetuning for
natural language understanding, few works have studied the effects of task
families on abstractive text summarization. Task families are a form of task
grouping during the pre-finetuning stage to learn common skills, such as
reading comprehension. To close this gap, we analyze the influence of
multi-task learning strategies using task families for the English abstractive
text summarization task. We group tasks into one of three strategies, i.e.,
sequential, simultaneous, and continual multi-task learning, and evaluate
trained models through two downstream tasks. We find that certain combinations
of task families (e.g., advanced reading comprehension and natural language
inference) positively impact downstream performance. Further, we find that
choice and combinations of task families influence downstream performance more
than the training scheme, supporting the use of task families for abstractive
text summarization.
",4.0
93,10,89699,1905.12565,web archive,"Mohamed Aturban, Sawood Alam, Michael L. Nelson, Michele C. Weigle",Archive Assisted Archival Fixity Verification Framework,"  The number of public and private web archives has increased, and we
implicitly trust content delivered by these archives. Fixity is checked to
ensure an archived resource has remained unaltered since the time it was
captured. Some web archives do not allow users to access fixity information
and, more importantly, even if fixity information is available, it is provided
by the same archive from which the archived resources are requested. In this
research, we propose two approaches, namely Atomic and Block, to establish and
check fixity of archived resources. In the Atomic approach, the fixity
information of each archived web page is stored in a JSON file (or a manifest),
and published in a well-known web location (an Archival Fixity server) before
it is disseminated to several on-demand web archives. In the Block approach, we
first batch together fixity information of multiple archived pages in a single
binary-searchable file (or a block) before it is published and disseminated to
archives. In both approaches, the fixity information is not obtained directly
from archives. Instead, we compute the fixity information (e.g., hash values)
based on the playback of archived resources. One advantage of the Atomic
approach is the ability to verify fixity of archived pages even with the
absence of the Archival Fixity server. The Block approach requires pushing
fewer resources into archives, and it performs fixity verification faster than
the Atomic approach. On average, it takes about 1.25X, 4X, and 36X longer to
disseminate a manifest to perma.cc, archive.org, and webcitation.org,
respectively, than archive.is, while it takes 3.5X longer to disseminate a
block to archive.org than perma.cc. The Block approach performs 4.46X faster
than the Atomic approach on verifying the fixity of archived pages.
",3.0
94,3,58326,1710.01792,database management system,"Ashish Tapdiya, Yuan Xue, Daniel Fabbri (Vanderbilt University)","A Comparative Analysis of Materialized Views Selection and Concurrency
  Control Mechanisms in NoSQL Databases","  Increasing resource demands require relational databases to scale. While
relational databases are well suited for vertical scaling, specialized hardware
can be expensive. Conversely, emerging NewSQL and NoSQL data stores are
designed to scale horizontally. NewSQL databases provide ACID transaction
support; however, joins are limited to the partition keys, resulting in
restricted query expressiveness. On the other hand, NoSQL databases are
designed to scale out linearly on commodity hardware; however, they are limited
by slow join performance. Hence, we consider if the NoSQL join performance can
be improved while ensuring ACID semantics and without drastically sacrificing
write performance, disk utilization and query expressiveness.
  This paper presents the Synergy system that leverages schema and workload
driven mechanism to identify materialized views and a specialized concurrency
control system on top of a NoSQL database to enable scalable data management
with familiar relational conventions. Synergy trades slight write performance
degradation and increased disk utilization for faster join performance
(compared to standard NoSQL databases) and improved query expressiveness
(compared to NewSQL databases). Experimental results using the TPC-W benchmark
show that, for a database populated with 1M customers, the Synergy system
exhibits a maximum performance improvement of 80.5% as compared to other
evaluated systems.
",5.0
95,4,198120,2205.1593,pre-trained language model,"Sanatbek Matlatipov, Hulkar Rahimboeva, Jaloliddin Rajabov, Elmurod
  Kuriyozov",Uzbek Sentiment Analysis based on local Restaurant Reviews,"  Extracting useful information for sentiment analysis and classification
problems from a big amount of user-generated feedback, such as restaurant
reviews, is a crucial task of natural language processing, which is not only
for customer satisfaction where it can give personalized services, but can also
influence the further development of a company. In this paper, we present a
work done on collecting restaurant reviews data as a sentiment analysis dataset
for the Uzbek language, a member of the Turkic family which is heavily affected
by the low-resource constraint, and provide some further analysis of the novel
dataset by evaluation using different techniques, from logistic regression
based models, to support vector machines, and even deep learning models, such
as recurrent neural networks, as well as convolutional neural networks. The
paper includes detailed information on how the data was collected, how it was
pre-processed for better quality optimization, as well as experimental setups
for the evaluation process. The overall evaluation results indicate that by
performing pre-processing steps, such as stemming for agglutinative languages,
the system yields better results, eventually achieving 91% accuracy result in
the best performing model
",2.0
96,12,25536,1403.3807,COVID-19 and social media,"Bibo Hao, Lin Li, Rui Gao, Ang Li, Tingshao Zhu",Sensing Subjective Well-being from Social Media,"  Subjective Well-being(SWB), which refers to how people experience the quality
of their lives, is of great use to public policy-makers as well as economic,
sociological research, etc. Traditionally, the measurement of SWB relies on
time-consuming and costly self-report questionnaires. Nowadays, people are
motivated to share their experiences and feelings on social media, so we
propose to sense SWB from the vast user generated data on social media. By
utilizing 1785 users' social media data with SWB labels, we train machine
learning models that are able to ""sense"" individual SWB from users' social
media. Our model, which attains the state-by-art prediction accuracy, can then
be used to identify SWB of large population of social media users in time with
very low cost.
",2.0
97,9,8553,1105.2392,language model for long documents,"Serge Autexier, Catalin David, Dominik Dietrich, Michael Kohlhase,
  Vyacheslav Zholudev","Workflows for the Management of Change in Science, Technologies,
  Engineering and Mathematics","  Mathematical knowledge is a central component in science, engineering, and
technology (documentation). Most of it is represented informally, and -- in
contrast to published research mathematics -- subject to continual change.
Unfortunately, machine support for change management has either been very
coarse grained and thus barely useful, or restricted to formal languages, where
automation is possible. In this paper, we report on an effort to extend change
management to collections of semi-formal documents which flexibly intermix
mathematical formulas and natural language and to integrate it into a semantic
publishing system for mathematical knowledge. We validate the long-standing
assumption that the semantic annotations in these flexiformal documents that
drive the machine-supported interaction with documents can support semantic
impact analyses at the same time. But in contrast to the fully formal setting,
where adaptations of impacted documents can be automated to some degree, the
flexiformal setting requires much more user interaction and thus a much tighter
integration into document management workflows.
",1.0
98,16,126590,2007.1173,activation function in neutral networks,"Scott Mahan, Emily King, Alex Cloninger",Nonclosedness of Sets of Neural Networks in Sobolev Spaces,"  We examine the closedness of sets of realized neural networks of a fixed
architecture in Sobolev spaces. For an exactly $m$-times differentiable
activation function $\rho$, we construct a sequence of neural networks
$(\Phi_n)_{n \in \mathbb{N}}$ whose realizations converge in order-$(m-1)$
Sobolev norm to a function that cannot be realized exactly by a neural network.
Thus, sets of realized neural networks are not closed in order-$(m-1)$ Sobolev
spaces $W^{m-1,p}$ for $p \in [1,\infty]$. We further show that these sets are
not closed in $W^{m,p}$ under slightly stronger conditions on the $m$-th
derivative of $\rho$. For a real analytic activation function, we show that
sets of realized neural networks are not closed in $W^{k,p}$ for any $k \in
\mathbb{N}$. The nonclosedness allows for approximation of non-network target
functions with unbounded parameter growth. We partially characterize the rate
of parameter growth for most activation functions by showing that a specific
sequence of realized neural networks can approximate the activation function's
derivative with weights increasing inversely proportional to the $L^p$
approximation error. Finally, we present experimental results showing that
networks are capable of closely approximating non-network target functions with
increasing parameters via training.
",3.0
99,8,141307,2012.08019,node embedding for graph,Mengjia Xu,Understanding graph embedding methods and their applications,"  Graph analytics can lead to better quantitative understanding and control of
complex networks, but traditional methods suffer from high computational cost
and excessive memory requirements associated with the high-dimensionality and
heterogeneous characteristics of industrial size networks. Graph embedding
techniques can be effective in converting high-dimensional sparse graphs into
low-dimensional, dense and continuous vector spaces, preserving maximally the
graph structure properties. Another type of emerging graph embedding employs
Gaussian distribution-based graph embedding with important uncertainty
estimation. The main goal of graph embedding methods is to pack every node's
properties into a vector with a smaller dimension, hence, node similarity in
the original complex irregular spaces can be easily quantified in the embedded
vector spaces using standard metrics. The generated nonlinear and highly
informative graph embeddings in the latent space can be conveniently used to
address different downstream graph analytics tasks (e.g., node classification,
link prediction, community detection, visualization, etc.). In this Review, we
present some fundamental concepts in graph analytics and graph embedding
methods, focusing in particular on random walk-based and neural network-based
methods. We also discuss the emerging deep learning-based dynamic graph
embedding methods. We highlight the distinct advantages of graph embedding
methods in four diverse applications, and present implementation details and
references to open-source software as well as available databases in the
Appendix for the interested readers to start their exploration into graph
analytics.
",3.0
100,13,200842,2206.1028,social network analysis with natrual language processing,"Manish Pathak, Aditya Jain","muBoost: An Effective Method for Solving Indic Multilingual Text
  Classification Problem","  Text Classification is an integral part of many Natural Language Processing
tasks such as sarcasm detection, sentiment analysis and many more such
applications. Many e-commerce websites, social-media/entertainment platforms
use such models to enhance user-experience to generate traffic and thus,
revenue on their platforms. In this paper, we are presenting our solution to
Multilingual Abusive Comment Identification Problem on Moj, an Indian
video-sharing social networking service, powered by ShareChat. The problem
dealt with detecting abusive comments, in 13 regional Indic languages such as
Hindi, Telugu, Kannada etc., on the videos on Moj platform. Our solution
utilizes the novel muBoost, an ensemble of CatBoost classifier models and
Multilingual Representations for Indian Languages (MURIL) model, to produce
SOTA performance on Indic text classification tasks. We were able to achieve a
mean F1-score of 89.286 on the test data, an improvement over baseline MURIL
model with a F1-score of 87.48.
",5.0
101,18,67832,1805.07899,infomation retrieval time complexity,Meng Huang and Zhiqiang Xu,Phase retrieval from the norms of affine transformations,"  In this paper, we consider the generalized phase retrieval from affine
measurements. This problem aims to recover signals ${\mathbf x} \in {\mathbb
F}^d$ from the affine measurements $y_j=\norm{M_j^*\vx +{\mathbb b}_j}^2,\;
j=1,\ldots,m,$ where $M_j \in {\mathbb F}^{d\times r}, {\mathbf b}_j\in
{\mathbb F}^{r}, {\mathbb F}\in \{{\mathbb R},{\mathbb C}\}$ and we call it as
{\em generalized affine phase retrieval}. We develop a framework for
generalized affine phase retrieval with presenting necessary and sufficient
conditions for $\{(M_j,{\mathbf b}_j)\}_{j=1}^m$ having generalized affine
phase retrieval property. We also establish results on minimal measurement
number for generalized affine phase retrieval. Particularly, we show if
$\{(M_j,{\mathbf b}_j)\}_{j=1}^m \subset {\mathbb F}^{d\times r}\times {\mathbb
F}^{r}$ has generalized affine phase retrieval property, then $m\geq
d+\floor{d/r}$ for ${\mathbb F}={\mathbb R}$ ($m\geq 2d+\floor{d/r}$ for
${\mathbb F}={\mathbb C}$ ). We also show that the bound is tight provided
$r\mid d$. These results imply that one can reduce the measurement number by
raising $r$, i.e. the rank of $M_j$. This highlights a notable difference
between generalized affine phase retrieval and generalized phase retrieval.
Furthermore, using tools of algebraic geometry, we show that $m\geq 2d$ (resp.
$m\geq 4d-1$) generic measurements ${\mathcal A}=\{(M_j,b_j)\}_{j=1}^m$ have
the generalized phase retrieval property for ${\mathbb F}={\mathbb R}$ (resp.
${\mathbb F}={\mathbb C}$).
",0.0
102,0,114173,2003.12198,learning to rank with partitioned preference,Xingwei Hu,"Sorting Big Data by Revealed Preference with Application to College
  Ranking","  When ranking big data observations such as colleges in the United States,
diverse consumers reveal heterogeneous preferences. The objective of this paper
is to sort out a linear ordering for these observations and to recommend
strategies to improve their relative positions in the ranking. A properly
sorted solution could help consumers make the right choices, and governments
make wise policy decisions. Previous researchers have applied exogenous
weighting or multivariate regression approaches to sort big data objects,
ignoring their variety and variability. By recognizing the diversity and
heterogeneity among both the observations and the consumers, we instead apply
endogenous weighting to these contradictory revealed preferences. The outcome
is a consistent steady-state solution to the counterbalance equilibrium within
these contradictions. The solution takes into consideration the spillover
effects of multiple-step interactions among the observations. When information
from data is efficiently revealed in preferences, the revealed preferences
greatly reduce the volume of the required data in the sorting process. The
employed approach can be applied in many other areas, such as sports team
ranking, academic journal ranking, voting, and real effective exchange rates.
",1.0
103,6,138322,2011.08399,query expansion for imformation retrieval,"Kai Wang, Wenjie Zhang, Xuemin Lin, Ying Zhang, Lu Qin, Yuting Zhang",Efficient and Effective Community Search on Large-scale Bipartite Graphs,"  Bipartite graphs are widely used to model relationships between two types of
entities. Community search retrieves densely connected subgraphs containing a
query vertex, which has been extensively studied on unipartite graphs. However,
community search on bipartite graphs remains largely unexplored. Moreover, all
existing cohesive subgraph models on bipartite graphs can only be applied to
measure the structure cohesiveness between two sets of vertices while
overlooking the edge weight in forming the community. In this paper, we study
the significant (alpha, beta)-community search problem on weighted bipartite
graphs. Given a query vertex q, we aim to find the significant (alpha,
beta)-community R of q which adopts (alpha, beta)-core to characterize the
engagement level of vertices, and maximizes the minimum edge weight
(significance) within R.
  To support fast retrieval of R, we first retrieve the maximal connected
subgraph of (alpha, beta)-core containing the query vertex (the (alpha,
beta)-community), and the search space is limited to this subgraph with a much
smaller size than the original graph. A novel index structure is presented
which can be built in O(delta * m) time and takes O(delta * m) space where m is
the number of edges in G, delta is bounded by the square root of m and is much
smaller in practice. Utilizing the index, the (alpha, beta)-community can be
retrieved in optimal time. To further obtain R, we develop peeling and
expansion algorithms to conduct searches by shrinking from the (alpha,
beta)-community and expanding from the query vertex, respectively. The
experimental results on real graphs not only demonstrate the effectiveness of
the significant (alpha, beta)-community model but also validate the efficiency
of our query processing and indexing techniques.
",3.0
104,16,111300,2002.09889,activation function in neutral networks,D. Kafka and Daniel. N. Wilke,"Investigating the interaction between gradient-only line searches and
  different activation functions","  Gradient-only line searches (GOLS) adaptively determine step sizes along
search directions for discontinuous loss functions resulting from dynamic
mini-batch sub-sampling in neural network training. Step sizes in GOLS are
determined by localizing Stochastic Non-Negative Associated Gradient Projection
Points (SNN-GPPs) along descent directions. These are identified by a sign
change in the directional derivative from negative to positive along a descent
direction. Activation functions are a significant component of neural network
architectures as they introduce non-linearities essential for complex function
approximations. The smoothness and continuity characteristics of the activation
functions directly affect the gradient characteristics of the loss function to
be optimized. Therefore, it is of interest to investigate the relationship
between activation functions and different neural network architectures in the
context of GOLS. We find that GOLS are robust for a range of activation
functions, but sensitive to the Rectified Linear Unit (ReLU) activation
function in standard feedforward architectures. The zero-derivative in ReLU's
negative input domain can lead to the gradient-vector becoming sparse, which
severely affects training. We show that implementing architectural features
such as batch normalization and skip connections can alleviate these
difficulties and benefit training with GOLS for all activation functions
considered.
",3.0
105,15,161099,2106.11251,relevance feedback for imformation retrieval,"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis",Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval,"  Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,
have shown the usefulness of expanding and reweighting the users' initial
queries using information occurring in an initial set of retrieved documents,
known as the pseudo-relevant set. Recently, dense retrieval -- through the use
of neural contextual language models such as BERT for analysing the documents'
and queries' contents and computing their relevance scores -- has shown a
promising performance on several information retrieval tasks still relying on
the traditional inverted index for identifying documents relevant to a query.
Two different dense retrieval families have emerged: the use of single embedded
representations for each passage and query (e.g. using BERT's [CLS] token), or
via multiple representations (e.g. using an embedding for each token of the
query and document). In this work, we conduct the first study into the
potential for multiple representation dense retrieval to be enhanced using
pseudo-relevance feedback. In particular, based on the pseudo-relevant set of
documents identified using a first-pass dense retrieval, we extract
representative feedback embeddings (using KMeans clustering) -- while ensuring
that these embeddings discriminate among passages (based on IDF) -- which are
then added to the query representation. These additional feedback embeddings
are shown to both enhance the effectiveness of a reranking as well as an
additional dense retrieval operation. Indeed, experiments on the MSMARCO
passage ranking dataset show that MAP can be improved by upto 26% on the TREC
2019 query set and 10% on the TREC 2020 query set by the application of our
proposed ColBERT-PRF method on a ColBERT dense retrieval approach.
",5.0
106,7,103082,1911.05369,gradient boosting,"Vincent Grari, Boris Ruf, Sylvain Lamprier, Marcin Detyniecki",Fair Adversarial Gradient Tree Boosting,"  Fair classification has become an important topic in machine learning
research. While most bias mitigation strategies focus on neural networks, we
noticed a lack of work on fair classifiers based on decision trees even though
they have proven very efficient. In an up-to-date comparison of
state-of-the-art classification algorithms in tabular data, tree boosting
outperforms deep learning. For this reason, we have developed a novel approach
of adversarial gradient tree boosting. The objective of the algorithm is to
predict the output $Y$ with gradient tree boosting while minimizing the ability
of an adversarial neural network to predict the sensitive attribute $S$. The
approach incorporates at each iteration the gradient of the neural network
directly in the gradient tree boosting. We empirically assess our approach on 4
popular data sets and compare against state-of-the-art algorithms. The results
show that our algorithm achieves a higher accuracy while obtaining the same
level of fairness, as measured using a set of different common fairness
definitions.
",3.0
107,7,50856,1703.00377,gradient boosting,"Hanzhang Hu and Wen Sun and Arun Venkatraman and Martial Hebert and J.
  Andrew Bagnell",Gradient Boosting on Stochastic Data Streams,"  Boosting is a popular ensemble algorithm that generates more powerful
learners by linearly combining base models from a simpler hypothesis class. In
this work, we investigate the problem of adapting batch gradient boosting for
minimizing convex loss functions to online setting where the loss at each
iteration is i.i.d sampled from an unknown distribution. To generalize from
batch to online, we first introduce the definition of online weak learning edge
with which for strongly convex and smooth loss functions, we present an
algorithm, Streaming Gradient Boosting (SGB) with exponential shrinkage
guarantees in the number of weak learners. We further present an adaptation of
SGB to optimize non-smooth loss functions, for which we derive a O(ln N/N)
convergence rate. We also show that our analysis can extend to adversarial
online learning setting under a stronger assumption that the online weak
learning edge will hold in adversarial setting. We finally demonstrate
experimental results showing that in practice our algorithms can achieve
competitive results as classic gradient boosting while using less computation.
",4.0
108,1,10285,1110.24,advanced search engine,"Stephan Kiefer, Jochen Rauch, Riccardo Albertoni, Marco Attene, Franca
  Giannini, Simone Marini, Luc Schneider, Carlos Mesquita, Xin Xing, Michael
  Lawo","The CHRONIOUS Ontology-Driven Search Tool: Enabling Access to Focused
  and Up-to-Date Healthcare Literature","  This paper presents an advanced search engine prototype for bibliography
retrieval developed within the CHRONIOUS European IP project of the seventh
Framework Program (FP7). This search engine is specifically targeted to
clinicians and healthcare practitioners searching for documents related to
Chronic Obstructive Pulmonary Disease (COPD) and Chronic Kidney Disease (CKD).
To this aim, the presented tool exploits two pathology-specific ontologies that
allow focused document indexing and retrieval. These ontologies have been
developed on the top of the Middle Layer Ontology for Clinical Care (MLOCC),
which provides a link with the Basic Formal Ontology, a foundational ontology
used in the Open Biological and Biomedical Ontologies (OBO) Foundry. In
addition link with the terms of the MeSH (Medical Subject Heading) thesaurus
has been provided to guarantee the coverage with the general certified medical
terms and multilingual capabilities.
",5.0
109,19,116357,2004.09725,artificial intelligence for low carbon,"Simona Santamaria, David Dao, Bj\""orn L\""utjens, Ce Zhang","TrueBranch: Metric Learning-based Verification of Forest Conservation
  Projects","  International stakeholders increasingly invest in offsetting carbon
emissions, for example, via issuing Payments for Ecosystem Services (PES) to
forest conservation projects. Issuing trusted payments requires a transparent
monitoring, reporting, and verification (MRV) process of the ecosystem services
(e.g., carbon stored in forests). The current MRV process, however, is either
too expensive (on-ground inspection of forest) or inaccurate (satellite).
Recent works propose low-cost and accurate MRV via automatically determining
forest carbon from drone imagery, collected by the landowners. The automation
of MRV, however, opens up the possibility that landowners report untruthful
drone imagery. To be robust against untruthful reporting, we propose
TrueBranch, a metric learning-based algorithm that verifies the truthfulness of
drone imagery from forest conservation projects. TrueBranch aims to detect
untruthfully reported drone imagery by matching it with public satellite
imagery. Preliminary results suggest that nominal distance metrics are not
sufficient to reliably detect untruthfully reported imagery. TrueBranch
leverages metric learning to create a feature embedding in which truthfully and
untruthfully collected imagery is easily distinguishable by distance
thresholding.
",5.0
110,12,210909,2209.14091,COVID-19 and social media,"Nikhil Chilwant, Syed Taqi Abbas Rizvi, Hassan Soliman",Offensive Language Detection on Twitter,"  Detection of offensive language in social media is one of the key challenges
for social media. Researchers have proposed many advanced methods to accomplish
this task. In this report, we try to use the learnings from their approach and
incorporate our ideas to improve upon them. We have successfully achieved an
accuracy of 74% in classifying offensive tweets. We also list upcoming
challenges in the abusive content detection in the social media world.
",1.0
111,4,109836,2002.03438,pre-trained language model,"Lav R. Varshney, Nitish Shirish Keskar, and Richard Socher",Limits of Detecting Text Generated by Large-Scale Language Models,"  Some consider large-scale language models that can generate long and coherent
pieces of text as dangerous, since they may be used in misinformation
campaigns. Here we formulate large-scale language model output detection as a
hypothesis testing problem to classify text as genuine or generated. We show
that error exponents for particular language models are bounded in terms of
their perplexity, a standard measure of language generation performance. Under
the assumption that human language is stationary and ergodic, the formulation
is extended from considering specific language models to considering maximum
likelihood language models, among the class of k-order Markov approximations;
error probabilities are characterized. Some discussion of incorporating
semantic side information is also given.
",3.0
112,5,161487,2106.12933,matrix completion,Pini Zilber and Boaz Nadler,GNMR: A provable one-line algorithm for low rank matrix recovery,"  Low rank matrix recovery problems, including matrix completion and matrix
sensing, appear in a broad range of applications. In this work we present GNMR
-- an extremely simple iterative algorithm for low rank matrix recovery, based
on a Gauss-Newton linearization. On the theoretical front, we derive recovery
guarantees for GNMR in both the matrix sensing and matrix completion settings.
Some of these results improve upon the best currently known for other methods.
A key property of GNMR is that it implicitly keeps the factor matrices
approximately balanced throughout its iterations. On the empirical front, we
show that for matrix completion with uniform sampling, GNMR performs better
than several popular methods, especially when given very few observations close
to the information limit.
",4.0
113,10,21458,1307.8067,web archive,"Mat Kelly, Justin F. Brunelle, Michele C. Weigle, Michael L. Nelson",On the Change in Archivability of Websites Over Time,"  As web technologies evolve, web archivists work to keep up so that our
digital history is preserved. Recent advances in web technologies have
introduced client-side executed scripts that load data without a referential
identifier or that require user interaction (e.g., content loading when the
page has scrolled). These advances have made automating methods for capturing
web pages more difficult. Because of the evolving schemes of publishing web
pages along with the progressive capability of web preservation tools, the
archivability of pages on the web has varied over time. In this paper we show
that the archivability of a web page can be deduced from the type of page being
archived, which aligns with that page's accessibility in respect to dynamic
content. We show concrete examples of when these technologies were introduced
by referencing mementos of pages that have persisted through a long evolution
of available technologies. Identifying these reasons for the inability of these
web pages to be archived in the past in respect to accessibility serves as a
guide for ensuring that content that has longevity is published using good
practice methods that make it available for preservation.
",4.0
114,2,149823,2103.06397,random forests,"Wai Tong Chung, Aashwin Ananda Mishra, Matthias Ihme","Interpretable Data-driven Methods for Subgrid-scale Closure in LES for
  Transcritical LOX/GCH4 Combustion","  Many practical combustion systems such as those in rockets, gas turbines, and
internal combustion engines operate under high pressures that surpass the
thermodynamic critical limit of fuel-oxidizer mixtures. These conditions
require the consideration of complex fluid behaviors that pose challenges for
numerical simulations, casting doubts on the validity of existing subgrid-scale
(SGS) models in large-eddy simulations of these systems. While data-driven
methods have shown high accuracy as closure models in simulations of turbulent
flames, these models are often criticized for lack of physical
interpretability, wherein they provide answers but no insight into their
underlying rationale. The objective of this study is to assess SGS stress
models from conventional physics-driven approaches and an interpretable machine
learning algorithm, i.e., the random forest regressor, in a turbulent
transcritical non-premixed flame. To this end, direct numerical simulations
(DNS) of transcritical liquid-oxygen/gaseous-methane (LOX/GCH4) inert and
reacting flows are performed. Using this data, a priori analysis is performed
on the Favre-filtered DNS data to examine the accuracy of physics-based and
random forest SGS-models under these conditions. SGS stresses calculated with
the gradient model show good agreement with the exact terms extracted from
filtered DNS. The accuracy of the random-forest regressor decreased when
physics-based constraints are applied to the feature set. Results demonstrate
that random forests can perform as effectively as algebraic models when
modeling subgrid stresses, only when trained on a sufficiently representative
database. The employment of random forest feature importance score is shown to
provide insight into discovering subgrid-scale stresses through sparse
regression.
",3.0
115,13,169310,2109.07296,social network analysis with natrual language processing,"Jisun An, Haewoon Kwak, Claire Seungeun Lee, Bogang Jun, Yong-Yeol Ahn",Predicting Anti-Asian Hateful Users on Twitter during COVID-19,"  We investigate predictors of anti-Asian hate among Twitter users throughout
COVID-19. With the rise of xenophobia and polarization that has accompanied
widespread social media usage in many nations, online hate has become a major
social issue, attracting many researchers. Here, we apply natural language
processing techniques to characterize social media users who began to post
anti-Asian hate messages during COVID-19. We compare two user groups -- those
who posted anti-Asian slurs and those who did not -- with respect to a rich set
of features measured with data prior to COVID-19 and show that it is possible
to predict who later publicly posted anti-Asian slurs. Our analysis of
predictive features underlines the potential impact of news media and
information sources that report on online hate and calls for further
investigation into the role of polarized communication networks and news media.
",5.0
116,16,156910,2105.09513,activation function in neutral networks,"Ameya D. Jagtap, Yeonjong Shin, Kenji Kawaguchi, George Em Karniadakis","Deep Kronecker neural networks: A general framework for neural networks
  with adaptive activation functions","  We propose a new type of neural networks, Kronecker neural networks (KNNs),
that form a general framework for neural networks with adaptive activation
functions. KNNs employ the Kronecker product, which provides an efficient way
of constructing a very wide network while keeping the number of parameters low.
Our theoretical analysis reveals that under suitable conditions, KNNs induce a
faster decay of the loss than that by the feed-forward networks. This is also
empirically verified through a set of computational examples. Furthermore,
under certain technical assumptions, we establish global convergence of
gradient descent for KNNs. As a specific case, we propose the Rowdy activation
function that is designed to get rid of any saturation region by injecting
sinusoidal fluctuations, which include trainable parameters. The proposed Rowdy
activation function can be employed in any neural network architecture like
feed-forward neural networks, Recurrent neural networks, Convolutional neural
networks etc. The effectiveness of KNNs with Rowdy activation is demonstrated
through various computational experiments including function approximation
using feed-forward neural networks, solution inference of partial differential
equations using the physics-informed neural networks, and standard deep
learning benchmark problems using convolutional and fully-connected neural
networks.
",5.0
117,12,125075,2007.04113,COVID-19 and social media,"Fabian Braesemann, Fabian Stephany","Social media self-branding and success: Quantitative evidence from a
  model competition","  Thanks to the availability of large online data sets, it has become possible
to quantify success in different fields of human endeavour. The study presented
here contributes to this literature in evaluating the effect of social media
activity, as a means of 'self-branding', to increase the chances of models
being elected for the Playboy Magazine's Playmate of the Year award. We
hypothesise that candidates who actively manage their Instagram accounts can
increase their likelihood to win the award: they use social media to gain more
followers, who then might vote for them in the award polls. The findings
indicate that social media activity actually has predictive capacity to
estimate the outcome of the award. We find evidence that candidates who manage
their social media accounts more actively than other candidates have a higher
probability to become Playmate of the Year. The findings underline the benefits
of social media
",1.0
118,3,216511,cs/0106055,database management system,"Raj P. Gopalan, Tariq Nuruddin, Yudho Giri Sucahyo",A Seamless Integration of Association Rule Mining with Database Systems,"  The need for Knowledge and Data Discovery Management Systems (KDDMS) that
support ad hoc data mining queries has been long recognized. A significant
amount of research has gone into building tightly coupled systems that
integrate association rule mining with database systems. In this paper, we
describe a seamless integration scheme for database queries and association
rule discovery using a common query optimizer for both. Query trees of
expressions in an extended algebra are used for internal representation in the
optimizer. The algebraic representation is flexible enough to deal with
constrained association rule queries and other variations of association rule
specifications. We propose modularization to simplify the query tree for
complex tasks in data mining. It paves the way for making use of existing
algorithms for constructing query plans in the optimization process. How the
integration scheme we present will facilitate greater user control over the
data mining process is also discussed. The work described in this paper forms
part of a larger project for fully integrating data mining with database
management.
",4.0
119,8,86439,1904.08157,node embedding for graph,"Tianshu Lyu, Fei Sun, Peng Jiang, Wenwu Ou, Yan Zhang",Compositional Network Embedding,"  Network embedding has proved extremely useful in a variety of network
analysis tasks such as node classification, link prediction, and network
visualization. Almost all the existing network embedding methods learn to map
the node IDs to their corresponding node embeddings. This design principle,
however, hinders the existing methods from being applied in real cases. Node ID
is not generalizable and, thus, the existing methods have to pay great effort
in cold-start problem. The heterogeneous network usually requires extra work to
encode node types, as node type is not able to be identified by node ID. Node
ID carries rare information, resulting in the criticism that the existing
methods are not robust to noise.
  To address this issue, we introduce Compositional Network Embedding, a
general inductive network representation learning framework that generates node
embeddings by combining node features based on the principle of
compositionally. Instead of directly optimizing an embedding lookup based on
arbitrary node IDs, we learn a composition function that infers node embeddings
by combining the corresponding node attribute embeddings through a graph-based
loss. For evaluation, we conduct the experiments on link prediction under four
different settings. The results verified the effectiveness and generalization
ability of compositional network embeddings, especially on unseen nodes.
",5.0
120,15,194268,2205.00235,relevance feedback for imformation retrieval,"Hang Li, Shuai Wang, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy
  Lin, Guido Zuccon","To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers","  Current pre-trained language model approaches to information retrieval can be
broadly divided into two categories: sparse retrievers (to which belong also
non-neural approaches such as bag-of-words methods, e.g., BM25) and dense
retrievers. Each of these categories appears to capture different
characteristics of relevance. Previous work has investigated how relevance
signals from sparse retrievers could be combined with those from dense
retrievers via interpolation. Such interpolation would generally lead to higher
retrieval effectiveness. In this paper we consider the problem of combining the
relevance signals from sparse and dense retrievers in the context of Pseudo
Relevance Feedback (PRF). This context poses two key challenges: (1) When
should interpolation occur: before, after, or both before and after the PRF
process? (2) Which sparse representation should be considered: a zero-shot
bag-of-words model (BM25), or a learnt sparse representation? To answer these
questions we perform a thorough empirical evaluation considering an effective
and scalable neural PRF approach (Vector-PRF), three effective dense retrievers
(ANCE, TCTv2, DistillBERT), and one state-of-the-art learnt sparse retriever
(uniCOIL). The empirical findings from our experiments suggest that, regardless
of sparse representation and dense retriever, interpolation both before and
after PRF achieves the highest effectiveness across most datasets and metrics.
",5.0
121,4,42771,1605.07969,pre-trained language model,"Rudy Bunel, Alban Desmaison, Pushmeet Kohli, Philip H.S. Torr and M.
  Pawan Kumar",Adaptive Neural Compilation,"  This paper proposes an adaptive neural-compilation framework to address the
problem of efficient program learning. Traditional code optimisation strategies
used in compilers are based on applying pre-specified set of transformations
that make the code faster to execute without changing its semantics. In
contrast, our work involves adapting programs to make them more efficient while
considering correctness only on a target input distribution. Our approach is
inspired by the recent works on differentiable representations of programs. We
show that it is possible to compile programs written in a low-level language to
a differentiable representation. We also show how programs in this
representation can be optimised to make them efficient on a target distribution
of inputs. Experimental results demonstrate that our approach enables learning
specifically-tuned algorithms for given data distributions with a high success
rate.
",1.0
122,1,42314,1605.02917,advanced search engine,"Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki",Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine,"  Search engines are the most important tools for web data acquisition. Web
pages are crawled and indexed by search Engines. Users typically locate useful
web pages by querying a search engine. One of the challenges in search engines
administration is spam pages which waste search engine resources. These pages
by deception of search engine ranking algorithms try to be showed in the first
page of results. There are many approaches to web spam pages detection such as
measurement of HTML code style similarity, pages linguistic pattern analysis
and machine learning algorithm on page content features. One of the famous
algorithms has been used in machine learning approach is Support Vector Machine
(SVM) classifier. Recently basic structure of SVM has been changed by new
extensions to increase robustness and classification accuracy. In this paper we
improved accuracy of web spam detection by using two nonlinear kernels into
Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data
separation has been increased by using two separated kernels for each class of
data. Effectiveness of new proposed method has been experimented with two
publicly used spam datasets called UK-2007 and UK-2006. Results show the
effectiveness of proposed kernelized version of TSVM in web spam page
detection.
",3.0
123,19,150659,2103.11148,artificial intelligence for low carbon,"G.G. Samatas, S.S. Moumgiakmas, G.A. Papakostas",Predictive Maintenance -- Bridging Artificial Intelligence and IoT,"  This paper highlights the trends in the field of predictive maintenance with
the use of machine learning. With the continuous development of the Fourth
Industrial Revolution, through IoT, the technologies that use artificial
intelligence are evolving. As a result, industries have been using these
technologies to optimize their production. Through scientific research
conducted for this paper, conclusions were drawn about the trends in Predictive
Maintenance applications with the use of machine learning bridging Artificial
Intelligence and IoT. These trends are related to the types of industries in
which Predictive Maintenance was applied, the models of artificial intelligence
were implemented, mainly of machine learning and the types of sensors that are
applied through the IoT to the applications. Six sectors were presented and the
production sector was dominant as it accounted for 54.54% of total
publications. In terms of artificial intelligence models, the most prevalent
among ten were the Artificial Neural Networks, Support Vector Machine and
Random Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve
categories of sensors emerged, of which the most widely used were the sensors
of temperature and vibration with percentages of 60.71% and 46.42%
correspondingly.
",1.0
124,11,217246,cs/0412002,PageRank for web search,Jose Borges and Mark Levene,Ranking Pages by Topology and Popularity within Web Sites,"  We compare two link analysis ranking methods of web pages in a site. The
first, called Site Rank, is an adaptation of PageRank to the granularity of a
web site and the second, called Popularity Rank, is based on the frequencies of
user clicks on the outlinks in a page that are captured by navigation sessions
of users through the web site. We ran experiments on artificially created web
sites of different sizes and on two real data sets, employing the relative
entropy to compare the distributions of the two ranking methods. For the real
data sets we also employ a nonparametric measure, called Spearman's footrule,
which we use to compare the top-ten web pages ranked by the two methods. Our
main result is that the distributions of the Popularity Rank and Site Rank are
surprisingly close to each other, implying that the topology of a web site is
very instrumental in guiding users through the site. Thus, in practice, the
Site Rank provides a reasonable first order approximation of the aggregate
behaviour of users within a web site given by the Popularity Rank.
",4.0
125,14,213737,2210.09474,text summarization model,"Seongmin Park, Dongchan Shin, Jihwa Lee",Leveraging Non-dialogue Summaries for Dialogue Summarization,"  To mitigate the lack of diverse dialogue summarization datasets in academia,
we present methods to utilize non-dialogue summarization data for enhancing
dialogue summarization systems. We apply transformations to document
summarization data pairs to create training data that better befit dialogue
summarization. The suggested transformations also retain desirable properties
of non-dialogue datasets, such as improved faithfulness to the source text. We
conduct extensive experiments across both English and Korean to verify our
approach. Although absolute gains in ROUGE naturally plateau as more dialogue
summarization samples are introduced, utilizing non-dialogue data for training
significantly improves summarization performance in zero- and few-shot settings
and enhances faithfulness across all training regimes.
",3.0
126,9,102089,1911.00359,language model for long documents,"Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav
  Chaudhary, Francisco Guzm\'an, Armand Joulin, Edouard Grave",CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data,"  Pre-training text representations have led to significant improvements in
many areas of natural language processing. The quality of these models benefits
greatly from the size of the pretraining corpora as long as its quality is
preserved. In this paper, we describe an automatic pipeline to extract massive
high-quality monolingual datasets from Common Crawl for a variety of languages.
Our pipeline follows the data processing introduced in fastText (Mikolov et
al., 2017; Grave et al., 2018), that deduplicates documents and identifies
their language. We augment this pipeline with a filtering step to select
documents that are close to high quality corpora like Wikipedia.
",1.0
127,7,114810,2004.01497,gradient boosting,"Mojtaba Nabipour, Pooyan Nayyeri, Hamed Jabani, Amir Mosavi",Deep learning for Stock Market Prediction,"  Prediction of stock groups' values has always been attractive and challenging
for shareholders. This paper concentrates on the future prediction of stock
market groups. Four groups named diversified financials, petroleum,
non-metallic minerals and basic metals from Tehran stock exchange are chosen
for experimental evaluations. Data are collected for the groups based on ten
years of historical records. The values predictions are created for 1, 2, 5,
10, 15, 20 and 30 days in advance. The machine learning algorithms utilized for
prediction of future values of stock market groups. We employed Decision Tree,
Bagging, Random Forest, Adaptive Boosting (Adaboost), Gradient Boosting and
eXtreme Gradient Boosting (XGBoost), and Artificial neural network (ANN),
Recurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical
indicators are selected as the inputs into each of the prediction models.
Finally, the result of predictions is presented for each technique based on
three metrics. Among all the algorithms used in this paper, LSTM shows more
accurate results with the highest model fitting ability. Also, for tree-based
models, there is often an intense competition between Adaboost, Gradient
Boosting, and XGBoost.
",3.0
128,16,144647,2101.09957,activation function in neutral networks,Johannes Lederer,"Activation Functions in Artificial Neural Networks: A Systematic
  Overview","  Activation functions shape the outputs of artificial neurons and, therefore,
are integral parts of neural networks in general and deep learning in
particular. Some activation functions, such as logistic and relu, have been
used for many decades. But with deep learning becoming a mainstream research
topic, new activation functions have mushroomed, leading to confusion in both
theory and practice. This paper provides an analytic yet up-to-date overview of
popular activation functions and their properties, which makes it a timely
resource for anyone who studies or applies neural networks.
",5.0
129,0,88182,1905.05348,learning to rank with partitioned preference,"Sejun Park, Eunho Yang, Se-Young Yun, Jinwoo Shin",Spectral Approximate Inference,"  Given a graphical model (GM), computing its partition function is the most
essential inference task, but it is computationally intractable in general. To
address the issue, iterative approximation algorithms exploring certain local
structure/consistency of GM have been investigated as popular choices in
practice. However, due to their local/iterative nature, they often output poor
approximations or even do not converge, e.g., in low-temperature regimes (hard
instances of large parameters). To overcome the limitation, we propose a novel
approach utilizing the global spectral feature of GM. Our contribution is
two-fold: (a) we first propose a fully polynomial-time approximation scheme
(FPTAS) for approximating the partition function of GM associating with a
low-rank coupling matrix; (b) for general high-rank GMs, we design a spectral
mean-field scheme utilizing (a) as a subroutine, where it approximates a
high-rank GM into a product of rank-1 GMs for an efficient approximation of the
partition function. The proposed algorithm is more robust in its running time
and accuracy than prior methods, i.e., neither suffers from the convergence
issue nor depends on hard local structures, as demonstrated in our experiments.
",0.0
130,10,49929,1702.00436,web archive,"Zeon Trevor Fernando, Ivana Marenzi, Wolfgang Nejdl","ArchiveWeb: collaboratively extending and exploring web archive
  collections - How would you like to work with your collections?","  Curated web archive collections contain focused digital content which is
collected by archiving organizations, groups, and individuals to provide a
representative sample covering specific topics and events to preserve them for
future exploration and analysis. In this paper, we discuss how to best support
collaborative construction and exploration of these collections through the
ArchiveWeb system. ArchiveWeb has been developed using an iterative
evaluation-driven design-based research approach, with considerable user
feedback at all stages. The first part of this paper describes the important
insights we gained from our initial requirements engineering phase during the
first year of the project and the main functionalities of the current
ArchiveWeb system for searching, constructing, exploring, and discussing web
archive collections. The second part summarizes the feedback we received on
this version from archiving organizations and libraries, as well as our
corresponding plans for improving and extending the system for the next
release.
",5.0
131,4,134098,2010.06047,pre-trained language model,"Sofia de la Fuente Garcia, Craig Ritchie and Saturnino Luz","Artificial Intelligence, speech and language processing approaches to
  monitoring Alzheimer's Disease: a systematic review","  Language is a valuable source of clinical information in Alzheimer's Disease,
as it declines concurrently with neurodegeneration. Consequently, speech and
language data have been extensively studied in connection with its diagnosis.
This paper summarises current findings on the use of artificial intelligence,
speech and language processing to predict cognitive decline in the context of
Alzheimer's Disease, detailing current research procedures, highlighting their
limitations and suggesting strategies to address them. We conducted a
systematic review of original research between 2000 and 2019, registered in
PROSPERO (reference CRD42018116606). An interdisciplinary search covered six
databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine
(PubMed and Embase) and Web of Science. Bibliographies of relevant papers were
screened until December 2019. From 3,654 search results 51 articles were
selected against the eligibility criteria. Four tables summarise their
findings: study details (aim, population, interventions, comparisons, methods
and outcomes), data details (size, type, modalities, annotation, balance,
availability and language of study), methodology (pre-processing, feature
generation, machine learning, evaluation and results) and clinical
applicability (research implications, clinical potential, risk of bias and
strengths/limitations). While promising results are reported across nearly all
51 studies, very few have been implemented in clinical research or practice. We
concluded that the main limitations of the field are poor standardisation,
limited comparability of results, and a degree of disconnect between study aims
and clinical applications. Attempts to close these gaps should support
translation of future research into clinical practice.
",3.0
132,18,166423,2108.06279,infomation retrieval time complexity,"Craig Macdonald, Nicola Tonellotto, Iadh Ounis",On Single and Multiple Representations in Dense Passage Retrieval,"  The advent of contextualised language models has brought gains in search
effectiveness, not just when applied for re-ranking the output of classical
weighting models such as BM25, but also when used directly for passage indexing
and retrieval, a technique which is called dense retrieval. In the existing
literature in neural ranking, two dense retrieval families have become
apparent: single representation, where entire passages are represented by a
single embedding (usually BERT's [CLS] token, as exemplified by the recent ANCE
approach), or multiple representations, where each token in a passage is
represented by its own embedding (as exemplified by the recent ColBERT
approach). These two families have not been directly compared. However, because
of the likely importance of dense retrieval moving forward, a clear
understanding of their advantages and disadvantages is paramount. To this end,
this paper contributes a direct study on their comparative effectiveness,
noting situations where each method under/over performs w.r.t. each other, and
w.r.t. a BM25 baseline. We observe that, while ANCE is more efficient than
ColBERT in terms of response time and memory usage, multiple representations
are statistically more effective than the single representations for MAP and
MRR@10. We also show that multiple representations obtain better improvements
than single representations for queries that are the hardest for BM25, as well
as for definitional queries, and those with complex information needs.
",3.0
133,14,134149,2010.06253,text summarization model,"Peng Cui, Le Hu, and Yuanchao Liu","Enhancing Extractive Text Summarization with Topic-Aware Graph Neural
  Networks","  Text summarization aims to compress a textual document to a short summary
while keeping salient information. Extractive approaches are widely used in
text summarization because of their fluency and efficiency. However, most of
existing extractive models hardly capture inter-sentence relationships,
particularly in long documents. They also often ignore the effect of topical
information on capturing important contents. To address these issues, this
paper proposes a graph neural network (GNN)-based extractive summarization
model, enabling to capture inter-sentence relationships efficiently via
graph-structured document representation. Moreover, our model integrates a
joint neural topic model (NTM) to discover latent topics, which can provide
document-level features for sentence selection. The experimental results
demonstrate that our model not only substantially achieves state-of-the-art
results on CNN/DM and NYT datasets but also considerably outperforms existing
approaches on scientific paper datasets consisting of much longer documents,
indicating its better robustness in document genres and lengths. Further
discussions show that topical information can help the model preselect salient
contents from an entire document, which interprets its effectiveness in long
document summarization.
",5.0
134,11,1674,807.4325,PageRank for web search,"Nicola Perra (1,2), Vinko Zlatic (3,4), Alessandro Chessa (1,2),
  Claudio Conti (5), Debora Donato (6), Guido Caldarelli (3,2) ((1) Dep of
  Physics, SLACS-CNR University of Cagliari Italy, (2) Linkalab, Complex
  Systems Computational Lab. Cagliari, Italy, (3) Centre SMC CNR-INFM, Dip.
  Fisica, Universita' Sapienza Rome, Italy, (4) Theor. Physics Div., Rudjer
  Boskovic Inst., Zagreb Croatia, (5) Centre SOFT CNR-INFM, Dip. Fisica,
  Universita' Sapienza Rome, Italy, (6) Yahoo! Research Barcelona Spain)",Schroedinger-like PageRank equation and localization in the WWW,"  The WorldWide Web is one of the most important communication systems we use
in our everyday life. Despite its central role, the growth and the development
of the WWW is not controlled by any central authority. This situation has
created a huge ensemble of connections whose complexity can be fruitfully
described and quantified by network theory. One important application that
allows to sort out the information present in these connections is given by the
PageRank alghorithm. Computation of this quantity is usually made iteratively
with a large use of computational time. In this paper we show that the PageRank
can be expressed in terms of a wave function obeying a Schroedinger-like
equation. In particular the topological disorder given by the unbalance of
outgoing and ingoing links between pages, induces wave function and potential
structuring. This allows to directly localize the pages with the largest score.
Through this new representation we can now compute the PageRank without
iterative techniques. For most of the cases of interest our method is faster
than the original one. Our results also clarify the role of topology in the
diffusion of information within complex networks. The whole approach opens the
possibility to novel techniques inspired by quantum physics for the analysis of
the WWW properties.
",5.0
135,11,204480,2207.11429,PageRank for web search,"Naini Dudhe, Colin Benjamin",Resolving degeneracies in Google search via quantum stochastic walks,"  The internet is one of the most valuable technologies invented to date. Among
them, Google is the most widely used search engine. The PageRank algorithm is
the backbone of Google search, ranking web pages according to relevance and
recency. We employ quantum stochastic walks (QSW) with the hope of bettering
the classical PageRank (CPR) algorithm, which is based on classical continuous
time random walks (CTRW). We implement QSW via two schemes: only incoherence
and dephasing with incoherence. PageRank using QSW with only incoherence or QSW
with dephasing and incoherence best resolves degeneracies that are unresolvable
via CPR and with a convergence time comparable to that for CPR, which is
generally the minimum. For some networks, the two QSW schemes obtain a
convergence time lower than CPR and an almost degeneracy-free ranking compared
to CPR.
",5.0
136,11,14606,1207.6328,PageRank for web search,Pierluigi Amodio and Luigi Brugnano,Recent advances in bibliometric indexes and the PaperRank problem,"  Bibliometric indexes are customary used in evaluating the impact of
scientific research, even though it is very well known that in different
research areas they may range in very different intervals. Sometimes, this is
evident even within a single given field of investigation making very difficult
(and inaccurate) the assessment of scientific papers. On the other hand, the
problem can be recast in the same framework which has allowed to efficiently
cope with the ordering of web-pages, i.e., to formulate the PageRank of Google.
For this reason, we call such problem the PaperRank problem, here solved by
using a similar approach to that employed by PageRank. The obtained solution,
which is mathematically grounded, will be used to compare the usual heuristics
of the number of citations with a new one here proposed. Some numerical tests
show that the new heuristics is much more reliable than the currently used
ones, based on the bare number of citations. Moreover, we show that our model
improves on recently proposed ones.
",4.0
137,11,7170,1012.4872,PageRank for web search,"Ying Ding, Erjia Yan, Arthur Frazho, James Caverlee",PageRank for ranking authors in co-citation networks,"  Google's PageRank has created a new synergy to information retrieval for a
better ranking of Web pages. It ranks documents depending on the topology of
the graphs and the weights of the nodes. PageRank has significantly advanced
the field of information retrieval and keeps Google ahead of competitors in the
search engine market. It has been deployed in bibliometrics to evaluate
research impact, yet few of these studies focus on the important impact of the
damping factor (d) for ranking purposes. This paper studies how varied damping
factors in the PageRank algorithm can provide additional insight into the
ranking of authors in an author co-citation network. Furthermore, we propose
weighted PageRank algorithms. We select 108 most highly cited authors in the
information retrieval (IR) area from the 1970s to 2008 to form the author
co-citation network. We calculate the ranks of these 108 authors based on
PageRank with damping factor ranging from 0.05 to 0.95. In order to test the
relationship between these different measures, we compare PageRank and weighted
PageRank results with the citation ranking, h-index, and centrality measures.
We found that in our author co-citation network, citation rank is highly
correlated with PageRank's with different damping factors and also with
different PageRank algorithms; citation rank and PageRank are not significantly
correlated with centrality measures; and h-index is not significantly
correlated with centrality measures.
",5.0
138,9,189008,2203.06408,language model for long documents,Chunyu Li and Jiajia Ding and Xing hu and Fan Wang,"Information retrieval for label noise document ranking by bag sampling
  and group-wise loss","  Long Document retrieval (DR) has always been a tremendous challenge for
reading comprehension and information retrieval. The pre-training model has
achieved good results in the retrieval stage and Ranking for long documents in
recent years. However, there is still some crucial problem in long document
ranking, such as data label noises, long document representations, negative
data Unbalanced sampling, etc. To eliminate the noise of labeled data and to be
able to sample the long documents in the search reasonably negatively, we
propose the bag sampling method and the group-wise Localized Contrastive
Estimation(LCE) method. We use the head middle tail passage for the long
document to encode the long document, and in the retrieval, stage Use dense
retrieval to generate the candidate's data. The retrieval data is divided into
multiple bags at the ranking stage, and negative samples are selected in each
bag. After sampling, two losses are combined. The first loss is LCE. To fit bag
sampling well, after query and document are encoded, the global features of
each group are extracted by convolutional layer and max-pooling to improve the
model's resistance to the impact of labeling noise, finally, calculate the LCE
group-wise loss. Notably, our model shows excellent performance on the MS MARCO
Long document ranking leaderboard.
",2.0
139,15,39753,1602.01665,relevance feedback for imformation retrieval,Ronan Cummins,Improved Query Topic Models via Pseudo-Relevant P\'olya Document Models,"  Query-expansion via pseudo-relevance feedback is a popular method of
overcoming the problem of vocabulary mismatch and of increasing average
retrieval effectiveness. In this paper, we develop a new method that estimates
a query topic model from a set of pseudo-relevant documents using a new
language modelling framework.
  We assume that documents are generated via a mixture of multivariate Polya
distributions, and we show that by identifying the topical terms in each
document, we can appropriately select terms that are likely to belong to the
query topic model. The results of experiments on several TREC collections show
that the new approach compares favourably to current state-of-the-art expansion
methods.
",3.0
140,19,215621,2210.17051,artificial intelligence for low carbon,"Gege Wen, Zongyi Li, Qirui Long, Kamyar Azizzadenesheli, Anima
  Anandkumar, Sally M. Benson","Accelerating Carbon Capture and Storage Modeling using Fourier Neural
  Operators","  Carbon capture and storage (CCS) is an important strategy for reducing carbon
dioxide emissions and mitigating climate change. We consider the storage aspect
of CCS, which involves injecting carbon dioxide into underground reservoirs.
This requires accurate and high-resolution predictions of carbon dioxide plume
migration and reservoir pressure buildup. However, such modeling is challenging
at scale due to the high computational costs of existing numerical methods. We
introduce a novel machine learning approach for four-dimensional
spatial-temporal modeling, which speeds up predictions nearly 700,000 times
compared to existing methods. It provides highly accurate predictions under
diverse reservoir conditions, geological heterogeneity, and injection schemes.
Our framework, Nested Fourier Neural Operator (FNO), learns the solution
operator for the family of partial differential equations governing the carbon
dioxide-water multiphase flow. It uses a hierarchy of FNO models to produce
outputs at different refinement levels. Thus, our approach enables
unprecedented real-time high-resolution modeling for carbon dioxide storage.
",5.0
141,12,27742,1407.3552,COVID-19 and social media,"Shuotian Bai, Rui Gao, Bibo Hao, Sha Yuan, and Tingshao Zhu",Identifying Social Satisfaction from Social Media,"  We demonstrate the critical need to identify social situation and instability
factors by acquiring public social satisfaction in this research. However,
subject to the large amount of manual work cost in subject recruitment and data
processing, conventional self-reported method cannot be implemented in real
time or applied in large scale investigation. To solve the problem, this paper
proposed an approach to predict users' social satisfaction, especially for the
economy-related satisfaction based on users' social media records. We recruited
2,018 Cantonese active participants from each city in Guangdong province
according to the population distribution. Both behavioral and linguistic
features of the participants are extracted from the online records of social
media, i.e., Sina Weibo. Regression models are used to predict Sina Weibo
users' social satisfaction. Furthermore, we consult the economic indexes of
Guangdong in 2012, and calculate the correlations between these indexes and the
predicted social satisfaction. Results indicate that social satisfaction can be
significantly expressed by specific social media features; local economy
satisfaction has significant positive correlations with several local economy
indexes, which supports that it is reliable to predict social satisfaction from
social media.
",1.0
142,10,49918,1702.00198,web archive,"Zeon Trevor Fernando, Ivana Marenzi, Wolfgang Nejdl, Rishita Kalyani","ArchiveWeb: Collaboratively Extending and Exploring Web Archive
  Collections","  Curated web archive collections contain focused digital contents which are
collected by archiving organizations to provide a representative sample
covering specific topics and events to preserve them for future exploration and
analysis. In this paper, we discuss how to best support collaborative
construction and exploration of these collections through the ArchiveWeb
system. ArchiveWeb has been developed using an iterative evaluation-driven
design-based research approach, with considerable user feedback at all stages.
This paper describes the functionalities of our current prototype for
searching, constructing, exploring and discussing web archive collections, as
well as feedback on this prototype from seven archiving organizations, and our
plans for improving the next release of the system.
",5.0
143,19,54846,1706.07269,artificial intelligence for low carbon,Tim Miller,"Explanation in Artificial Intelligence: Insights from the Social
  Sciences","  There has been a recent resurgence in the area of explainable artificial
intelligence as researchers and practitioners seek to make their algorithms
more understandable. Much of this research is focused on explicitly explaining
decisions or actions to a human observer, and it should not be controversial to
say that looking at how humans explain to each other can serve as a useful
starting point for explanation in artificial intelligence. However, it is fair
to say that most work in explainable artificial intelligence uses only the
researchers' intuition of what constitutes a `good' explanation. There exists
vast and valuable bodies of research in philosophy, psychology, and cognitive
science of how people define, generate, select, evaluate, and present
explanations, which argues that people employ certain cognitive biases and
social expectations towards the explanation process. This paper argues that the
field of explainable artificial intelligence should build on this existing
research, and reviews relevant papers from philosophy, cognitive
psychology/science, and social psychology, which study these topics. It draws
out some important findings, and discusses ways that these can be infused with
work on explainable artificial intelligence.
",1.0
144,18,217183,cs/0409044,infomation retrieval time complexity,Luca Trevisan,Some Applications of Coding Theory in Computational Complexity,"  Error-correcting codes and related combinatorial constructs play an important
role in several recent (and old) results in computational complexity theory. In
this paper we survey results on locally-testable and locally-decodable
error-correcting codes, and their applications to complexity theory and to
cryptography.
  Locally decodable codes are error-correcting codes with sub-linear time
error-correcting algorithms. They are related to private information retrieval
(a type of cryptographic protocol), and they are used in average-case
complexity and to construct ``hard-core predicates'' for one-way permutations.
  Locally testable codes are error-correcting codes with sub-linear time
error-detection algorithms, and they are the combinatorial core of
probabilistically checkable proofs.
",1.0
145,0,31359,1501.07467,learning to rank with partitioned preference,"Hamed Zamani, Azadeh Shakery, Pooya Moradi","Regression and Learning to Rank Aggregation for User Engagement
  Evaluation","  User engagement refers to the amount of interaction an instance (e.g., tweet,
news, and forum post) achieves. Ranking the items in social media websites
based on the amount of user participation in them, can be used in different
applications, such as recommender systems. In this paper, we consider a tweet
containing a rating for a movie as an instance and focus on ranking the
instances of each user based on their engagement, i.e., the total number of
retweets and favorites it will gain.
  For this task, we define several features which can be extracted from the
meta-data of each tweet. The features are partitioned into three categories:
user-based, movie-based, and tweet-based. We show that in order to obtain good
results, features from all categories should be considered. We exploit
regression and learning to rank methods to rank the tweets and propose to
aggregate the results of regression and learning to rank methods to achieve
better performance. We have run our experiments on an extended version of
MovieTweeting dataset provided by ACM RecSys Challenge 2014. The results show
that learning to rank approach outperforms most of the regression models and
the combination can improve the performance significantly.
",1.0
146,12,164926,2107.13236,COVID-19 and social media,"David Garcia, Max Pellert, Jana Lasser, Hannah Metzler","Social media emotion macroscopes reflect emotional experiences in
  society at large","  Social media generate data on human behaviour at large scales and over long
periods of time, posing a complementary approach to traditional methods in the
social sciences. Millions of texts from social media can be processed with
computational methods to study emotions over time and across regions. However,
recent research has shown weak correlations between social media emotions and
affect questionnaires at the individual level and between static regional
aggregates of social media emotion and subjective well-being at the population
level, questioning the validity of social media data to study emotions. Yet, to
date, no research has tested the validity of social media emotion macroscopes
to track the temporal evolution of emotions at the level of a whole society.
Here we present a pre-registered prediction study that shows how
gender-rescaled time series of Twitter emotional expression at the national
level substantially correlate with aggregates of self-reported emotions in a
weekly representative survey in the United Kingdom. A follow-up exploratory
analysis shows a high prevalence of third-person references in
emotionally-charged tweets, indicating that social media data provide a way of
social sensing the emotions of others rather than just the emotional
experiences of users. These results show that, despite the issues that social
media have in terms of representativeness and algorithmic confounding, the
combination of advanced text analysis methods with user demographic information
in social media emotion macroscopes can provide measures that are informative
of the general population beyond social media users.
",1.0
147,18,34011,1505.07204,infomation retrieval time complexity,Zhiqiang Xu,The minimal measurement number for low-rank matrices recovery,"  The paper presents several results that address a fundamental question in
low-rank matrices recovery: how many measurements are needed to recover low
rank matrices? We begin by investigating the complex matrices case and show
that $4nr-4r^2$ generic measurements are both necessary and sufficient for the
recovery of rank-$r$ matrices in $\C^{n\times n}$ by algebraic tools. Thus, we
confirm a conjecture which is raised by Eldar, Needell and Plan for the complex
case. We next consider the real case and prove that the bound $4nr-4r^2$ is
tight provided $n=2^k+r, k\in \Z_+$. Motivated by Vinzant's work, we construct
$11$ matrices in $\R^{4\times 4}$ by computer random search and prove they
define injective measurements on rank-$1$ matrices in $\R^{4\times 4}$. This
disproves the conjecture raised by Eldar, Needell and Plan for the real case.
Finally, we use the results in this paper to investigate the phase retrieval by
projection and show fewer than $2n-1$ orthogonal projections are possible for
the recovery of $x\in \R^n$ from the norm of them.
",0.0
148,15,112071,2003.00559,relevance feedback for imformation retrieval,Kshitij Bakliwal and Sai Ravela,The Sloop System for Individual Animal Identification with Deep Learning,"  The MIT Sloop system indexes and retrieves photographs from databases of
non-stationary animal population distributions. To do this, it adaptively
represents and matches generic visual feature representations using sparse
relevance feedback from experts and crowds. Here, we describe the Sloop system
and its application, then compare its approach to a standard deep learning
formulation. We then show that priming with amplitude and deformation features
requires very shallow networks to produce superior recognition results. Results
suggest that relevance feedback, which enables Sloop's high-recall performance
may also be essential for deep learning approaches to individual identification
to deliver comparable results.
",0.0
149,18,37463,1511.01669,infomation retrieval time complexity,"Tianyu Qiu, Prabhu Babu, and Daniel P. Palomar",PRIME: Phase Retrieval via Majorization-Minimization,"  This paper considers the phase retrieval problem in which measurements
consist of only the magnitude of several linear measurements of the unknown,
e.g., spectral components of a time sequence. We develop low-complexity
algorithms with superior performance based on the majorization-minimization
(MM) framework. The proposed algorithms are referred to as PRIME: Phase
Retrieval vIa the Majorization-minimization techniquE. They are preferred to
existing benchmark methods since at each iteration a simple surrogate problem
is solved with a closed-form solution that monotonically decreases the original
objective function. In total, four algorithms are proposed using different
majorization-minimization techniques. Experimental results validate that our
algorithms outperform existing methods in terms of successful recovery and mean
square error under various settings.
",0.0
150,2,30466,1412.5083,random forests,"Qiang Qiu, Guillermo Sapiro, Alex Bronstein",Random Forests Can Hash,"  Hash codes are a very efficient data representation needed to be able to cope
with the ever growing amounts of data. We introduce a random forest semantic
hashing scheme with information-theoretic code aggregation, showing for the
first time how random forest, a technique that together with deep learning have
shown spectacular results in classification, can also be extended to
large-scale retrieval. Traditional random forest fails to enforce the
consistency of hashes generated from each tree for the same class data, i.e.,
to preserve the underlying similarity, and it also lacks a principled way for
code aggregation across trees. We start with a simple hashing scheme, where
independently trained random trees in a forest are acting as hashing functions.
We the propose a subspace model as the splitting function, and show that it
enforces the hash consistency in a tree for data from the same class. We also
introduce an information-theoretic approach for aggregating codes of individual
trees into a single hash code, producing a near-optimal unique hash for each
class. Experiments on large-scale public datasets are presented, showing that
the proposed approach significantly outperforms state-of-the-art hashing
methods for retrieval tasks.
",5.0
151,3,219120,cs/9910019,database management system,"R.Baldoni, F. Quaglia, and M.Raynal","Consistent Checkpointing in Distributed Databases: Towards a Formal
  Approach","  Whether it is for audit or for recovery purposes, data checkpointing is an
important problem of distributed database systems. Actually, transactions
establish dependence relations on data checkpoints taken by data object
managers. So, given an arbitrary set of data checkpoints (including at least a
single data checkpoint from a data manager, and at most a data checkpoint from
each data manager), an important question is the following one: ``Can these
data checkpoints be members of a same consistent global checkpoint?''. This
paper answers this question by providing a necessary and sufficient condition
suited for database systems. Moreover, to show the usefulness of this
condition, two {\em non-intrusive} data checkpointing protocols are derived
from this condition. It is also interesting to note that this paper, by
exhibiting ``correspondences'', establishes a bridge between the data
object/transaction model and the process/message-passing model.
",3.0
152,11,70483,1807.02255,PageRank for web search,"Mohammad Masudur Rahman, Shamima Yeasmin and Chanchal K. Roy","Towards a Context-Aware IDE-Based Meta Search Engine for Recommendation
  about Programming Errors and Exceptions","  Study shows that software developers spend about 19% of their time looking
for information in the web during software development and maintenance.
Traditional web search forces them to leave the working environment (e.g., IDE)
and look for information in the web browser. It also does not consider the
context of the problems that the developers search solutions for. The frequent
switching between web browser and the IDE is both time-consuming and
distracting, and the keyword-based traditional web search often does not help
much in problem solving. In this paper, we propose an Eclipse IDE-based web
search solution that exploits the APIs provided by three popular web search
engines-- Google, Yahoo, Bing and a popular programming Q & A site, Stack
Overflow, and captures the content-relevance, context-relevance, popularity and
search engine confidence of each candidate result against the encountered
programming problems. Experiments with 75 programming errors and exceptions
using the proposed approach show that inclusion of different types of context
information associated with a given exception can enhance the recommendation
accuracy of a given exception. Experiments both with two existing approaches
and existing web search engines confirm that our approach can perform better
than them in terms of recall, mean precision and other performance measures
with little computational cost.
",5.0
153,4,79517,1812.08092,pre-trained language model,"Martin Gerlach, Francesc Font-Clos","A standardized Project Gutenberg corpus for statistical analysis of
  natural language and quantitative linguistics","  The use of Project Gutenberg (PG) as a text corpus has been extremely popular
in statistical analysis of language for more than 25 years. However, in
contrast to other major linguistic datasets of similar importance, no
consensual full version of PG exists to date. In fact, most PG studies so far
either consider only a small number of manually selected books, leading to
potential biased subsets, or employ vastly different pre-processing strategies
(often specified in insufficient details), raising concerns regarding the
reproducibility of published results. In order to address these shortcomings,
here we present the Standardized Project Gutenberg Corpus (SPGC), an open
science approach to a curated version of the complete PG data containing more
than 50,000 books and more than $3 \times 10^9$ word-tokens. Using different
sources of annotated metadata, we not only provide a broad characterization of
the content of PG, but also show different examples highlighting the potential
of SPGC for investigating language variability across time, subjects, and
authors. We publish our methodology in detail, the code to download and process
the data, as well as the obtained corpus itself on 3 different levels of
granularity (raw text, timeseries of word tokens, and counts of words). In this
way, we provide a reproducible, pre-processed, full-size version of Project
Gutenberg as a new scientific resource for corpus linguistics, natural language
processing, and information retrieval.
",5.0
154,7,72771,1808.0967,gradient boosting,"Erwan Fouillen, Claire Boyer, Maxime Sangnier",Proximal boosting and variants,"  Gradient boosting is a prediction method that iteratively combines weak
learners to produce a complex and accurate model. From an optimization point of
view, the learning procedure of gradient boosting mimics a gradient descent on
a functional variable. This paper proposes to build upon the proximal point
algorithm, when the empirical risk to minimize is not differentiable, in order
to introduce a novel boosting approach, called proximal boosting. Besides being
motivated by non-differentiable optimization, the proposed algorithm benefits
from algorithmic improvements such as controlling the approximation error and
Nesterov's acceleration, in the same way as gradient boosting [Grubb and
Bagnell, 2011, Biau et al., 2018]. This leads to two variants, respectively
called residual proximal boosting and accelerated proximal boosting.
Theoretical convergence is proved for the first two procedures under different
hypotheses on the empirical risk and advantages of leveraging proximal methods
for boosting are illustrated by numerical experiments on simulated and
real-world data. In particular, we exhibit a favorable comparison over gradient
boosting regarding convergence rate and prediction accuracy.
",5.0
155,15,108874,2001.10781,relevance feedback for imformation retrieval,"Prajna Upadhyay, Srikanta Bedathur, Tanmoy Chakraborty, Maya Ramanath",Aspect-based Academic Search using Domain-specific KB,"  Academic search engines allow scientists to explore related work relevant to
a given query. Often, the user is also aware of the ""aspect"" to retrieve a
relevant document. In such cases, existing search engines can be used by
expanding the query with terms describing that aspect. However, this approach
does not guarantee good results since plain keyword matches do not always imply
relevance. To address this issue, we define and solve a novel academic search
task, called ""aspect-based retrieval"", which allows the user to specify the
aspect along with the query to retrieve a ranked list of relevant documents.
The primary idea is to estimate a language model for the aspect as well as the
query using a domain-specific knowledge base and use a mixture of the two to
determine the relevance of the article. Our evaluation of the results over the
Open Research Corpus dataset shows that our method outperforms keyword-based
expansion of query with aspect with and without relevance feedback.
",2.0
156,0,140999,2012.06576,learning to rank with partitioned preference,Harrie Oosterhuis,"Learning from User Interactions with Rankings: A Unification of the
  Field","  Ranking systems form the basis for online search engines and recommendation
services. They process large collections of items, for instance web pages or
e-commerce products, and present the user with a small ordered selection. The
goal of a ranking system is to help a user find the items they are looking for
with the least amount of effort. Thus the rankings they produce should place
the most relevant or preferred items at the top of the ranking. Learning to
rank is a field within machine learning that covers methods which optimize
ranking systems w.r.t. this goal. Traditional supervised learning to rank
methods utilize expert-judgements to evaluate and learn, however, in many
situations such judgements are impossible or infeasible to obtain. As a
solution, methods have been introduced that perform learning to rank based on
user clicks instead. The difficulty with clicks is that they are not only
affected by user preferences, but also by what rankings were displayed.
Therefore, these methods have to prevent being biased by other factors than
user preference. This thesis concerns learning to rank methods based on user
clicks and specifically aims to unify the different families of these methods.
  As a whole, the second part of this thesis proposes a framework that bridges
many gaps between areas of online, counterfactual, and supervised learning to
rank. It has taken approaches, previously considered independent, and unified
them into a single methodology for widely applicable and effective learning to
rank from user clicks.
",1.0
157,18,42994,1606.00531,infomation retrieval time complexity,"Dong Yin, Kangwook Lee, Ramtin Pedarsani, Kannan Ramchandran",Fast and Robust Compressive Phase Retrieval with Sparse-Graph Codes,"  In this paper, we tackle the compressive phase retrieval problem in the
presence of noise. The noisy compressive phase retrieval problem is to recover
a $K$-sparse complex signal $s \in \mathbb{C}^n$, from a set of $m$ noisy
quadratic measurements: $ y_i=| a_i^H s |^2+w_i$, where $a_i^H\in\mathbb{C}^n$
is the $i$th row of the measurement matrix $A\in\mathbb{C}^{m\times n}$, and
$w_i$ is the additive noise to the $i$th measurement. We consider the regime
where $K=\beta n^\delta$, with constants $\beta>0$ and $\delta\in(0,1)$. We use
the architecture of PhaseCode algorithm, and robustify it using two schemes:
the almost-linear scheme and the sublinear scheme. We prove that with high
probability, the almost-linear scheme recovers $s$ with sample complexity
$\Theta(K \log(n))$ and computational complexity $\Theta(n \log(n))$, and the
sublinear scheme recovers $s$ with sample complexity $\Theta(K\log^3(n))$ and
computational complexity $\Theta(K\log^3(n))$. To the best of our knowledge,
this is the first scheme that achieves sublinear computational complexity for
compressive phase retrieval problem. Finally, we provide simulation results
that support our theoretical contributions.
",1.0
158,0,100822,1910.08795,learning to rank with partitioned preference,"Ekhine Irurozki, Jesus Lobo, Aritz Perez, Javier Del Ser",Rank aggregation for non-stationary data streams,"  We consider the problem of learning over non-stationary ranking streams. The
rankings can be interpreted as the preferences of a population and the
non-stationarity means that the distribution of preferences changes over time.
Our goal is to learn, in an online manner, the current distribution of
rankings. The bottleneck of this process is a rank aggregation problem.
  We propose a generalization of the Borda algorithm for non-stationary ranking
streams. Moreover, we give bounds on the minimum number of samples required to
output the ground truth with high probability. Besides, we show how the optimal
parameters are set. Then, we generalize the whole family of weighted voting
rules (the family to which Borda belongs) to situations in which some rankings
are more \textit{reliable} than others and show that this generalization can
solve the problem of rank aggregation over non-stationary data streams.
",2.0
159,5,36389,1509.04397,matrix completion,"Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh",Exponential Family Matrix Completion under Structural Constraints,"  We consider the matrix completion problem of recovering a structured matrix
from noisy and partial measurements. Recent works have proposed tractable
estimators with strong statistical guarantees for the case where the underlying
matrix is low--rank, and the measurements consist of a subset, either of the
exact individual entries, or of the entries perturbed by additive Gaussian
noise, which is thus implicitly suited for thin--tailed continuous data.
Arguably, common applications of matrix completion require estimators for (a)
heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)
for heterogeneous noise models (beyond Gaussian), which capture varied
uncertainty in the measurements, and (c) heterogeneous structural constraints
beyond low--rank, such as block--sparsity, or a superposition structure of
low--rank plus elementwise sparseness, among others. In this paper, we provide
a vastly unified framework for generalized matrix completion by considering a
matrix completion setting wherein the matrix entries are sampled from any
member of the rich family of exponential family distributions; and impose
general structural constraints on the underlying matrix, as captured by a
general regularizer $\mathcal{R}(.)$. We propose a simple convex regularized
$M$--estimator for the generalized framework, and provide a unified and novel
statistical analysis for this general class of estimators. We finally
corroborate our theoretical results on simulated datasets.
",5.0
160,10,69797,1806.08246,web archive,"Eric M\""uller-Budack, Kader Pustu-Iren, Sebastian Diering, Ralph
  Ewerth",Finding Person Relations in Image Data of the Internet Archive,"  The multimedia content in the World Wide Web is rapidly growing and contains
valuable information for many applications in different domains. For this
reason, the Internet Archive initiative has been gathering billions of
time-versioned web pages since the mid-nineties. However, the huge amount of
data is rarely labeled with appropriate metadata and automatic approaches are
required to enable semantic search. Normally, the textual content of the
Internet Archive is used to extract entities and their possible relations
across domains such as politics and entertainment, whereas image and video
content is usually neglected. In this paper, we introduce a system for person
recognition in image content of web news stored in the Internet Archive. Thus,
the system complements entity recognition in text and allows researchers and
analysts to track media coverage and relations of persons more precisely. Based
on a deep learning face recognition approach, we suggest a system that
automatically detects persons of interest and gathers sample material, which is
subsequently used to identify them in the image data of the Internet Archive.
We evaluate the performance of the face recognition system on an appropriate
standard benchmark dataset and demonstrate the feasibility of the approach with
two use cases.
",3.0
161,7,120909,2006.04059,gradient boosting,"Ji Feng, Yi-Xuan Xu, Yuan Jiang, Zhi-Hua Zhou",Soft Gradient Boosting Machine,"  Gradient Boosting Machine has proven to be one successful function
approximator and has been widely used in a variety of areas. However, since the
training procedure of each base learner has to take the sequential order, it is
infeasible to parallelize the training process among base learners for
speed-up. In addition, under online or incremental learning settings, GBMs
achieved sub-optimal performance due to the fact that the previously trained
base learners can not adapt with the environment once trained. In this work, we
propose the soft Gradient Boosting Machine (sGBM) by wiring multiple
differentiable base learners together, by injecting both local and global
objectives inspired from gradient boosting, all base learners can then be
jointly optimized with linear speed-up. When using differentiable soft decision
trees as base learner, such device can be regarded as an alternative version of
the (hard) gradient boosting decision trees with extra benefits. Experimental
results showed that, sGBM enjoys much higher time efficiency with better
accuracy, given the same base learner in both on-line and off-line settings.
",5.0
162,4,118860,2005.07877,pre-trained language model,"Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han",MicroNet for Efficient Language Modeling,"  It is important to design compact language models for efficient deployment.
We improve upon recent advances in both the language modeling domain and the
model-compression domain to construct parameter and computation efficient
language models. We use an efficient transformer-based architecture with
adaptive embedding and softmax, differentiable non-parametric cache, Hebbian
softmax, knowledge distillation, network pruning, and low-bit quantization. In
this paper, we provide the winning solution to the NeurIPS 2019 MicroNet
Challenge in the language modeling track. Compared to the baseline language
model provided by the MicroNet Challenge, our model is 90 times more
parameter-efficient and 36 times more computation-efficient while achieving the
required test perplexity of 35 on the Wikitext-103 dataset. We hope that this
work will aid future research into efficient language models, and we have
released our full source code at
https://github.com/mit-han-lab/neurips-micronet.
",4.0
163,5,71332,1807.0901,matrix completion,"Mokhtar Z. Alaya (MODAL'X, Univ Paris Nanterre) and Olga Klopp (ESSEC
  Business School and CREST)",Collective Matrix Completion,"  Matrix completion aims to reconstruct a data matrix based on observations of
a small number of its entries. Usually in matrix completion a single matrix is
considered, which can be, for example, a rating matrix in recommendation
system. However, in practical situations, data is often obtained from multiple
sources which results in a collection of matrices rather than a single one. In
this work, we consider the problem of collective matrix completion with
multiple and heterogeneous matrices, which can be count, binary, continuous,
etc. We first investigate the setting where, for each source, the matrix
entries are sampled from an exponential family distribution. Then, we relax the
assumption of exponential family distribution for the noise and we investigate
the distribution-free case. In this setting, we do not assume any specific
model for the observations. The estimation procedures are based on minimizing
the sum of a goodness-of-fit term and the nuclear norm penalization of the
whole collective matrix. We prove that the proposed estimators achieve fast
rates of convergence under the two considered settings and we corroborate our
results with numerical experiments.
",5.0
164,15,31821,1502.05535,relevance feedback for imformation retrieval,Dmytro Filatov and Taras Filatov,"Evolutionary algorithm based adaptive navigation in information
  retrieval interfaces","  In computer interfaces in general, especially in information retrieval tasks,
it is important to be able to quickly find and retrieve information. State of
the art approach, used, for example, in search engines, is not effective as it
introduces losses of meanings due to context to keywords back and forth
translation. Authors argue it increases the time and reduces the accuracy of
information retrieval compared to what it could be in the system that employs
modern information retrieval and text mining methods while presenting results
in an adaptive human- computer interface where system effectively learns what
operator needs through iterative interaction. In current work, a combination of
adaptive navigational interface and real time collaborative feedback analysis
for documents relevance weighting is proposed as an viable alternative to
prevailing ""telegraphic"" approach in information retrieval systems. Adaptive
navigation is provided through a dynamic links panel controlled by an
evolutionary algorithm. Documents relevance is initially established with
standard information retrieval techniques and is further refined in real time
through interaction of users with the system. Introduced concepts of
multidimensional Knowledge Map and Weighted Point of Interest allow finding
relevant documents and users with common interests through a trivial
calculation. Browsing search approach, the ability of the algorithm to adapt
navigation to users interests, collaborative refinement and the self-organising
features of the system are the main factors making such architecture effective
in various fields where non-structured knowledge shall be represented to the
users.
",3.0
165,1,23299,1311.6227,advanced search engine,"Debajyoti Mukhopadhyay, Manoj Sharma, Gajanan Joshi, Trupti Pagare,
  Adarsha Palwe",Experience of Developing a Meta-Semantic Search Engine,"  Thinking of todays web search scenario which is mainly keyword based, leads
to the need of effective and meaningful search provided by Semantic Web.
Existing search engines are vulnerable to provide relevant answers to users
query due to their dependency on simple data available in web pages. On other
hand, semantic search engines provide efficient and relevant results as the
semantic web manages information with well defined meaning using ontology. A
Meta-Search engine is a search tool that forwards users query to several
existing search engines and provides combined results by using their own page
ranking algorithm. SemanTelli is a meta semantic search engine that fetches
results from different semantic search engines such as Hakia, DuckDuckGo,
SenseBot through intelligent agents. This paper proposes enhancement of
SemanTelli with improved snippet analysis based page ranking algorithm and
support for image and news search.
",4.0
166,4,143430,2101.03392,pre-trained language model,"Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang",Generate Natural Language Explanations for Recommendation,"  Providing personalized explanations for recommendations can help users to
understand the underlying insight of the recommendation results, which is
helpful to the effectiveness, transparency, persuasiveness and trustworthiness
of recommender systems. Current explainable recommendation models mostly
generate textual explanations based on pre-defined sentence templates. However,
the expressiveness power of template-based explanation sentences are limited to
the pre-defined expressions, and manually defining the expressions require
significant human efforts. Motivated by this problem, we propose to generate
free-text natural language explanations for personalized recommendation. In
particular, we propose a hierarchical sequence-to-sequence model (HSS) for
personalized explanation generation. Different from conventional sentence
generation in NLP research, a great challenge of explanation generation in
e-commerce recommendation is that not all sentences in user reviews are of
explanation purpose. To solve the problem, we further propose an auto-denoising
mechanism based on topical item feature words for sentence generation.
Experiments on various e-commerce product domains show that our approach can
not only improve the recommendation accuracy, but also the explanation quality
in terms of the offline measures and feature words coverage. This research is
one of the initial steps to grant intelligent agents with the ability to
explain itself based on natural language sentences.
",4.0
167,14,144103,2101.0712,text summarization model,"Mohan Bharath B, Aravindh Gowtham B, Akhil M",Neural Abstractive Text Summarizer for Telugu Language,"  Abstractive Text Summarization is the process of constructing semantically
relevant shorter sentences which captures the essence of the overall meaning of
the source text. It is actually difficult and very time consuming for humans to
summarize manually large documents of text. Much of work in abstractive text
summarization is being done in English and almost no significant work has been
reported in Telugu abstractive text summarization. So, we would like to propose
an abstractive text summarization approach for Telugu language using Deep
learning. In this paper we are proposing an abstractive text summarization Deep
learning model for Telugu language. The proposed architecture is based on
encoder-decoder sequential models with attention mechanism. We have applied
this model on manually created dataset to generate a one sentence summary of
the source text and have got good results measured qualitatively.
",5.0
168,7,126633,2007.11975,gradient boosting,Nataly Brukhim and Elad Hazan,Online Boosting with Bandit Feedback,"  We consider the problem of online boosting for regression tasks, when only
limited information is available to the learner. We give an efficient regret
minimization method that has two implications: an online boosting algorithm
with noisy multi-point bandit feedback, and a new projection-free online convex
optimization algorithm with stochastic gradient, that improves state-of-the-art
guarantees in terms of efficiency.
",0.0
169,5,64995,1803.06234,matrix completion,"Ryota Kawasumi, Koujin Takeda","Approximate Method of Variational Bayesian Matrix
  Factorization/Completion with Sparse Prior","  We derive analytical expression of matrix factorization/completion solution
by variational Bayes method, under the assumption that observed matrix is
originally the product of low-rank dense and sparse matrices with additive
noise. We assume the prior of sparse matrix is Laplace distribution by taking
matrix sparsity into consideration. Then we use several approximations for
derivation of matrix factorization/completion solution. By our solution, we
also numerically evaluate the performance of sparse matrix reconstruction in
matrix factorization, and completion of missing matrix element in matrix
completion.
",4.0
170,4,15572,1210.2195,pre-trained language model,"Marina De Vos, Do\u{g}a Gizem K{\i}za, Johannes Oetsch, J\""org
  P\""uhrer, Hans Tompits",Annotating Answer-Set Programs in LANA?,"  While past research in answer-set programming (ASP) mainly focused on theory,
ASP solver technology, and applications, the present work situates itself in
the context of a quite recent research trend: development support for ASP. In
particular, we propose to augment answer-set programs with additional
meta-information formulated in a dedicated annotation language, called LANA.
This language allows the grouping of rules into coherent blocks and to specify
language signatures, types, pre- and postconditions, as well as unit tests for
such blocks. While these annotations are invisible to an ASP solver, as they
take the form of program comments, they can be interpreted by tools for
documentation, testing, and verification purposes, as well as to eliminate
sources of common programming errors by realising syntax checking or code
completion features. To demonstrate its versatility, we introduce two such
tools, viz. (i) ASPDOC, for generating an HTML documentation for a program
based on the annotated information, and (ii) ASPUNIT, for running and
monitoring unit tests on program blocks. LANA is also exploited in the SeaLion
system, an integrated development environment for ASP based on Eclipse. To
appear in Theory and Practice of Logic Programming
",0.0
171,2,87898,1905.03729,random forests,"Hanyuan Hang, Hongwei Wen",Best-scored Random Forest Density Estimation,"  This paper presents a brand new nonparametric density estimation strategy
named the best-scored random forest density estimation whose effectiveness is
supported by both solid theoretical analysis and significant experimental
performance. The terminology best-scored stands for selecting one density tree
with the best estimation performance out of a certain number of purely random
density tree candidates and we then name the selected one the best-scored
random density tree. In this manner, the ensemble of these selected trees that
is the best-scored random density forest can achieve even better estimation
results than simply integrating trees without selection. From the theoretical
perspective, by decomposing the error term into two, we are able to carry out
the following analysis: First of all, we establish the consistency of the
best-scored random density trees under $L_1$-norm. Secondly, we provide the
convergence rates of them under $L_1$-norm concerning with three different tail
assumptions, respectively. Thirdly, the convergence rates under
$L_{\infty}$-norm is presented. Last but not least, we also achieve the above
convergence rates analysis for the best-scored random density forest. When
conducting comparative experiments with other state-of-the-art density
estimation approaches on both synthetic and real data sets, it turns out that
our algorithm has not only significant advantages in terms of estimation
accuracy over other methods, but also stronger resistance to the curse of
dimensionality.
",3.0
172,12,120802,2006.03644,COVID-19 and social media,Abeer AlDayel and Walid Magdy,Stance Detection on Social Media: State of the Art and Trends,"  Stance detection on social media is an emerging opinion mining paradigm for
various social and political applications in which sentiment analysis may be
sub-optimal. There has been a growing research interest for developing
effective methods for stance detection methods varying among multiple
communities including natural language processing, web science, and social
computing. This paper surveys the work on stance detection within those
communities and situates its usage within current opinion mining techniques in
social media. It presents an exhaustive review of stance detection techniques
on social media, including the task definition, different types of targets in
stance detection, features set used, and various machine learning approaches
applied. The survey reports state-of-the-art results on the existing benchmark
datasets on stance detection, and discusses the most effective approaches. In
addition, this study explores the emerging trends and different applications of
stance detection on social media. The study concludes by discussing the gaps in
the current existing research and highlights the possible future directions for
stance detection on social media.
",1.0
173,2,88772,1905.0877,random forests,"Antoine H\'ebert, Timoth\'ee Gu\'edon, Tristan Glatard, Brigitte
  Jaumard","High-Resolution Road Vehicle Collision Prediction for the City of
  Montreal","  Road accidents are an important issue of our modern societies, responsible
for millions of deaths and injuries every year in the world. In Quebec only, in
2018, road accidents are responsible for 359 deaths and 33 thousands of
injuries. In this paper, we show how one can leverage open datasets of a city
like Montreal, Canada, to create high-resolution accident prediction models,
using big data analytics. Compared to other studies in road accident
prediction, we have a much higher prediction resolution, i.e., our models
predict the occurrence of an accident within an hour, on road segments defined
by intersections. Such models could be used in the context of road accident
prevention, but also to identify key factors that can lead to a road accident,
and consequently, help elaborate new policies.
  We tested various machine learning methods to deal with the severe class
imbalance inherent to accident prediction problems. In particular, we
implemented the Balanced Random Forest algorithm, a variant of the Random
Forest machine learning algorithm in Apache Spark. Interestingly, we found that
in our case, Balanced Random Forest does not perform significantly better than
Random Forest.
  Experimental results show that 85% of road vehicle collisions are detected by
our model with a false positive rate of 13%. The examples identified as
positive are likely to correspond to high-risk situations. In addition, we
identify the most important predictors of vehicle collisions for the area of
Montreal: the count of accidents on the same road segment during previous
years, the temperature, the day of the year, the hour and the visibility.
",3.0
174,6,116584,2004.11083,query expansion for imformation retrieval,"Bhawani Selvaretnam, Mohammed Belkhatir","Coupled intrinsic and extrinsic human language resource-based query
  expansion","  Poor information retrieval performance has often been attributed to the
query-document vocabulary mismatch problem which is defined as the difficulty
for human users to formulate precise natural language queries that are in line
with the vocabulary of the documents deemed relevant to a specific search goal.
To alleviate this problem, query expansion processes are applied in order to
spawn and integrate additional terms to an initial query. This requires
accurate identification of main query concepts to ensure the intended search
goal is duly emphasized and relevant expansion concepts are extracted and
included in the enriched query. Natural language queries have intrinsic
linguistic properties such as parts-of-speech labels and grammatical relations
which can be utilized in determining the intended search goal. Additionally,
extrinsic language-based resources such as ontologies are needed to suggest
expansion concepts semantically coherent with the query content. We present
here a query expansion framework which capitalizes on both linguistic
characteristics of user queries and ontology resources for query constituent
encoding, expansion concept extraction and concept weighting. A thorough
empirical evaluation on real-world datasets validates our approach against
unigram language model, relevance model and a sequential dependence based
technique.
",5.0
175,11,217821,cs/0511016,PageRank for web search,"Santo Fortunato, Marian Boguna, Alessandro Flammini, Filippo Menczer",How to make the top ten: Approximating PageRank from in-degree,"  PageRank has become a key element in the success of search engines, allowing
to rank the most important hits in the top screen of results. One key aspect
that distinguishes PageRank from other prestige measures such as in-degree is
its global nature. From the information provider perspective, this makes it
difficult or impossible to predict how their pages will be ranked. Consequently
a market has emerged for the optimization of search engine results. Here we
study the accuracy with which PageRank can be approximated by in-degree, a
local measure made freely available by search engines. Theoretical and
empirical analyses lead to conclude that given the weak degree correlations in
the Web link graph, the approximation can be relatively accurate, giving
service and information providers an effective new marketing tool.
",4.0
176,7,211471,2210.00736,gradient boosting,Clement Dombry and Jean-Jil Duchamps,A large sample theory for infinitesimal gradient boosting,"  Infinitesimal gradient boosting is defined as the vanishing-learning-rate
limit of the popular tree-based gradient boosting algorithm from machine
learning (Dombry and Duchamps, 2021). It is characterized as the solution of a
nonlinear ordinary differential equation in a infinite-dimensional function
space where the infinitesimal boosting operator driving the dynamics depends on
the training sample. We consider the asymptotic behavior of the model in the
large sample limit and prove its convergence to a deterministic process. This
infinite population limit is again characterized by a differential equation
that depends on the population distribution. We explore some properties of this
population limit: we prove that the dynamics makes the test error decrease and
we consider its long time behavior.
",4.0
177,19,119651,2005.1215,artificial intelligence for low carbon,"Petar Radanliev, David De Roure, Kevin Page, Max Van Kleek, Omar
  Santos, La Treall Maddox, Pete Burnap, Eirini Anthi, Carsten Maple","Design of a dynamic and self adapting system, supported with artificial
  intelligence, machine learning and real time intelligence for predictive
  cyber risk analytics in extreme environments, cyber risk in the colonisation
  of Mars","  Multiple governmental agencies and private organisations have made
commitments for the colonisation of Mars. Such colonisation requires complex
systems and infrastructure that could be very costly to repair or replace in
cases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber
security and risk models, and established mathematical formulas to identify the
best approach for developing a dynamic and self adapting system for predictive
cyber risk analytics supported with Artificial Intelligence and Machine
Learning and real time intelligence in edge computing. The paper presents a new
mathematical approach for integrating concepts for cognition engine design,
edge computing and Artificial Intelligence and Machine Learning to automate
anomaly detection. This engine instigates a step change by applying Artificial
Intelligence and Machine Learning embedded at the edge of IoT networks, to
deliver safe and functional real time intelligence for predictive cyber risk
analytics. This will enhance capacities for risk analytics and assists in the
creation of a comprehensive and systematic understanding of the opportunities
and threats that arise when edge computing nodes are deployed, and when
Artificial Intelligence and Machine Learning technologies are migrated to the
periphery of the internet and into local IoT networks.
",1.0
178,14,178950,2112.03203,text summarization model,"Dehao Tao, Yingzhu Xiong, Zhongliang Yang, Yongfeng Huang, Jin He and
  Kevin Song","An unsupervised extractive summarization method based on multi-round
  computation","  Text summarization methods have attracted much attention all the time. In
recent years, deep learning has been applied to text summarization, and it
turned out to be pretty effective. Most of the current text summarization
methods based on deep learning are supervised methods which need large-scale
datasets. However, large-scale datasets are difficult to obtain in practical
applications. In this paper, an unsupervised extractive text summarization
method based on multi-round calculation is proposed. Based on the directed
graph algorithm, we change the common method which calculates the sentence
ranking at one time to multi-round calculation, and we dynamically optimize the
relation of sentences after each round of calculation to reduce the redundancy
of summarization. Experiments are carried out on four data sets, each
separately containing Chinese, English, long and short texts. The experiment
results show that our method has better performance than other unsupervised
methods.
",5.0
179,4,191882,2204.03214,pre-trained language model,"Chandra Thapa and Seung Ick Jang and Muhammad Ejaz Ahmed and Seyit
  Camtepe and Josef Pieprzyk and Surya Nepal",Transformer-Based Language Models for Software Vulnerability Detection,"  The large transformer-based language models demonstrate excellent performance
in natural language processing. By considering the transferability of the
knowledge gained by these models in one domain to other related domains, and
the closeness of natural languages to high-level programming languages, such as
C/C++, this work studies how to leverage (large) transformer-based language
models in detecting software vulnerabilities and how good are these models for
vulnerability detection tasks. In this regard, firstly, a systematic (cohesive)
framework that details source code translation, model preparation, and
inference is presented. Then, an empirical analysis is performed with software
vulnerability datasets with C/C++ source codes having multiple vulnerabilities
corresponding to the library function call, pointer usage, array usage, and
arithmetic expression. Our empirical results demonstrate the good performance
of the language models in vulnerability detection. Moreover, these language
models have better performance metrics, such as F1-score, than the contemporary
models, namely bidirectional long short-term memory and bidirectional gated
recurrent unit. Experimenting with the language models is always challenging
due to the requirement of computing resources, platforms, libraries, and
dependencies. Thus, this paper also analyses the popular platforms to
efficiently fine-tune these models and present recommendations while choosing
the platforms.
",5.0
180,3,129104,2008.09268,database management system,"Meihui Zhang, Zhongle Xie, Cong Yue, Ziyue Zhong",Spitz: A Verifiable Database System,"  Databases in the past have helped businesses maintain and extract insights
from their data. Today, it is common for a business to involve multiple
independent, distrustful parties. This trend towards decentralization
introduces a new and important requirement to databases: the integrity of the
data, the history, and the execution must be protected. In other words, there
is a need for a new class of database systems whose integrity can be verified
(or verifiable databases).
  In this paper, we identify the requirements and the design challenges of
verifiable databases.We observe that the main challenges come from the need to
balance data immutability, tamper evidence, and performance. We first consider
approaches that extend existing OLTP and OLAP systems with support for
verification. We next examine a clean-slate approach, by describing a new
system, Spitz, specifically designed for efficiently supporting immutable and
tamper-evident transaction management. We conduct a preliminary performance
study of both approaches against a baseline system, and provide insights on
their performance.
",4.0
181,13,16318,1211.6166,social network analysis with natrual language processing,"Tao Zhu, David Phipps, Adam Pridgen, Jedidiah R. Crandall, Dan S.
  Wallach",Tracking and Quantifying Censorship on a Chinese Microblogging Site,"  We present measurements and analysis of censorship on Weibo, a popular
microblogging site in China. Since we were limited in the rate at which we
could download posts, we identified users likely to participate in sensitive
topics and recursively followed their social contacts. We also leveraged new
natural language processing techniques to pick out trending topics despite the
use of neologisms, named entities, and informal language usage in Chinese
social media. We found that Weibo dynamically adapts to the changing interests
of its users through multiple layers of filtering. The filtering includes both
retroactively searching posts by keyword or repost links to delete them, and
rejecting posts as they are posted. The trend of sensitive topics is
short-lived, suggesting that the censorship is effective in stopping the
""viral"" spread of sensitive issues. We also give evidence that sensitive topics
in Weibo only scarcely propagate beyond a core of sensitive posters.
",1.0
182,12,129447,2008.11238,COVID-19 and social media,"Vishal Dey, Peter Krasniak, Minh Nguyen, Clara Lee and Xia Ning","A Pipeline to Understand Emerging Illness via Social Media Data
  Analysis: A Case Study on Breast Implant Illness","  Background: A new illness could first come to the public attention over
social media before it is medically defined, formally documented or
systematically studied. One example is a phenomenon known as breast implant
illness (BII) that has been extensively discussed on social media, though
vaguely defined in medical literature. Objectives: The objective of this study
is to construct a data analysis pipeline to understand emerging illness using
social media data, and to apply the pipeline to understand key attributes of
BII. Methods: We conducted a pipeline of social media data analysis using
Natural Language Processing (NLP) and topic modeling. We extracted mentions
related to signs/symptoms, diseases/disorders and medical procedures using the
Clinical Text Analysis and Knowledge Extraction System (cTAKES) from social
media data. We mapped the mentions to standard medical concepts. We summarized
mapped concepts to topics using Latent Dirichlet Allocation (LDA). Finally, we
applied this pipeline to understand BII from several BII-dedicated social media
sites. Results: Our pipeline identified topics related to toxicity, cancer and
mental health issues that are highly associated with BII. Our pipeline also
shows that cancers, autoimmune disorders and mental health problems are
emerging concerns associated with breast implants based on social media
discussions. The pipeline also identified mentions such as rupture, infection,
pain and fatigue as common self-reported issues among the public, as well as
toxicity from silicone implants. Conclusions: Our study could inspire future
work studying the suggested symptoms and factors of BII. Our study provides the
first analysis and derived knowledge of BII from social media using NLP
techniques, and demonstrates the potential of using social media information to
better understand similar emerging illnesses.
",2.0
183,9,160192,2106.07719,language model for long documents,"Mahdi Hajiaghayi, Monir Hajiaghayi, Mark Bolin",Unbiased Sentence Encoder For Large-Scale Multi-lingual Search Engines,"  In this paper, we present a multi-lingual sentence encoder that can be used
in search engines as a query and document encoder. This embedding enables a
semantic similarity score between queries and documents that can be an
important feature in document ranking and relevancy. To train such a customized
sentence encoder, it is beneficial to leverage users search data in the form of
query-document clicked pairs however, we must avoid relying too much on search
click data as it is biased and does not cover many unseen cases. The search
data is heavily skewed towards short queries and for long queries is small and
often noisy. The goal is to design a universal multi-lingual encoder that works
for all cases and covers both short and long queries. We select a number of
public NLI datasets in different languages and translation data and together
with user search data we train a language model using a multi-task approach. A
challenge is that these datasets are not homogeneous in terms of content, size
and the balance ratio. While the public NLI datasets are usually two-sentence
based with the same portion of positive and negative pairs, the user search
data can contain multi-sentence documents and only positive pairs. We show how
multi-task training enables us to leverage all these datasets and exploit
knowledge sharing across these tasks.
",2.0
184,18,49776,1701.08222,infomation retrieval time complexity,"Ayush Bhandari, Aurelien Bourquard and Ramesh Raskar","Sampling Without Time: Recovering Echoes of Light via Temporal Phase
  Retrieval","  This paper considers the problem of sampling and reconstruction of a
continuous-time sparse signal without assuming the knowledge of the sampling
instants or the sampling rate. This topic has its roots in the problem of
recovering multiple echoes of light from its low-pass filtered and
auto-correlated, time-domain measurements. Our work is closely related to the
topic of sparse phase retrieval and in this context, we discuss the advantage
of phase-free measurements. While this problem is ill-posed, cues based on
physical constraints allow for its appropriate regularization. We validate our
theory with experiments based on customized, optical time-of-flight imaging
sensors. What singles out our approach is that our sensing method allows for
temporal phase retrieval as opposed to the usual case of spatial phase
retrieval. Preliminary experiments and results demonstrate a compelling
capability of our phase-retrieval based imaging device.
",0.0
185,11,13259,1205.6343,PageRank for web search,"K. M. Frahm, A. D. Chepelianskii and D. L. Shepelyansky",PageRank of integers,"  We build up a directed network tracing links from a given integer to its
divisors and analyze the properties of the Google matrix of this network. The
PageRank vector of this matrix is computed numerically and it is shown that its
probability is inversely proportional to the PageRank index thus being similar
to the Zipf law and the dependence established for the World Wide Web. The
spectrum of the Google matrix of integers is characterized by a large gap and a
relatively small number of nonzero eigenvalues. A simple semi-analytical
expression for the PageRank of integers is derived that allows to find this
vector for matrices of billion size. This network provides a new PageRank order
of integers.
",5.0
186,4,186306,2202.08373,pre-trained language model,"Kebing Jin, Huaixun Chen, Hankz Hankui Zhuo",Text-Based Action-Model Acquisition for Planning,"  Although there have been approaches that are capable of learning action
models from plan traces, there is no work on learning action models from
textual observations, which is pervasive and much easier to collect from
real-world applications compared to plan traces. In this paper we propose a
novel approach to learning action models from natural language texts by
integrating Constraint Satisfaction and Natural Language Processing techniques.
Specifically, we first build a novel language model to extract plan traces from
texts, and then build a set of constraints to generate action models based on
the extracted plan traces. After that, we iteratively improve the language
model and constraints until we achieve the convergent language model and action
models. We empirically exhibit that our approach is both effective and
efficient.
",5.0
187,18,10941,1112.2015,infomation retrieval time complexity,Anamika Sharma,"A Framework for Picture Extraction on Search Engine Improved and
  Meaningful Result","  Searching is an important tool of information gathering, if information is in
the form of picture than it play a major role to take quick action and easy to
memorize. This is a human tendency to retain more picture than text. The
complexity and the occurrence of variety of query can give variation in result
and provide the humans to learn something new or get confused. This paper
presents a development of a framework that will focus on recourse
identification for the user so that they can get faster access with accurate &
concise results on time and analysis of the change that is evident as the
scenario changes from text to picture retrieval. This paper also provides a
glimpse how to get accurate picture information in advance and extended
technologies searching framework. The new challenges and design techniques of
picture retrieval systems are also suggested in this paper.
",1.0
188,5,28174,1408.1717,matrix completion,"Vassilis Kalofolias, Xavier Bresson, Michael Bronstein, Pierre
  Vandergheynst",Matrix Completion on Graphs,"  The problem of finding the missing values of a matrix given a few of its
entries, called matrix completion, has gathered a lot of attention in the
recent years. Although the problem under the standard low rank assumption is
NP-hard, Cand\`es and Recht showed that it can be exactly relaxed if the number
of observed entries is sufficiently large. In this work, we introduce a novel
matrix completion model that makes use of proximity information about rows and
columns by assuming they form communities. This assumption makes sense in
several real-world problems like in recommender systems, where there are
communities of people sharing preferences, while products form clusters that
receive similar ratings. Our main goal is thus to find a low-rank solution that
is structured by the proximities of rows and columns encoded by graphs. We
borrow ideas from manifold learning to constrain our solution to be smooth on
these graphs, in order to implicitly force row and column proximities. Our
matrix recovery model is formulated as a convex non-smooth optimization
problem, for which a well-posed iterative scheme is provided. We study and
evaluate the proposed matrix completion on synthetic and real data, showing
that the proposed structured low-rank recovery model outperforms the standard
matrix completion model in many situations.
",5.0
189,12,65984,1804.0354,COVID-19 and social media,Arkaitz Zubiaga,Mining Social Media for Newsgathering: A Review,"  Social media is becoming an increasingly important data source for learning
about breaking news and for following the latest developments of ongoing news.
This is in part possible thanks to the existence of mobile devices, which
allows anyone with access to the Internet to post updates from anywhere,
leading in turn to a growing presence of citizen journalism. Consequently,
social media has become a go-to resource for journalists during the process of
newsgathering. Use of social media for newsgathering is however challenging,
and suitable tools are needed in order to facilitate access to useful
information for reporting. In this paper, we provide an overview of research in
data mining and natural language processing for mining social media for
newsgathering. We discuss five different areas that researchers have worked on
to mitigate the challenges inherent to social media newsgathering: news
discovery, curation of news, validation and verification of content,
newsgathering dashboards, and other tasks. We outline the progress made so far
in the field, summarise the current challenges as well as discuss future
directions in the use of computational journalism to assist with social media
newsgathering. This review is relevant to computer scientists researching news
in social media as well as for interdisciplinary researchers interested in the
intersection of computer science and journalism.
",1.0
190,1,50277,1702.03991,advanced search engine,"Enrique Orduna-Malea, Alberto Martin-Martin and Emilio Delgado
  Lopez-Cozar",Google Scholar and the gray literature: A reply to Bonato's review,"  Recently, a review concluded that Google Scholar (GS) is not a suitable
source of information ""for identifying recent conference papers or other gray
literature publications"". The goal of this letter is to demonstrate that GS can
be an effective tool to search and find gray literature, as long as appropriate
search strategies are used. To do this, we took as examples the same two case
studies used by the original review, describing first how GS processes
original's search strategies, then proposing alternative search strategies, and
finally generalizing each case study to compose a general search procedure
aimed at finding gray literature in Google Scholar for two wide selected case
studies: a) all contributions belonging to a congress (the ASCO Annual
Meeting); and b) indexed guidelines as well as gray literature within medical
institutions (National Institutes of Health) and governmental agencies (U.S.
Department of Health & Human Services). The results confirm that original
search strategies were undertrained offering misleading results and erroneous
conclusions. Google Scholar lacks many of the advanced search features
available in other bibliographic databases (such as Pubmed), however, it is one
thing to have a friendly search experience, and quite another to find gray
literature. We finally conclude that Google Scholar is a powerful tool for
searching gray literature, as long as the users are familiar with all the
possibilities it offers as a search engine. Poorly formulated searches will
undoubtedly return misleading results.
",3.0
191,11,36789,1510.00819,PageRank for web search,Jai Manral,Intelligent Search Optimization using Artificial Fuzzy Logics,"  Information on the web is prodigious; searching relevant information is
difficult making web users to rely on search engines for finding relevant
information on the web. Search engines index and categorize web pages according
to their contents using crawlers and rank them accordingly. For given user
query they retrieve millions of webpages and display them to users according to
web-page rank. Every search engine has their own algorithms based on certain
parameters for ranking web-pages. Search Engine Optimization (SEO) is that
technique by which webmasters try to improve ranking of their websites by
optimizing it according to search engines ranking parameters. It is the aim of
this research to identify the most popular SEO techniques used by search
engines for ranking web-pages and to establish their importance for indexing
and categorizing web data. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction.
  In the accomplished research, a web based Meta search engine is proposed to
aggregates search results from different search engines and rank web-pages
based on new page ranking algorithm which will assign heuristic page rank to
web-pages based on SEO parameters such as title tag, Meta description, sitemap
etc. The research also provides insight into techniques which webmasters can
use for better ranking their websites in Google and Bing.
  Initial results has shown that using certain SEO parameters in present
ranking algorithm helps in retrieving more useful results for user queries.
These results generated from Meta search engine outperformed existing search
engines in terms of better retrieved search results and high precision.
",3.0
192,4,126232,2007.09645,pre-trained language model,Kennedy E. Ehimwenma and Sujatha Krishnamoorthy,"Design and Analysis of a Multi-Agent E-Learning System Using Prometheus
  Design Tool","  Agent unified modeling languages (AUML) are agent-oriented approaches that
supports the specification, design, visualization and documentation of an
agent-based system. This paper presents the use of Prometheus AUML approach for
the modeling of a Pre-assessment System of five interactive agents. The
Pre-assessment System, as previously reported, is a multi-agent based
e-learning system that is developed to support the assessment of prior learning
skills in students so as to classify their skills and make recommendation for
their learning. This paper discusses the detailed design approach of the system
in a step-by-step manner; and domain knowledge abstraction and organization in
the system. In addition, the analysis of the data collated and models of
prediction for future pre-assessment results are also presented.
",0.0
193,10,154702,2104.14041,web archive,"Dhruv Patel, Alexander C. Nwala, Michael L. Nelson, Michele C. Weigle","What Did It Look Like: A service for creating website timelapses using
  the Memento framework","  Popular web pages are archived frequently, which makes it difficult to
visualize the progression of the site through the years at web archives. The
What Did It Look Like (WDILL) Twitter bot shows web page transitions by
creating a timelapse of a given website using one archived copy from each
calendar year. Originally implemented in 2015, we recently added new features
to WDILL, such as date range requests, diversified memento selection, updated
visualizations, and sharing visualizations to Instagram. This would allow
scholars and the general public to explore the temporal nature of web archives.
",4.0
194,6,79229,1812.06082,query expansion for imformation retrieval,"Fl\'avio Martins, Jo\~ao Magalh\~aes, Jamie Callan",Modeling Temporal Evidence from External Collections,"  Newsworthy events are broadcast through multiple mediums and prompt the
crowds to produce comments on social media. In this paper, we propose to
leverage on this behavioral dynamics to estimate the most relevant time periods
for an event (i.e., query). Recent advances have shown how to improve the
estimation of the temporal relevance of such topics. In this approach, we build
on two major novelties. First, we mine temporal evidences from hundreds of
external sources into topic-based external collections to improve the
robustness of the detection of relevant time periods. Second, we propose a
formal retrieval model that generalizes the use of the temporal dimension
across different aspects of the retrieval process. In particular, we show that
temporal evidence of external collections can be used to (i) infer a topic's
temporal relevance, (ii) select the query expansion terms, and (iii) re-rank
the final results for improved precision. Experiments with TREC Microblog
collections show that the proposed time-aware retrieval model makes an
effective and extensive use of the temporal dimension to improve search results
over the most recent temporal models. Interestingly, we observe a strong
correlation between precision and the temporal distribution of retrieved and
relevant documents.
",3.0
195,8,98751,1909.11793,node embedding for graph,"John Palowitch, Bryan Perozzi","MONET: Debiasing Graph Embeddings via the Metadata-Orthogonal Training
  Unit","  Are Graph Neural Networks (GNNs) fair? In many real world graphs, the
formation of edges is related to certain node attributes (e.g. gender,
community, reputation). In this case, standard GNNs using these edges will be
biased by this information, as it is encoded in the structure of the adjacency
matrix itself. In this paper, we show that when metadata is correlated with the
formation of node neighborhoods, unsupervised node embedding dimensions learn
this metadata. This bias implies an inability to control for important
covariates in real-world applications, such as recommendation systems. To solve
these issues, we introduce the Metadata-Orthogonal Node Embedding Training
(MONET) unit, a general model for debiasing embeddings of nodes in a graph.
MONET achieves this by ensuring that the node embeddings are trained on a
hyperplane orthogonal to that of the node metadata. This effectively organizes
unstructured embedding dimensions into an interpretable topology-only,
metadata-only division with no linear interactions. We illustrate the
effectiveness of MONET though our experiments on a variety of real world
graphs, which shows that our method can learn and remove the effect of
arbitrary covariates in tasks such as preventing the leakage of political party
affiliation in a blog network, and thwarting the gaming of embedding-based
recommendation systems.
",5.0
196,6,3591,908.2588,query expansion for imformation retrieval,"Davood Rafiei, Haobin Li",Wild Card Queries for Searching Resources on the Web,"  We propose a domain-independent framework for searching and retrieving facts
and relationships within natural language text sources. In this framework, an
extraction task over a text collection is expressed as a query that combines
text fragments with wild cards, and the query result is a set of facts in the
form of unary, binary and general $n$-ary tuples. A significance of our
querying mechanism is that, despite being both simple and declarative, it can
be applied to a wide range of extraction tasks. A problem in querying natural
language text though is that a user-specified query may not retrieve enough
exact matches. Unlike term queries which can be relaxed by removing some of the
terms (as is done in search engines), removing terms from a wild card query
without ruining its meaning is more challenging. Also, any query expansion has
the potential to introduce false positives. In this paper, we address the
problem of query expansion, and also analyze a few ranking alternatives to
score the results and to remove false positives. We conduct experiments and
report an evaluation of the effectiveness of our querying and scoring
functions.
",4.0
197,3,8530,1105.193,database management system,"Anisoara Nica, Fabian Suchanek (INRIA Saclay - Ile de France), Aparna
  Varde",Emerging multidisciplinary research across database management systems,"  The database community is exploring more and more multidisciplinary avenues:
Data semantics overlaps with ontology management; reasoning tasks venture into
the domain of artificial intelligence; and data stream management and
information retrieval shake hands, e.g., when processing Web click-streams.
These new research avenues become evident, for example, in the topics that
doctoral students choose for their dissertations. This paper surveys the
emerging multidisciplinary research by doctoral students in database systems
and related areas. It is based on the PIKM 2010, which is the 3rd Ph.D.
workshop at the International Conference on Information and Knowledge
Management (CIKM). The topics addressed include ontology development, data
streams, natural language processing, medical databases, green energy, cloud
computing, and exploratory search. In addition to core ideas from the workshop,
we list some open research questions in these multidisciplinary areas.
",3.0
198,8,77739,1811.088,node embedding for graph,"Mahsa Ghorbani, Mahdieh Soleymani Baghshah, Hamid R. Rabiee","MGCN: Semi-supervised Classification in Multi-layer Graphs with Graph
  Convolutional Networks","  Graph embedding is an important approach for graph analysis tasks such as
node classification and link prediction. The goal of graph embedding is to find
a low dimensional representation of graph nodes that preserves the graph
information. Recent methods like Graph Convolutional Network (GCN) try to
consider node attributes (if available) besides node relations and learn node
embeddings for unsupervised and semi-supervised tasks on graphs. On the other
hand, multi-layer graph analysis has been received attention recently. However,
the existing methods for multi-layer graph embedding cannot incorporate all
available information (like node attributes). Moreover, most of them consider
either type of nodes or type of edges, and they do not treat within and between
layer edges differently. In this paper, we propose a method called MGCN that
utilizes the GCN for multi-layer graphs. MGCN embeds nodes of multi-layer
graphs using both within and between layers relations and nodes attributes. We
evaluate our method on the semi-supervised node classification task.
Experimental results demonstrate the superiority of the proposed method to
other multi-layer and single-layer competitors and also show the positive
effect of using cross-layer edges.
",5.0
199,3,18301,1302.499,database management system,"Michael S. K. M. Wong, C. J. Butz, Yang Xiang",A Method for Implementing a Probabilistic Model as a Relational Database,"  This paper discusses a method for implementing a probabilistic inference
system based on an extended relational data model. This model provides a
unified approach for a variety of applications such as dynamic programming,
solving sparse linear equations, and constraint propagation. In this framework,
the probability model is represented as a generalized relational database.
Subsequent probabilistic requests can be processed as standard relational
queries. Conventional database management systems can be easily adopted for
implementing such an approximate reasoning system.
",5.0
200,14,83900,1903.02861,text summarization model,Milad Moradi,Small-world networks for summarization of biomedical articles,"  In recent years, many methods have been developed to identify important
portions of text documents. Summarization tools can utilize these methods to
extract summaries from large volumes of textual information. However, to
identify concepts representing central ideas within a text document and to
extract the most informative sentences that best convey those concepts still
remain two crucial tasks in summarization methods. In this paper, we introduce
a graph-based method to address these two challenges in the context of
biomedical text summarization. We show that how a summarizer can discover
meaningful concepts within a biomedical text document using the Helmholtz
principle. The summarizer considers the meaningful concepts as the main topics
and constructs a graph based on the topics that the sentences share. The
summarizer can produce an informative summary by extracting those sentences
having higher values of the degree. We assess the performance of our method for
summarization of biomedical articles using the Recall-Oriented Understudy for
Gisting Evaluation (ROUGE) toolkit. The results show that the degree can be a
useful centrality measure to identify important sentences in this type of
graph-based modelling. Our method can improve the performance of biomedical
text summarization compared to some state-of-the-art and publicly available
summarizers. Combining a concept-based modelling strategy and a graph-based
approach to sentence extraction, our summarizer can produce summaries with the
highest scores of informativeness among the comparison methods. This research
work can be regarded as a start point to the study of small-world networks in
summarization of biomedical texts.
",5.0
201,2,125572,2007.06379,random forests,"S. Ilker Birbil, Mert Edali, Birol Yuceoglu",Rule Covering for Interpretation and Boosting,"  We propose two algorithms for interpretation and boosting of tree-based
ensemble methods. Both algorithms make use of mathematical programming models
that are constructed with a set of rules extracted from an ensemble of decision
trees. The objective is to obtain the minimum total impurity with the least
number of rules that cover all the samples. The first algorithm uses the
collection of decision trees obtained from a trained random forest model. Our
numerical results show that the proposed rule covering approach selects only a
few rules that could be used for interpreting the random forest model.
Moreover, the resulting set of rules closely matches the accuracy level of the
random forest model. Inspired by the column generation algorithm in linear
programming, our second algorithm uses a rule generation scheme for boosting
decision trees. We use the dual optimal solutions of the linear programming
models as sample weights to obtain only those rules that would improve the
accuracy. With a computational study, we observe that our second algorithm
performs competitively with the other well-known boosting methods. Our
implementations also demonstrate that both algorithms can be trivially coupled
with the existing random forest and decision tree packages.
",3.0
202,9,24369,1401.3896,language model for long documents,"Oren Kurland, Eyal Krikon","The Opposite of Smoothing: A Language Model Approach to Ranking
  Query-Specific Document Clusters","  Exploiting information induced from (query-specific) clustering of
top-retrieved documents has long been proposed as a means for improving
precision at the very top ranks of the returned results. We present a novel
language model approach to ranking query-specific clusters by the presumed
percentage of relevant documents that they contain. While most previous cluster
ranking approaches focus on the cluster as a whole, our model utilizes also
information induced from documents associated with the cluster. Our model
substantially outperforms previous approaches for identifying clusters
containing a high relevant-document percentage. Furthermore, using the model to
produce document ranking yields precision-at-top-ranks performance that is
consistently better than that of the initial ranking upon which clustering is
performed. The performance also favorably compares with that of a
state-of-the-art pseudo-feedback-based retrieval method.
",1.0
203,14,139954,2012.01747,text summarization model,"Prithwiraj Bhattacharjee, Avi Mallick, Md Saiful Islam,
  Marium-E-Jannat","Bengali Abstractive News Summarization(BANS): A Neural Attention
  Approach","  Abstractive summarization is the process of generating novel sentences based
on the information extracted from the original text document while retaining
the context. Due to abstractive summarization's underlying complexities, most
of the past research work has been done on the extractive summarization
approach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)
model, abstractive summarization becomes more viable. Although a significant
number of notable research has been done in the English language based on
abstractive summarization, only a couple of works have been done on Bengali
abstractive news summarization (BANS). In this article, we presented a seq2seq
based Long Short-Term Memory (LSTM) network model with attention at
encoder-decoder. Our proposed system deploys a local attention-based model that
produces a long sequence of words with lucid and human-like generated sentences
with noteworthy information of the original document. We also prepared a
dataset of more than 19k articles and corresponding human-written summaries
collected from bangla.bdnews24.com1 which is till now the most extensive
dataset for Bengali news document summarization and publicly published in
Kaggle2. We evaluated our model qualitatively and quantitatively and compared
it with other published results. It showed significant improvement in terms of
human evaluation scores with state-of-the-art approaches for BANS.
",5.0
204,9,158965,2106.03379,language model for long documents,"Hongyu Gong, Vishrav Chaudhary, Yuqing Tang, Francisco Guzm\'an","LAWDR: Language-Agnostic Weighted Document Representations from
  Pre-trained Models","  Cross-lingual document representations enable language understanding in
multilingual contexts and allow transfer learning from high-resource to
low-resource languages at the document level. Recently large pre-trained
language models such as BERT, XLM and XLM-RoBERTa have achieved great success
when fine-tuned on sentence-level downstream tasks. It is tempting to apply
these cross-lingual models to document representation learning. However, there
are two challenges: (1) these models impose high costs on long document
processing and thus many of them have strict length limit; (2) model
fine-tuning requires extra data and computational resources, which is not
practical in resource-limited settings. In this work, we address these
challenges by proposing unsupervised Language-Agnostic Weighted Document
Representations (LAWDR). We study the geometry of pre-trained sentence
embeddings and leverage it to derive document representations without
fine-tuning. Evaluated on cross-lingual document alignment, LAWDR demonstrates
comparable performance to state-of-the-art models on benchmark datasets.
",4.0
205,11,219235,math/0607507,PageRank for web search,"N. Litvak, W.R.W. Scheinhardt and Y. Volkovich","In-Degree and PageRank of Web pages: Why do they follow similar power
  laws?","  The PageRank is a popularity measure designed by Google to rank Web pages.
Experiments confirm that the PageRank obeys a `power law' with the same
exponent as the In-Degree. This paper presents a novel mathematical model that
explains this phenomenon. The relation between the PageRank and In-Degree is
modelled through a stochastic equation, which is inspired by the original
definition of the PageRank, and is analogous to the well-known distributional
identity for the busy period in the M/G/1 queue. Further, we employ the theory
of regular variation and Tauberian theorems to analytically prove that the tail
behavior of the PageRank and the In-Degree differ only by a multiplicative
factor, for which we derive a closed-form expression. Our analytical results
are in good agreement with experimental data.
",4.0
206,8,73500,1809.04234,node embedding for graph,"Liheng Chen, Yanru Qu, Zhenghui Wang, Lin Qiu, Weinan Zhang, Ken Chen,
  Shaodian Zhang, Yong Yu",Sampled in Pairs and Driven by Text: A New Graph Embedding Framework,"  In graphs with rich texts, incorporating textual information with structural
information would benefit constructing expressive graph embeddings. Among
various graph embedding models, random walk (RW)-based is one of the most
popular and successful groups. However, it is challenged by two issues when
applied on graphs with rich texts: (i) sampling efficiency: deriving from the
training objective of RW-based models (e.g., DeepWalk and node2vec), we show
that RW-based models are likely to generate large amounts of redundant training
samples due to three main drawbacks. (ii) text utilization: these models have
difficulty in dealing with zero-shot scenarios where graph embedding models
have to infer graph structures directly from texts. To solve these problems, we
propose a novel framework, namely Text-driven Graph Embedding with Pairs
Sampling (TGE-PS). TGE-PS uses Pairs Sampling (PS) to improve the sampling
strategy of RW, being able to reduce ~99% training samples while preserving
competitive performance. TGE-PS uses Text-driven Graph Embedding (TGE), an
inductive graph embedding approach, to generate node embeddings from texts.
Since each node contains rich texts, TGE is able to generate high-quality
embeddings and provide reasonable predictions on existence of links to unseen
nodes. We evaluate TGE-PS on several real-world datasets, and experiment
results demonstrate that TGE-PS produces state-of-the-art results on both
traditional and zero-shot link prediction tasks.
",5.0
207,2,55082,1706.09865,random forests,"C.H. Bryan Liu, Benjamin Paul Chamberlain, Duncan A. Little, Angelo
  Cardoso","Generalising Random Forest Parameter Optimisation to Include Stability
  and Cost","  Random forests are among the most popular classification and regression
methods used in industrial applications. To be effective, the parameters of
random forests must be carefully tuned. This is usually done by choosing values
that minimize the prediction error on a held out dataset. We argue that error
reduction is only one of several metrics that must be considered when
optimizing random forest parameters for commercial applications. We propose a
novel metric that captures the stability of random forests predictions, which
we argue is key for scenarios that require successive predictions. We motivate
the need for multi-criteria optimization by showing that in practical
applications, simply choosing the parameters that lead to the lowest error can
introduce unnecessary costs and produce predictions that are not stable across
independent runs. To optimize this multi-criteria trade-off, we present a new
framework that efficiently finds a principled balance between these three
considerations using Bayesian optimisation. The pitfalls of optimising forest
parameters purely for error reduction are demonstrated using two publicly
available real world datasets. We show that our framework leads to parameter
settings that are markedly different from the values discovered by error
reduction metrics.
",3.0
208,17,97066,1909.02436,robustness of neutral networks,"Alfred Laugros, Alice Caplier, Matthieu Ospici","Are Adversarial Robustness and Common Perturbation Robustness
  Independent Attributes ?","  Neural Networks have been shown to be sensitive to common perturbations such
as blur, Gaussian noise, rotations, etc. They are also vulnerable to some
artificial malicious corruptions called adversarial examples. The adversarial
examples study has recently become very popular and it sometimes even reduces
the term ""adversarial robustness"" to the term ""robustness"". Yet, we do not know
to what extent the adversarial robustness is related to the global robustness.
Similarly, we do not know if a robustness to various common perturbations such
as translations or contrast losses for instance, could help with adversarial
corruptions. We intend to study the links between the robustnesses of neural
networks to both perturbations. With our experiments, we provide one of the
first benchmark designed to estimate the robustness of neural networks to
common perturbations. We show that increasing the robustness to carefully
selected common perturbations, can make neural networks more robust to unseen
common perturbations. We also prove that adversarial robustness and robustness
to common perturbations are independent. Our results make us believe that
neural network robustness should be addressed in a broader sense.
",5.0
209,14,162133,2106.15876,text summarization model,"Paheli Bhattacharya and Soham Poddar and Koustav Rudra and Kripabandhu
  Ghosh and Saptarshi Ghosh","Incorporating Domain Knowledge for Extractive Summarization of Legal
  Case Documents","  Automatic summarization of legal case documents is an important and practical
challenge. Apart from many domain-independent text summarization algorithms
that can be used for this purpose, several algorithms have been developed
specifically for summarizing legal case documents. However, most of the
existing algorithms do not systematically incorporate domain knowledge that
specifies what information should ideally be present in a legal case document
summary. To address this gap, we propose an unsupervised summarization
algorithm DELSumm which is designed to systematically incorporate guidelines
from legal experts into an optimization setup. We conduct detailed experiments
over case documents from the Indian Supreme Court. The experiments show that
our proposed unsupervised method outperforms several strong baselines in terms
of ROUGE scores, including both general summarization algorithms and
legal-specific ones. In fact, though our proposed algorithm is unsupervised, it
outperforms several supervised summarization models that are trained over
thousands of document-summary pairs.
",5.0
210,8,183371,2201.09882,node embedding for graph,"Zhengrong Xue, Ziao Guo, Yiwei Guo",GlobalWalk: Learning Global-aware Node Embeddings via Biased Sampling,"  Popular node embedding methods such as DeepWalk follow the paradigm of
performing random walks on the graph, and then requiring each node to be
proximate to those appearing along with it. Though proved to be successful in
various tasks, this paradigm reduces a graph with topology to a set of
sequential sentences, thus omitting global information. To produce global-aware
node embeddings, we propose GlobalWalk, a biased random walk strategy that
favors nodes with similar semantics. Empirical evidence suggests GlobalWalk can
generally enhance global awareness of the generated embeddings.
",3.0
211,18,49474,1701.05596,infomation retrieval time complexity,"Dimitrios Markonis, Roger Schaer, Alba Garc\'ia Seco de Herrera,
  Henning M\""uller",The Parallel Distributed Image Search Engine (ParaDISE),"  Image retrieval is a complex task that differs according to the context and
the user requirements in any specific field, for example in a medical
environment. Search by text is often not possible or optimal and retrieval by
the visual content does not always succeed in modelling high-level concepts
that a user is looking for. Modern image retrieval techniques consist of
multiple steps and aim to retrieve information from large--scale datasets and
not only based on global image appearance but local features and if possible in
a connection between visual features and text or semantics. This paper presents
the Parallel Distributed Image Search Engine (ParaDISE), an image retrieval
system that combines visual search with text--based retrieval and that is
available as open source and free of charge. The main design concepts of
ParaDISE are flexibility, expandability, scalability and interoperability.
These concepts constitute the system, able to be used both in real-world
applications and as an image retrieval research platform. Apart from the
architecture and the implementation of the system, two use cases are described,
an application of ParaDISE in retrieval of images from the medical literature
and a visual feature evaluation for medical image retrieval. Future steps
include the creation of an open source community that will contribute and
expand this platform based on the existing parts.
",1.0
212,3,32609,1503.07759,database management system,Edvard Pedersen and Lars Ailo Bongo,Large-scale Biological Meta-database Management,"  Up-to-date meta-databases are vital for the analysis of biological data.
However,the current exponential increase in biological data leads to
exponentially increasing meta-database sizes. Large-scale meta-database
management is therefore an important challenge for production platforms
providing services for biological data analysis. In particular, there is often
a need either to run an analysis with a particular version of a meta-database,
or to rerun an analysis with an updated meta-database. We present our GeStore
approach for biological meta-database management. It provides efficient storage
and runtime generation of specific meta-database versions, and efficient
incremental updates for biological data analysis tools. The approach is
transparent to the tools, and we provide a framework that makes it easy to
integrate GeStore with biological data analysis frameworks. We present the
GeStore system, an evaluation of the performance characteristics of the system,
and an evaluation of the benefits for a biological data analysis workflow.
",5.0
213,1,69781,1806.0813,advanced search engine,"Chao Liu, Zhenzhen Zheng, Jinkang Jia",Behavior-based evaluation of session satisfaction,"  Nowadays, web search becomes more and more popular all over the world. Many
researchers and developers have done lots of studies on behaviors of search
users. In practice, the full understanding of these behaviors can not only help
to evaluate the usefulness of newly-developed ranking algorithms and other
changes of search engine, but also to guide the growth direction of search
engine. As far as we know, most of past work are mainly focused on single
search evaluation, which do promote the rapid development of search engine in
early stage. However,these page-level behaviors are so limited that can no
longer give explicit feedbacks on minor changes of the search engine. We think
that it will be more accurate and sensitive when more information on search
session are provided. In this paper, a session level evaluation method is
proposed. The session-level features are retrieved and carefully analyzed. Some
linear and non-linear features which can reflect the final degree of
satisfaction are chosen and adopted in evaluation models. A two-layer hybrid
evaluation model with different granularity, which can achieve good precision
and recall, is designed and trained. Lots of real experiments are evaluated by
the model, the result shows it achieved a higher accuracy performance than
traditional page-level evaluation metrics. Furthermore, for practical
application, it is important to interpret the reason of each session's
satisfaction judgement. In all, a session-level evaluation model with improved
performance and well capability on interpretation is proposed and applied in
real practice in search engine companies.
",3.0
214,15,76292,1810.12936,relevance feedback for imformation retrieval,"Canjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, Andrew Yates, Le
  Sun, Jungang Xu","NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc
  Information Retrieval","  Pseudo-relevance feedback (PRF) is commonly used to boost the performance of
traditional information retrieval (IR) models by using top-ranked documents to
identify and weight new query terms, thereby reducing the effect of
query-document vocabulary mismatches. While neural retrieval models have
recently demonstrated strong results for ad-hoc retrieval, combining them with
PRF is not straightforward due to incompatibilities between existing PRF
approaches and neural architectures. To bridge this gap, we propose an
end-to-end neural PRF framework that can be used with existing neural IR models
by embedding different neural models as building blocks. Extensive experiments
on two standard test collections confirm the effectiveness of the proposed NPRF
framework in improving the performance of two state-of-the-art neural IR
models.
",4.0
215,4,191579,2204.01691,pre-trained language model,"Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
  Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
  Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter,
  Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth,
  Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei
  Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell
  Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet,
  Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia,
  Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, Andy Zeng","Do As I Can, Not As I Say: Grounding Language in Robotic Affordances","  Large language models can encode a wealth of semantic knowledge about the
world. Such knowledge could be extremely useful to robots aiming to act upon
high-level, temporally extended instructions expressed in natural language.
However, a significant weakness of language models is that they lack real-world
experience, which makes it difficult to leverage them for decision making
within a given embodiment. For example, asking a language model to describe how
to clean a spill might result in a reasonable narrative, but it may not be
applicable to a particular agent, such as a robot, that needs to perform this
task in a particular environment. We propose to provide real-world grounding by
means of pretrained skills, which are used to constrain the model to propose
natural language actions that are both feasible and contextually appropriate.
The robot can act as the language model's ""hands and eyes,"" while the language
model supplies high-level semantic knowledge about the task. We show how
low-level skills can be combined with large language models so that the
language model provides high-level knowledge about the procedures for
performing complex and temporally-extended instructions, while value functions
associated with these skills provide the grounding necessary to connect this
knowledge to a particular physical environment. We evaluate our method on a
number of real-world robotic tasks, where we show the need for real-world
grounding and that this approach is capable of completing long-horizon,
abstract, natural language instructions on a mobile manipulator. The project's
website and the video can be found at https://say-can.github.io/.
",3.0
216,0,42641,1605.06743,learning to rank with partitioned preference,Nadav Cohen and Amnon Shashua,Inductive Bias of Deep Convolutional Networks through Pooling Geometry,"  Our formal understanding of the inductive bias that drives the success of
convolutional networks on computer vision tasks is limited. In particular, it
is unclear what makes hypotheses spaces born from convolution and pooling
operations so suitable for natural images. In this paper we study the ability
of convolutional networks to model correlations among regions of their input.
We theoretically analyze convolutional arithmetic circuits, and empirically
validate our findings on other types of convolutional networks as well.
Correlations are formalized through the notion of separation rank, which for a
given partition of the input, measures how far a function is from being
separable. We show that a polynomially sized deep network supports
exponentially high separation ranks for certain input partitions, while being
limited to polynomial separation ranks for others. The network's pooling
geometry effectively determines which input partitions are favored, thus serves
as a means for controlling the inductive bias. Contiguous pooling windows as
commonly employed in practice favor interleaved partitions over coarse ones,
orienting the inductive bias towards the statistics of natural images. Other
pooling schemes lead to different preferences, and this allows tailoring the
network to data that departs from the usual domain of natural imagery. In
addition to analyzing deep networks, we show that shallow ones support only
linear separation ranks, and by this gain insight into the benefit of functions
brought forth by depth - they are able to efficiently model strong correlation
under favored partitions of the input.
",1.0
217,6,34562,1506.05672,query expansion for imformation retrieval,"Nadine Dulisch, Andreas Oskar Kempf, Philipp Schaer",Query Expansion for Survey Question Retrieval in the Social Sciences,"  In recent years, the importance of research data and the need to archive and
to share it in the scientific community have increased enormously. This
introduces a whole new set of challenges for digital libraries. In the social
sciences typical research data sets consist of surveys and questionnaires. In
this paper we focus on the use case of social science survey question reuse and
on mechanisms to support users in the query formulation for data sets. We
describe and evaluate thesaurus- and co-occurrence-based approaches for query
expansion to improve retrieval quality in digital libraries and research data
archives. The challenge here is to translate the information need and the
underlying sociological phenomena into proper queries. As we can show retrieval
quality can be improved by adding related terms to the queries. In a direct
comparison automatically expanded queries using extracted co-occurring terms
can provide better results than queries manually reformulated by a domain
expert and better results than a keyword-based BM25 baseline.
",4.0
218,9,208341,2209.01335,language model for long documents,Dawn Lawrie and Eugene Yang and Douglas W. Oard and James Mayfield,Multilingual ColBERT-X,"  ColBERT-X is a dense retrieval model for Cross Language Information Retrieval
(CLIR). In CLIR, documents are written in one natural language, while the
queries are expressed in another. A related task is multilingual IR (MLIR)
where the system creates a single ranked list of documents written in many
languages. Given that ColBERT-X relies on a pretrained multilingual neural
language model to rank documents, a multilingual training procedure can enable
a version of ColBERT-X well-suited for MLIR. This paper describes that training
procedure. An important factor for good MLIR ranking is fine-tuning XLM-R using
mixed-language batches, where the same query is matched with documents in
different languages in the same batch. Neural machine translations of MS MARCO
passages are used to fine-tune the model.
",1.0
219,14,15216,1209.3126,text summarization model,Juan-Manuel Torres-Moreno,"Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic
  Text Summarization","  In Automatic Text Summarization, preprocessing is an important phase to
reduce the space of textual representation. Classically, stemming and
lemmatization have been widely used for normalizing words. However, even using
normalization on large texts, the curse of dimensionality can disturb the
performance of summarizers. This paper describes a new method for normalization
of words to further reduce the space of representation. We propose to reduce
each word to its initial letters, as a form of Ultra-stemming. The results show
that Ultra-stemming not only preserve the content of summaries produced by this
representation, but often the performances of the systems can be dramatically
improved. Summaries on trilingual corpora were evaluated automatically with
Fresa. Results confirm an increase in the performance, regardless of summarizer
system used.
",5.0
220,3,4289,912.2282,database management system,"Mrs. Neelu Nihalani, Dr. Sanjay Silakari and Dr. Mahesh Motwani",Design of Intelligent layer for flexible querying in databases,"  Computer-based information technologies have been extensively used to help
many organizations, private companies, and academic and education institutions
manage their processes and information systems hereby become their nervous
centre. The explosion of massive data sets created by businesses, science and
governments necessitates intelligent and more powerful computing paradigms so
that users can benefit from this data. Therefore most new-generation database
applications demand intelligent information management to enhance efficient
interactions between database and the users. Database systems support only a
Boolean query model. A selection query on SQL database returns all those tuples
that satisfy the conditions in the query.
",5.0
221,11,28099,1408.0719,PageRank for web search,"Konstantin Avrachenkov (INRIA Sophia Antipolis), Remco W. Van Der
  Hofstad, Marina Sokol (INRIA Sophia Antipolis)",Personalized PageRank with Node-dependent Restart,"  Personalized PageRank is an algorithm to classify the improtance of web pages
on a user-dependent basis. We introduce two generalizations of Personalized
PageRank with node-dependent restart. The first generalization is based on the
proportion of visits to nodes before the restart, whereas the second
generalization is based on the probability of visited node just before the
restart. In the original case of constant restart probability, the two measures
coincide. We discuss interesting particular cases of restart probabilities and
restart distributions. We show that the both generalizations of Personalized
PageRank have an elegant expression connecting the so-called direct and reverse
Personalized PageRanks that yield a symmetry property of these Personalized
PageRanks.
",5.0
222,3,70222,1807.00212,database management system,"Serhiy O. Semerikov, Vladyslav S. Pototskyi, Kateryna I. Slovak,
  Svitlana M. Hryshchenko, Arnold E. Kiv","Automation of the Export Data from Open Journal Systems to the Russian
  Science Citation Index","  It is shown that the calculation of scientometric indicators of the scientist
and also the scientific journal continues to be an actual problem nowadays. It
is revealed that the leading scientometric databases have the capabilities of
automated metadata collection from the scientific journal website by the use of
specialized electronic document management systems, in particular Open Journal
Systems. It is established that Open Journal Systems successfully exports
metadata about an article from scientific journals to scientometric databases
Scopus, Web of Science and Google Scholar. However, there is no standard method
of export from Open Journal Systems to such scientometric databases as the
Russian Science Citation Index and Index Copernicus, which determined the need
for research. The aim of the study is to develop the plug-in to the Open
Journal Systems for the export of data from this system to scientometric
database Russian Science Citation Index. As a result of the study, an
infological model for exporting metadata from Open Journal Systems to the
Russian Science Citation Index was proposed. The SirenExpo plug-in was
developed to export data from Open Journal Systems to the Russian Science
Citation Index by the use of the Articulus release preparation system.
",3.0
223,3,85894,1904.04702,database management system,"Jim Webber, Paul Ezhilchelvan and Isi Mitrani",Modeling Corruption in Eventually-Consistent Graph Databases,"  We present a model and analysis of an eventually consistent graph database
where loosely cooperating servers accept concurrent updates to a partitioned,
distributed graph. The model is high-fidelity and preserves design choices from
contemporary graph database management systems. To explore the problem space,
we use two common graph topologies as data models for realistic
experimentation. The analysis reveals, even assuming completely fault-free
hardware and bug-free software, that if it is possible for updates to interfere
with one-another, corruption will occur and spread significantly through the
graph within the production database lifetime. Using our model, database
designers and operators can compute the rate of corruption for their systems
and determine whether they are sufficiently dependable for their intended use.
",4.0
224,18,129064,2008.09032,infomation retrieval time complexity,Yu Xia and Zhiqiang Xu,Sparse phase retrieval via Phaseliftoff,"  The aim of sparse phase retrieval is to recover a $k$-sparse signal
$\mathbf{x}_0\in \mathbb{C}^{d}$ from quadratic measurements $|\langle
\mathbf{a}_i,\mathbf{x}_0\rangle|^2$ where $\mathbf{a}_i\in \mathbb{C}^d,
i=1,\ldots,m$. Noting $|\langle
\mathbf{a}_i,\mathbf{x}_0\rangle|^2={\text{Tr}}(A_iX_0)$ with
$A_i=\mathbf{a}_i\mathbf{a}_i^*\in \mathbb{C}^{d\times d},
X_0=\mathbf{x}_0\mathbf{x}_0^*\in \mathbb{C}^{d\times d}$, one can recast
sparse phase retrieval as a problem of recovering a rank-one sparse matrix from
linear measurements. Yin and Xin introduced PhaseLiftOff which presents a proxy
of rank-one condition via the difference of trace and Frobenius norm. By adding
sparsity penalty to PhaseLiftOff, in this paper, we present a novel model to
recover sparse signals from quadratic measurements. Theoretical analysis shows
that the solution to our model provides the stable recovery of $\mathbf{x}_0$
under almost optimal sampling complexity $m=O(k\log(d/k))$. The computation of
our model is carried out by the difference of convex function algorithm (DCA).
Numerical experiments demonstrate that our algorithm outperforms other
state-of-the-art algorithms used for solving sparse phase retrieval.
",0.0
225,18,217633,cs/0508017,infomation retrieval time complexity,"Jovan Pehcevski (RMIT), James A. Thom (RMIT), Anne-Marie Vercoustre","Enhancing Content-And-Structure Information Retrieval using a Native XML
  Database","  Three approaches to content-and-structure XML retrieval are analysed in this
paper: first by using Zettair, a full-text information retrieval system; second
by using eXist, a native XML database, and third by using a hybrid XML
retrieval system that uses eXist to produce the final answers from likely
relevant articles retrieved by Zettair. INEX 2003 content-and-structure topics
can be classified in two categories: the first retrieving full articles as
final answers, and the second retrieving more specific elements within articles
as final answers. We show that for both topic categories our initial hybrid
system improves the retrieval effectiveness of a native XML database. For
ranking the final answer elements, we propose and evaluate a novel retrieval
model that utilises the structural relationships between the answer elements of
a native XML database and retrieves Coherent Retrieval Elements. The final
results of our experiments show that when the XML retrieval task focusses on
highly relevant elements our hybrid XML retrieval system with the Coherent
Retrieval Elements module is 1.8 times more effective than Zettair and 3 times
more effective than eXist, and yields an effective content-and-structure XML
retrieval.
",2.0
226,14,91709,1906.07901,text summarization model,"Shruti Palaskar, Jindrich Libovick\'y, Spandana Gella and Florian
  Metze",Multimodal Abstractive Summarization for How2 Videos,"  In this paper, we study abstractive summarization for open-domain videos.
Unlike the traditional text news summarization, the goal is less to ""compress""
text information but rather to provide a fluent textual summary of information
that has been collected and fused from different source modalities, in our case
video and audio transcripts (or text). We show how a multi-source
sequence-to-sequence model with hierarchical attention can integrate
information from different modalities into a coherent output, compare various
models trained with different modalities and present pilot experiments on the
How2 corpus of instructional videos. We also propose a new evaluation metric
(Content F1) for abstractive summarization task that measures semantic adequacy
rather than fluency of the summaries, which is covered by metrics like ROUGE
and BLEU.
",5.0
227,11,3610,908.328,PageRank for web search,Andri Mirzal,"On the Relationship between Trading Network and WWW Network: A
  Preferential Attachment Perspective","  This paper describes the relationship between trading network and WWW network
from preferential attachment mechanism perspective. This mechanism is known to
be the underlying principle in the network evolution and has been incorporated
to formulate two famous web pages ranking algorithms, PageRank and HITS. We
point out the differences between trading network and WWW network in this
mechanism, derive the formulation of HITS-based ranking algorithm for trading
network as a direct consequence of the differences, and apply the same
framework when deriving the formulation back to the HITS formulation that turns
to become a technique to accelerate its convergences.
",0.0
228,4,205969,2208.04417,pre-trained language model,"Babak Hemmatian, Lav R. Varshney","Debiased Large Language Models Still Associate Muslims with Uniquely
  Violent Acts","  Recent work demonstrates a bias in the GPT-3 model towards generating violent
text completions when prompted about Muslims, compared with Christians and
Hindus. Two pre-registered replication attempts, one exact and one approximate,
found only the weakest bias in the more recent Instruct Series version of
GPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent
completions were observed. Additional pre-registered experiments, however,
showed that using common names associated with the religions in prompts yields
a highly significant increase in violent completions, also revealing a stronger
second-order bias against Muslims. Names of Muslim celebrities from non-violent
domains resulted in relatively fewer violent completions, suggesting that
access to individualized information can steer the model away from using
stereotypes. Nonetheless, content analysis revealed religion-specific violent
themes containing highly offensive ideas regardless of prompt format. Our
results show the need for additional debiasing of large language models to
address higher-order schemas and associations.
",3.0
229,7,60069,1711.06793,gradient boosting,"Jos\'e Marcio Luna, Eric Eaton, Lyle H. Ungar, Eric Diffenderfer,
  Shane T. Jensen, Efstathios D. Gennatas, Mateo Wirth, Charles B. Simone II,
  Timothy D. Solberg, Gilmer Valdes","Tree-Structured Boosting: Connections Between Gradient Boosted Stumps
  and Full Decision Trees","  Additive models, such as produced by gradient boosting, and full interaction
models, such as classification and regression trees (CART), are widely used
algorithms that have been investigated largely in isolation. We show that these
models exist along a spectrum, revealing never-before-known connections between
these two approaches. This paper introduces a novel technique called
tree-structured boosting for creating a single decision tree, and shows that
this method can produce models equivalent to CART or gradient boosted stumps at
the extremes by varying a single parameter. Although tree-structured boosting
is designed primarily to provide both the model interpretability and predictive
performance needed for high-stake applications like medicine, it also can
produce decision trees represented by hybrid models between CART and boosted
stumps that can outperform either of these approaches.
",2.0
230,5,140886,2012.06063,matrix completion,"Saeid Mehrdad, Mohammad Hossein Kahaei",Deep Learning Approach for Matrix Completion Using Manifold Learning,"  Matrix completion has received vast amount of attention and research due to
its wide applications in various study fields. Existing methods of matrix
completion consider only nonlinear (or linear) relations among entries in a
data matrix and ignore linear (or nonlinear) relationships latent. This paper
introduces a new latent variables model for data matrix which is a combination
of linear and nonlinear models and designs a novel deep-neural-network-based
matrix completion algorithm to address both linear and nonlinear relations
among entries of data matrix. The proposed method consists of two branches. The
first branch learns the latent representations of columns and reconstructs the
columns of the partially observed matrix through a series of hidden neural
network layers. The second branch does the same for the rows. In addition,
based on multi-task learning principles, we enforce these two branches work
together and introduce a new regularization technique to reduce over-fitting.
More specifically, the missing entries of data are recovered as a main task and
manifold learning is performed as an auxiliary task. The auxiliary task
constrains the weights of the network so it can be considered as a regularizer,
improving the main task and reducing over-fitting. Experimental results
obtained on the synthetic data and several real-world data verify the
effectiveness of the proposed method compared with state-of-the-art matrix
completion methods.
",5.0
231,15,217625,cs/0507070,relevance feedback for imformation retrieval,"Jovan Pehcevski (RMIT), James A. Thom (RMIT), Anne-Marie Vercoustre","Hybrid XML Retrieval: Combining Information Retrieval and a Native XML
  Database","  This paper investigates the impact of three approaches to XML retrieval:
using Zettair, a full-text information retrieval system; using eXist, a native
XML database; and using a hybrid system that takes full article answers from
Zettair and uses eXist to extract elements from those articles. For the
content-only topics, we undertake a preliminary analysis of the INEX 2003
relevance assessments in order to identify the types of highly relevant
document components. Further analysis identifies two complementary sub-cases of
relevance assessments (""General"" and ""Specific"") and two categories of topics
(""Broad"" and ""Narrow""). We develop a novel retrieval module that for a
content-only topic utilises the information from the resulting answer list of a
native XML database and dynamically determines the preferable units of
retrieval, which we call ""Coherent Retrieval Elements"". The results of our
experiments show that -- when each of the three systems is evaluated against
different retrieval scenarios (such as different cases of relevance
assessments, different topic categories and different choices of evaluation
metrics) -- the XML retrieval systems exhibit varying behaviour and the best
performance can be reached for different values of the retrieval parameters. In
the case of INEX 2003 relevance assessments for the content-only topics, our
newly developed hybrid XML retrieval system is substantially more effective
than either Zettair or eXist, and yields a robust and a very effective XML
retrieval.
",1.0
232,9,218373,cs/0609061,language model for long documents,Ralf Steinberger (European Commission - Joint Research Centre),Cross-lingual keyword assignment,"  This paper presents a language-independent approach to controlled vocabulary
keyword assignment using the EUROVOC thesaurus. Due to the multilingual nature
of EUROVOC, the keywords for a document written in one language can be
displayed in all eleven official European Union languages. The mapping of
documents written in different languages to the same multilingual thesaurus
furthermore allows cross-language document comparison. The assignment of the
controlled vocabulary thesaurus descriptors is achieved by applying a
statistical method that uses a collection of manually indexed documents to
identify, for each thesaurus descriptor, a large number of lemmas that are
statistically associated to the descriptor. These associated words are then
used during the assignment procedure to identify a ranked list of those EUROVOC
terms that are most likely to be good keywords for a given document. The paper
also describes the challenges of this task and discusses the achieved results
of the fully functional prototype.
",1.0
233,7,102376,1911.01914,gradient boosting,"Candice Bent\'ejac and Anna Cs\""org\H{o} and Gonzalo
  Mart\'inez-Mu\~noz",A Comparative Analysis of XGBoost,"  XGBoost is a scalable ensemble technique based on gradient boosting that has
demonstrated to be a reliable and efficient machine learning challenge solver.
This work proposes a practical analysis of how this novel technique works in
terms of training speed, generalization performance and parameter setup. In
addition, a comprehensive comparison between XGBoost, random forests and
gradient boosting has been performed using carefully tuned models as well as
using the default settings. The results of this comparison may indicate that
XGBoost is not necessarily the best choice under all circumstances. Finally an
extensive analysis of XGBoost parametrization tuning process is carried out.
",4.0
234,9,122751,2006.10632,language model for long documents,"Yatin Chaudhary, Hinrich Sch\""utze, Pankaj Gupta",Explainable and Discourse Topic-aware Neural Language Understanding,"  Marrying topic models and language models exposes language understanding to a
broader source of document-level context beyond sentences via topics. While
introducing topical semantics in language models, existing approaches
incorporate latent document topic proportions and ignore topical discourse in
sentences of the document. This work extends the line of research by
additionally introducing an explainable topic representation in language
understanding, obtained from a set of key terms correspondingly for each latent
topic of the proportion. Moreover, we retain sentence-topic associations along
with document-topic association by modeling topical discourse for every
sentence in the document. We present a novel neural composite language model
that exploits both the latent and explainable topics along with topical
discourse at sentence-level in a joint learning framework of topic and language
models. Experiments over a range of tasks such as language modeling, word sense
disambiguation, document classification, retrieval and text generation
demonstrate ability of the proposed model in improving language understanding.
",2.0
235,5,16184,1211.4116,matrix completion,"Franz J. Kir\'aly, Louis Theran, Ryota Tomioka",The Algebraic Combinatorial Approach for Low-Rank Matrix Completion,"  We present a novel algebraic combinatorial view on low-rank matrix completion
based on studying relations between a few entries with tools from algebraic
geometry and matroid theory. The intrinsic locality of the approach allows for
the treatment of single entries in a closed theoretical and practical
framework. More specifically, apart from introducing an algebraic combinatorial
theory of low-rank matrix completion, we present probability-one algorithms to
decide whether a particular entry of the matrix can be completed. We also
describe methods to complete that entry from a few others, and to estimate the
error which is incurred by any method completing that entry. Furthermore, we
show how known results on matrix completion and their sampling assumptions can
be related to our new perspective and interpreted in terms of a completability
phase transition.
",5.0
236,1,43559,1606.06081,advanced search engine,"Philipp Schaer and Philipp Mayr and Sebastian S\""unkler and Dirk
  Lewandowski","How Relevant is the Long Tail? A Relevance Assessment Study on Million
  Short","  Users of web search engines are known to mostly focus on the top ranked
results of the search engine result page. While many studies support this well
known information seeking pattern only few studies concentrate on the question
what users are missing by neglecting lower ranked results. To learn more about
the relevance distributions in the so-called long tail we conducted a relevance
assessment study with the Million Short long-tail web search engine. While we
see a clear difference in the content between the head and the tail of the
search engine result list we see no statistical significant differences in the
binary relevance judgments and weak significant differences when using graded
relevance. The tail contains different but still valuable results. We argue
that the long tail can be a rich source for the diversification of web search
engine result lists but it needs more evaluation to clearly describe the
differences.
",5.0
237,6,112481,2003.02546,query expansion for imformation retrieval,"Byungsoo Ko, Geonmo Gu","Embedding Expansion: Augmentation in Embedding Space for Deep Metric
  Learning","  Learning the distance metric between pairs of samples has been studied for
image retrieval and clustering. With the remarkable success of pair-based
metric learning losses, recent works have proposed the use of generated
synthetic points on metric learning losses for augmentation and generalization.
However, these methods require additional generative networks along with the
main network, which can lead to a larger model size, slower training speed, and
harder optimization. Meanwhile, post-processing techniques, such as query
expansion and database augmentation, have proposed the combination of feature
points to obtain additional semantic information. In this paper, inspired by
query expansion and database augmentation, we propose an augmentation method in
an embedding space for pair-based metric learning losses, called embedding
expansion. The proposed method generates synthetic points containing augmented
information by a combination of feature points and performs hard negative pair
mining to learn with the most informative feature representations. Because of
its simplicity and flexibility, it can be used for existing metric learning
losses without affecting model size, training speed, or optimization
difficulty. Finally, the combination of embedding expansion and representative
metric learning losses outperforms the state-of-the-art losses and previous
sample generation methods in both image retrieval and clustering tasks. The
implementation is publicly available.
",2.0
238,7,201280,2206.12082,gradient boosting,Moshe Sipper and Jason H Moore,Symbolic-Regression Boosting,"  Modifying standard gradient boosting by replacing the embedded weak learner
in favor of a strong(er) one, we present SyRBo: Symbolic-Regression Boosting.
Experiments over 98 regression datasets show that by adding a small number of
boosting stages -- between 2--5 -- to a symbolic regressor, statistically
significant improvements can often be attained. We note that coding SyRBo on
top of any symbolic regressor is straightforward, and the added cost is simply
a few more evolutionary rounds. SyRBo is essentially a simple add-on that can
be readily added to an extant symbolic regressor, often with beneficial
results.
",5.0
239,11,42314,1605.02917,PageRank for web search,"Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki",Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine,"  Search engines are the most important tools for web data acquisition. Web
pages are crawled and indexed by search Engines. Users typically locate useful
web pages by querying a search engine. One of the challenges in search engines
administration is spam pages which waste search engine resources. These pages
by deception of search engine ranking algorithms try to be showed in the first
page of results. There are many approaches to web spam pages detection such as
measurement of HTML code style similarity, pages linguistic pattern analysis
and machine learning algorithm on page content features. One of the famous
algorithms has been used in machine learning approach is Support Vector Machine
(SVM) classifier. Recently basic structure of SVM has been changed by new
extensions to increase robustness and classification accuracy. In this paper we
improved accuracy of web spam detection by using two nonlinear kernels into
Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data
separation has been increased by using two separated kernels for each class of
data. Effectiveness of new proposed method has been experimented with two
publicly used spam datasets called UK-2007 and UK-2006. Results show the
effectiveness of proposed kernelized version of TSVM in web spam page
detection.
",2.0
240,14,116993,2004.13136,text summarization model,"Ahmad Hussein Ababneh, Joan Lu, Qiang Xu","The Effect of the Multi-Layer Text Summarization Model on the Efficiency
  and Relevancy of the Vector Space-based Information Retrieval","  The massive upload of text on the internet creates a huge inverted index in
information retrieval systems, which hurts their efficiency. The purpose of
this research is to measure the effect of the Multi-Layer Similarity model of
the automatic text summarization on building an informative and condensed
invert index in the IR systems. To achieve this purpose, we summarized a
considerable number of documents using the Multi-Layer Similarity model, and we
built the inverted index from the automatic summaries that were generated from
this model. A series of experiments were held to test the performance in terms
of efficiency and relevancy. The experiments include comparisons with three
existing text summarization models; the Jaccard Coefficient Model, the Vector
Space Model, and the Latent Semantic Analysis model. The experiments examined
three groups of queries with manual and automatic relevancy assessment. The
positive effect of the Multi-Layer Similarity in the efficiency of the IR
system was clear without noticeable loss in the relevancy results. However, the
evaluation showed that the traditional statistical models without semantic
investigation failed to improve the information retrieval efficiency. Comparing
with the previous publications that addressed the use of summaries as a source
of the index, the relevancy assessment of our work was higher, and the
Multi-Layer Similarity retrieval constructed an inverted index that was 58%
smaller than the main corpus inverted index.
",3.0
241,18,209135,2209.05673,infomation retrieval time complexity,Meng Huang and Zhiqiang Xu,No existence of linear algorithm for Fourier phase retrieval,"  Fourier phase retrieval, which seeks to reconstruct a signal from its Fourier
magnitude, is of fundamental importance in fields of engineering and science.
In this paper, we give a theoretical understanding of algorithms for Fourier
phase retrieval. Particularly, we show if there exists an algorithm which could
reconstruct an arbitrary signal ${\mathbf x}\in {\mathbb C}^N$ in $
\mbox{Poly}(N) \log(1/\epsilon)$ time to reach $\epsilon$-precision from its
magnitude of discrete Fourier transform and its initial value $x(0)$, then
$\mathcal{ P}=\mathcal{NP}$. This demystifies the phenomenon that, although
almost all signals are determined uniquely by their Fourier magnitude with a
prior conditions, there is no algorithm with theoretical guarantees being
proposed over the past few decades. Our proofs employ the result in
computational complexity theory that Product Partition problem is NP-complete
in the strong sense.
",0.0
242,4,151586,2103.15737,pre-trained language model,Pratik Jayarao and Arpit Sharma,"Retraining DistilBERT for a Voice Shopping Assistant by Using Universal
  Dependencies","  In this work, we retrained the distilled BERT language model for Walmart's
voice shopping assistant on retail domain-specific data. We also injected
universal syntactic dependencies to improve the performance of the model
further. The Natural Language Understanding (NLU) components of the voice
assistants available today are heavily dependent on language models for various
tasks. The generic language models such as BERT and RoBERTa are useful for
domain-independent assistants but have limitations when they cater to a
specific domain. For example, in the shopping domain, the token 'horizon' means
a brand instead of its literal meaning. Generic models are not able to capture
such subtleties. So, in this work, we retrained a distilled version of the BERT
language model on retail domain-specific data for Walmart's voice shopping
assistant. We also included universal dependency-based features in the
retraining process further to improve the performance of the model on
downstream tasks. We evaluated the performance of the retrained language model
on four downstream tasks, including intent-entity detection, sentiment
analysis, voice title shortening and proactive intent suggestion. We observed
an increase in the performance of all the downstream tasks of up to 1.31% on
average.
",5.0
243,5,50284,1702.04077,matrix completion,Tsuyoshi Kato and Rachelle Rivero,Mutual Kernel Matrix Completion,"  With the huge influx of various data nowadays, extracting knowledge from them
has become an interesting but tedious task among data scientists, particularly
when the data come in heterogeneous form and have missing information. Many
data completion techniques had been introduced, especially in the advent of
kernel methods. However, among the many data completion techniques available in
the literature, studies about mutually completing several incomplete kernel
matrices have not been given much attention yet. In this paper, we present a
new method, called Mutual Kernel Matrix Completion (MKMC) algorithm, that
tackles this problem of mutually inferring the missing entries of multiple
kernel matrices by combining the notions of data fusion and kernel matrix
completion, applied on biological data sets to be used for classification task.
We first introduced an objective function that will be minimized by exploiting
the EM algorithm, which in turn results to an estimate of the missing entries
of the kernel matrices involved. The completed kernel matrices are then
combined to produce a model matrix that can be used to further improve the
obtained estimates. An interesting result of our study is that the E-step and
the M-step are given in closed form, which makes our algorithm efficient in
terms of time and memory. After completion, the (completed) kernel matrices are
then used to train an SVM classifier to test how well the relationships among
the entries are preserved. Our empirical results show that the proposed
algorithm bested the traditional completion techniques in preserving the
relationships among the data points, and in accurately recovering the missing
kernel matrix entries. By far, MKMC offers a promising solution to the problem
of mutual estimation of a number of relevant incomplete kernel matrices.
",5.0
244,13,26542,1405.2584,social network analysis with natrual language processing,Rahul Tejwani (University at Buffalo),Sentiment Analysis: A Survey,"  Sentiment analysis (also known as opinion mining) refers to the use of
natural language processing, text analysis and computational linguistics to
identify and extract subjective information in source materials. Mining
opinions expressed in the user generated content is a challenging yet
practically very useful problem. This survey would cover various approaches and
methodology used in Sentiment Analysis and Opinion Mining in general. The focus
would be on Internet text like, Product review, tweets and other social media.
",5.0
245,19,47159,1611.00685,artificial intelligence for low carbon,"Marek Rosa, Jan Feyereisl and The GoodAI Collective",A Framework for Searching for General Artificial Intelligence,"  There is a significant lack of unified approaches to building generally
intelligent machines. The majority of current artificial intelligence research
operates within a very narrow field of focus, frequently without considering
the importance of the 'big picture'. In this document, we seek to describe and
unify principles that guide the basis of our development of general artificial
intelligence. These principles revolve around the idea that intelligence is a
tool for searching for general solutions to problems. We define intelligence as
the ability to acquire skills that narrow this search, diversify it and help
steer it to more promising areas. We also provide suggestions for studying,
measuring, and testing the various skills and abilities that a human-level
intelligent machine needs to acquire. The document aims to be both
implementation agnostic, and to provide an analytic, systematic, and scalable
way to generate hypotheses that we believe are needed to meet the necessary
conditions in the search for general artificial intelligence. We believe that
such a framework is an important stepping stone for bringing together
definitions, highlighting open problems, connecting researchers willing to
collaborate, and for unifying the arguably most significant search of this
century.
",0.0
246,2,24798,1402.0459,random forests,Hubert Haoyang Duan,"Applying Supervised Learning Algorithms and a New Feature Selection
  Method to Predict Coronary Artery Disease","  From a fresh data science perspective, this thesis discusses the prediction
of coronary artery disease based on genetic variations at the DNA base pair
level, called Single-Nucleotide Polymorphisms (SNPs), collected from the
Ontario Heart Genomics Study (OHGS).
  First, the thesis explains two commonly used supervised learning algorithms,
the k-Nearest Neighbour (k-NN) and Random Forest classifiers, and includes a
complete proof that the k-NN classifier is universally consistent in any finite
dimensional normed vector space. Second, the thesis introduces two
dimensionality reduction steps, Random Projections, a known feature extraction
technique based on the Johnson-Lindenstrauss lemma, and a new method termed
Mass Transportation Distance (MTD) Feature Selection for discrete domains.
Then, this thesis compares the performance of Random Projections with the k-NN
classifier against MTD Feature Selection and Random Forest, for predicting
artery disease based on accuracy, the F-Measure, and area under the Receiver
Operating Characteristic (ROC) curve.
  The comparative results demonstrate that MTD Feature Selection with Random
Forest is vastly superior to Random Projections and k-NN. The Random Forest
classifier is able to obtain an accuracy of 0.6660 and an area under the ROC
curve of 0.8562 on the OHGS genetic dataset, when 3335 SNPs are selected by MTD
Feature Selection for classification. This area is considerably better than the
previous high score of 0.608 obtained by Davies et al. in 2010 on the same
dataset.
",3.0
247,12,19576,1304.3013,COVID-19 and social media,Marianne Marcoux and David Lusseau,"The influence of repressive legislation on the structure of a social
  media network","  Social media have been widely used to organize citizen movements. In 2012,
75% university and college students in Quebec, Canada, participated in mass
protests against an increase in tuition fees, mainly organized using social
media. To reduce public disruption, the government introduced special
legislation designed to impede protest organization. Here, we show that the
legislation changed the behaviour of social media users but not the overall
structure of their social network on Twitter. Thus, users were still able to
spread information to efficiently organize demonstrations using their social
network. This natural experiment shows the power of social media in political
mobilization, as well as behavioural flexibility in information flow over a
large number of individuals.
",1.0
248,1,44773,1608.01247,advanced search engine,S.K Kolluru and Prasenjit Mukherjee,Query Clustering using Segment Specific Context Embeddings,"  This paper presents a novel query clustering approach to capture the broad
interest areas of users querying search engines. We make use of recent advances
in NLP - word2vec and extend it to get query2vec, vector representations of
queries, based on query contexts, obtained from the top search results for the
query and use a highly scalable Divide & Merge clustering algorithm on top of
the query vectors, to get the clusters. We have tried this approach on a
variety of segments, including Retail, Travel, Health, Phones and found the
clusters to be effective in discovering user's interest areas which have high
monetization potential.
",3.0
249,14,20203,1305.2831,text summarization model,Khushboo Thakkar and Urmila Shrawankar,Test Model for Text Categorization and Text Summarization,"  Text Categorization is the task of automatically sorting a set of documents
into categories from a predefined set and Text Summarization is a brief and
accurate representation of input text such that the output covers the most
important concepts of the source in a condensed manner. Document Summarization
is an emerging technique for understanding the main purpose of any kind of
documents. This paper presents a model that uses text categorization and text
summarization for searching a document based on user query.
",3.0
250,11,9735,1108.6016,PageRank for web search,Jim Gemmell and Benjamin I. P. Rubinstein and Ashok K. Chandra,Improving Entity Resolution with Global Constraints,"  Some of the greatest advances in web search have come from leveraging
socio-economic properties of online user behavior. Past advances include
PageRank, anchor text, hubs-authorities, and TF-IDF. In this paper, we
investigate another socio-economic property that, to our knowledge, has not yet
been exploited: sites that create lists of entities, such as IMDB and Netflix,
have an incentive to avoid gratuitous duplicates. We leverage this property to
resolve entities across the different web sites, and find that we can obtain
substantial improvements in resolution accuracy. This improvement in accuracy
also translates into robustness, which often reduces the amount of training
data that must be labeled for comparing entities across many sites.
Furthermore, the technique provides robustness when resolving sites that have
some duplicates, even without first removing these duplicates. We present
algorithms with very strong precision and recall, and show that max weight
matching, while appearing to be a natural choice turns out to have poor
performance in some situations. The presented techniques are now being used in
the back-end entity resolution system at a major Internet search engine.
",2.0
251,9,139954,2012.01747,language model for long documents,"Prithwiraj Bhattacharjee, Avi Mallick, Md Saiful Islam,
  Marium-E-Jannat","Bengali Abstractive News Summarization(BANS): A Neural Attention
  Approach","  Abstractive summarization is the process of generating novel sentences based
on the information extracted from the original text document while retaining
the context. Due to abstractive summarization's underlying complexities, most
of the past research work has been done on the extractive summarization
approach. Nevertheless, with the triumph of the sequence-to-sequence (seq2seq)
model, abstractive summarization becomes more viable. Although a significant
number of notable research has been done in the English language based on
abstractive summarization, only a couple of works have been done on Bengali
abstractive news summarization (BANS). In this article, we presented a seq2seq
based Long Short-Term Memory (LSTM) network model with attention at
encoder-decoder. Our proposed system deploys a local attention-based model that
produces a long sequence of words with lucid and human-like generated sentences
with noteworthy information of the original document. We also prepared a
dataset of more than 19k articles and corresponding human-written summaries
collected from bangla.bdnews24.com1 which is till now the most extensive
dataset for Bengali news document summarization and publicly published in
Kaggle2. We evaluated our model qualitatively and quantitatively and compared
it with other published results. It showed significant improvement in terms of
human evaluation scores with state-of-the-art approaches for BANS.
",2.0
252,17,109408,2002.01571,robustness of neutral networks,"Hyobin Kim, Stalin Mu\~noz, Pamela Osuna, and Carlos Gershenson","Antifragility Predicts the Robustness and Evolvability of Biological
  Networks through Multi-class Classification with a Convolutional Neural
  Network","  Robustness and evolvability are essential properties to the evolution of
biological networks. To determine if a biological network is robust and/or
evolvable, it is required to compare its functions before and after mutations.
However, this sometimes takes a high computational cost as the network size
grows. Here we develop a predictive method to estimate the robustness and
evolvability of biological networks without an explicit comparison of
functions. We measure antifragility in Boolean network models of biological
systems and use this as the predictor. Antifragility occurs when a system
benefits from external perturbations. By means of the differences of
antifragility between the original and mutated biological networks, we train a
convolutional neural network (CNN) and test it to classify the properties of
robustness and evolvability. We found that our CNN model successfully
classified the properties. Thus, we conclude that our antifragility measure can
be used as a predictor of the robustness and evolvability of biological
networks.
",3.0
253,0,33338,1504.07218,learning to rank with partitioned preference,"Yuxin Chen, Changho Suh",Spectral MLE: Top-$K$ Rank Aggregation from Pairwise Comparisons,"  This paper explores the preference-based top-$K$ rank aggregation problem.
Suppose that a collection of items is repeatedly compared in pairs, and one
wishes to recover a consistent ordering that emphasizes the top-$K$ ranked
items, based on partially revealed preferences. We focus on the
Bradley-Terry-Luce (BTL) model that postulates a set of latent preference
scores underlying all items, where the odds of paired comparisons depend only
on the relative scores of the items involved.
  We characterize the minimax limits on identifiability of top-$K$ ranked
items, in the presence of random and non-adaptive sampling. Our results
highlight a separation measure that quantifies the gap of preference scores
between the $K^{\text{th}}$ and $(K+1)^{\text{th}}$ ranked items. The minimum
sample complexity required for reliable top-$K$ ranking scales inversely with
the separation measure irrespective of other preference distribution metrics.
To approach this minimax limit, we propose a nearly linear-time ranking scheme,
called \emph{Spectral MLE}, that returns the indices of the top-$K$ items in
accordance to a careful score estimate. In a nutshell, Spectral MLE starts with
an initial score estimate with minimal squared loss (obtained via a spectral
method), and then successively refines each component with the assistance of
coordinate-wise MLEs. Encouragingly, Spectral MLE allows perfect top-$K$ item
identification under minimal sample complexity. The practical applicability of
Spectral MLE is further corroborated by numerical experiments.
",3.0
254,14,38979,1601.00643,text summarization model,"Chandra Shekhar Yadav, Aditi Sharan","Hybrid Approach for Single Text Document Summarization using Statistical
  and Sentiment Features","  Summarization is a way to represent same information in concise way with
equal sense. This can be categorized in two type Abstractive and Extractive
type. Our work is focused around Extractive summarization. A generic approach
to extractive summarization is to consider sentence as an entity, score each
sentence based on some indicative features to ascertain the quality of sentence
for inclusion in summary. Sort the sentences on the score and consider top n
sentences for summarization. Mostly statistical features have been used for
scoring the sentences. We are proposing a hybrid model for a single text
document summarization. This hybrid model is an extraction based approach,
which is combination of Statistical and semantic technique. The hybrid model
depends on the linear combination of statistical measures : sentence position,
TF-IDF, Aggregate similarity, centroid, and semantic measure. Our idea to
include sentiment analysis for salient sentence extraction is derived from the
concept that emotion plays an important role in communication to effectively
convey any message hence, it can play a vital role in text document
summarization. For comparison we have generated five system summaries Proposed
Work, MEAD system, Microsoft system, OPINOSIS system, and Human generated
summary, and evaluation is done using ROUGE score.
",5.0
255,15,75995,1810.11303,relevance feedback for imformation retrieval,"Dimitris Gkoumas, Sagar Uprety, and Dawei Song","Investigating non-classical correlations between decision fused
  multi-modal documents","  Correlation has been widely used to facilitate various information retrieval
methods such as query expansion, relevance feedback, document clustering, and
multi-modal fusion. Especially, correlation and independence are important
issues when fusing different modalities that influence a multi-modal
information retrieval process. The basic idea of correlation is that an
observable can help predict or enhance another observable. In quantum
mechanics, quantum correlation, called entanglement, is a sort of correlation
between the observables measured in atomic-size particles when these particles
are not necessarily collected in ensembles. In this paper, we examine a
multimodal fusion scenario that might be similar to that encountered in physics
by firstly measuring two observables (i.e., text-based relevance and
image-based relevance) of a multi-modal document without counting on an
ensemble of multi-modal documents already labeled in terms of these two
variables. Then, we investigate the existence of non-classical correlations
between pairs of multi-modal documents. Despite there are some basic
differences between entanglement and classical correlation encountered in the
macroscopic world, we investigate the existence of this kind of non-classical
correlation through the Bell inequality violation. Here, we experimentally test
several novel association methods in a small-scale experiment. However, in the
current experiment we did not find any violation of the Bell inequality.
Finally, we present a series of interesting discussions, which may provide
theoretical and empirical insights and inspirations for future development of
this direction.
",2.0
256,18,72738,1808.09407,infomation retrieval time complexity,"V\'it Novotn\'y (1) ((1) Faculty of Informatics, Masaryk University,
  Brno, Czech Republic)",Implementation Notes for the Soft Cosine Measure,"  The standard bag-of-words vector space model (VSM) is efficient, and
ubiquitous in information retrieval, but it underestimates the similarity of
documents with the same meaning, but different terminology. To overcome this
limitation, Sidorov et al. proposed the Soft Cosine Measure (SCM) that
incorporates term similarity relations. Charlet and Damnati showed that the SCM
is highly effective in question answering (QA) systems. However, the
orthonormalization algorithm proposed by Sidorov et al. has an impractical time
complexity of $\mathcal O(n^4)$, where n is the size of the vocabulary.
  In this paper, we prove a tighter lower worst-case time complexity bound of
$\mathcal O(n^3)$. We also present an algorithm for computing the similarity
between documents and we show that its worst-case time complexity is $\mathcal
O(1)$ given realistic conditions. Lastly, we describe implementation in
general-purpose vector databases such as Annoy, and Faiss and in the inverted
indices of text search engines such as Apache Lucene, and ElasticSearch. Our
results enable the deployment of the SCM in real-world information retrieval
systems.
",1.0
257,11,23299,1311.6227,PageRank for web search,"Debajyoti Mukhopadhyay, Manoj Sharma, Gajanan Joshi, Trupti Pagare,
  Adarsha Palwe",Experience of Developing a Meta-Semantic Search Engine,"  Thinking of todays web search scenario which is mainly keyword based, leads
to the need of effective and meaningful search provided by Semantic Web.
Existing search engines are vulnerable to provide relevant answers to users
query due to their dependency on simple data available in web pages. On other
hand, semantic search engines provide efficient and relevant results as the
semantic web manages information with well defined meaning using ontology. A
Meta-Search engine is a search tool that forwards users query to several
existing search engines and provides combined results by using their own page
ranking algorithm. SemanTelli is a meta semantic search engine that fetches
results from different semantic search engines such as Hakia, DuckDuckGo,
SenseBot through intelligent agents. This paper proposes enhancement of
SemanTelli with improved snippet analysis based page ranking algorithm and
support for image and news search.
",3.0
258,7,108227,2001.07248,gradient boosting,Aleksei Ustimenko and Liudmila Prokhorenkova,SGLB: Stochastic Gradient Langevin Boosting,"  This paper introduces Stochastic Gradient Langevin Boosting (SGLB) - a
powerful and efficient machine learning framework that may deal with a wide
range of loss functions and has provable generalization guarantees. The method
is based on a special form of the Langevin diffusion equation specifically
designed for gradient boosting. This allows us to theoretically guarantee the
global convergence even for multimodal loss functions, while standard gradient
boosting algorithms can guarantee only local optimum. We also empirically show
that SGLB outperforms classic gradient boosting when applied to classification
tasks with 0-1 loss function, which is known to be multimodal.
",4.0
259,0,39301,1601.05495,learning to rank with partitioned preference,Ashish Khetan and Sewoong Oh,Data-driven Rank Breaking for Efficient Rank Aggregation,"  Rank aggregation systems collect ordinal preferences from individuals to
produce a global ranking that represents the social preference. Rank-breaking
is a common practice to reduce the computational complexity of learning the
global ranking. The individual preferences are broken into pairwise comparisons
and applied to efficient algorithms tailored for independent paired
comparisons. However, due to the ignored dependencies in the data, naive
rank-breaking approaches can result in inconsistent estimates. The key idea to
produce accurate and consistent estimates is to treat the pairwise comparisons
unequally, depending on the topology of the collected data. In this paper, we
provide the optimal rank-breaking estimator, which not only achieves
consistency but also achieves the best error bound. This allows us to
characterize the fundamental tradeoff between accuracy and complexity. Further,
the analysis identifies how the accuracy depends on the spectral gap of a
corresponding comparison graph.
",1.0
260,19,158862,2106.03035,artificial intelligence for low carbon,Koya Ishikawa and Kazuhide Nakata,"Online Trading Models with Deep Reinforcement Learning in the Forex
  Market Considering Transaction Costs","  In recent years, a wide range of investment models have been created using
artificial intelligence. Automatic trading by artificial intelligence can
expand the range of trading methods, such as by conferring the ability to
operate 24 hours a day and the ability to trade with high frequency. Automatic
trading can also be expected to trade with more information than is available
to humans if it can sufficiently consider past data. In this paper, we propose
an investment agent based on a deep reinforcement learning model, which is an
artificial intelligence model. The model considers the transaction costs
involved in actual trading and creates a framework for trading over a long
period of time so that it can make a large profit on a single trade. In doing
so, it can maximize the profit while keeping transaction costs low. In
addition, in consideration of actual operations, we use online learning so that
the system can continue to learn by constantly updating the latest online data
instead of learning with static data. This makes it possible to trade in
non-stationary financial markets by always incorporating current market trend
information.
",0.0
261,18,72730,1808.09353,infomation retrieval time complexity,"Morgan Gallant, Haruna Isah, Farhana Zulkernine, Shahzad Khan",Xu: An Automated Query Expansion and Optimization Tool,"  The exponential growth of information on the Internet is a big challenge for
information retrieval systems towards generating relevant results. Novel
approaches are required to reformat or expand user queries to generate a
satisfactory response and increase recall and precision. Query expansion (QE)
is a technique to broaden users' queries by introducing additional tokens or
phrases based on some semantic similarity metrics. The tradeoff is the added
computational complexity to find semantically similar words and a possible
increase in noise in information retrieval. Despite several research efforts on
this topic, QE has not yet been explored enough and more work is needed on
similarity matching and composition of query terms with an objective to
retrieve a small set of most appropriate responses. QE should be scalable,
fast, and robust in handling complex queries with a good response time and
noise ceiling. In this paper, we propose Xu, an automated QE technique, using
high dimensional clustering of word vectors and Datamuse API, an open source
query engine to find semantically similar words. We implemented Xu as a command
line tool and evaluated its performances using datasets containing news
articles and human-generated QEs. The evaluation results show that Xu was
better than Datamuse by achieving about 88% accuracy with reference to the
human-generated QE.
",2.0
262,4,165168,2107.14593,pre-trained language model,"Nisha Pillai, Cynthia Matuszek, Francis Ferraro",Neural Variational Learning for Grounded Language Acquisition,"  We propose a learning system in which language is grounded in visual percepts
without specific pre-defined categories of terms. We present a unified
generative method to acquire a shared semantic/visual embedding that enables
the learning of language about a wide range of real-world objects. We evaluate
the efficacy of this learning by predicting the semantics of objects and
comparing the performance with neural and non-neural inputs. We show that this
generative approach exhibits promising results in language grounding without
pre-specifying visual categories under low resource settings. Our experiments
demonstrate that this approach is generalizable to multilingual, highly varied
datasets.
",5.0
263,2,151772,2103.167,random forests,Siyu Zhou and Lucas Mentch,"Trees, Forests, Chickens, and Eggs: When and Why to Prune Trees in a
  Random Forest","  Due to their long-standing reputation as excellent off-the-shelf predictors,
random forests continue remain a go-to model of choice for applied
statisticians and data scientists. Despite their widespread use, however, until
recently, little was known about their inner-workings and about which aspects
of the procedure were driving their success. Very recently, two competing
hypotheses have emerged -- one based on interpolation and the other based on
regularization. This work argues in favor of the latter by utilizing the
regularization framework to reexamine the decades-old question of whether
individual trees in an ensemble ought to be pruned. Despite the fact that
default constructions of random forests use near full depth trees in most
popular software packages, here we provide strong evidence that tree depth
should be seen as a natural form of regularization across the entire procedure.
In particular, our work suggests that random forests with shallow trees are
advantageous when the signal-to-noise ratio in the data is low. In building up
this argument, we also critique the newly popular notion of ""double descent"" in
random forests by drawing parallels to U-statistics and arguing that the
noticeable jumps in random forest accuracy are the result of simple averaging
rather than interpolation.
",5.0
264,15,198107,2205.1587,relevance feedback for imformation retrieval,"Devansh Gupta, Aditya Saini, Drishti Bhasin, Sarthak Bhagat, Shagun
  Uppal, Rishi Raj Jain, Ponnurangam Kumaraguru, Rajiv Ratn Shah",FaIRCoP: Facial Image Retrieval using Contrastive Personalization,"  Retrieving facial images from attributes plays a vital role in various
systems such as face recognition and suspect identification. Compared to other
image retrieval tasks, facial image retrieval is more challenging due to the
high subjectivity involved in describing a person's facial features. Existing
methods do so by comparing specific characteristics from the user's mental
image against the suggested images via high-level supervision such as using
natural language. In contrast, we propose a method that uses a relatively
simpler form of binary supervision by utilizing the user's feedback to label
images as either similar or dissimilar to the target image. Such supervision
enables us to exploit the contrastive learning paradigm for encapsulating each
user's personalized notion of similarity. For this, we propose a novel loss
function optimized online via user feedback. We validate the efficacy of our
proposed approach using a carefully designed testbed to simulate user feedback
and a large-scale user study. Our experiments demonstrate that our method
iteratively improves personalization, leading to faster convergence and
enhanced recommendation relevance, thereby, improving user satisfaction. Our
proposed framework is also equipped with a user-friendly web interface with a
real-time experience for facial image retrieval.
",3.0
265,19,93368,1907.04659,artificial intelligence for low carbon,Ravi Kashyap,Artificial Intelligence: A Child's Play,"  We discuss the objectives of any endeavor in creating artificial
intelligence, AI, and provide a possible alternative. Intelligence might be an
unintended consequence of curiosity left to roam free, best exemplified by a
frolicking infant. This suggests that our attempts at AI could have been
misguided. What we actually need to strive for can be termed artificial
curiosity, AC, and intelligence happens as a consequence of those efforts. For
this unintentional yet welcome aftereffect to set in a foundational list of
guiding principles needs to be present. We start with the intuition for this
line of reasoning and formalize it with a series of definitions, assumptions,
ingredients, models and iterative improvements that will be necessary to make
the incubation of intelligence a reality. Our discussion provides conceptual
modifications to the Turing Test and to Searle's Chinese room argument. We
discuss the future implications for society as AI becomes an integral part of
life.
  We provide a road-map for creating intelligence with the technical parts
relegated to the appendix so that the article is accessible to a wide audience.
The central techniques in our formal approach to creating intelligence draw
upon tools and concepts widely used in physics, cognitive science, psychology,
evolutionary biology, statistics, linguistics, communication systems, pattern
recognition, marketing, economics, finance, information science and
computational theory highlighting that solutions for creating artificial
intelligence have to transcend the artificial barriers between various fields
and be highly multi-disciplinary.
",1.0
266,0,11798,1202.3734,learning to rank with partitioned preference,"Jonathan Huang, Ashish Kapoor, Carlos E. Guestrin",Efficient Probabilistic Inference with Partial Ranking Queries,"  Distributions over rankings are used to model data in various settings such
as preference analysis and political elections. The factorial size of the space
of rankings, however, typically forces one to make structural assumptions, such
as smoothness, sparsity, or probabilistic independence about these underlying
distributions. We approach the modeling problem from the computational
principle that one should make structural assumptions which allow for efficient
calculation of typical probabilistic queries. For ranking models, ""typical""
queries predominantly take the form of partial ranking queries (e.g., given a
user's top-k favorite movies, what are his preferences over remaining movies?).
In this paper, we argue that riffled independence factorizations proposed in
recent literature [7, 8] are a natural structural assumption for ranking
distributions, allowing for particularly efficient processing of partial
ranking queries.
",2.0
267,19,212874,2210.06285,artificial intelligence for low carbon,"Mengxi Liu and Sizhen Bian and Bo Zhou and Agnes Gr\""unerbl and Paul
  Lukowicz","Smart Cup: An impedance sensing based fluid intake monitoring system for
  beverages classification and freshness detection","  This paper presents a novel beverage intake monitoring system that can
accurately recognize beverage kinds and freshness. By mounting carbon
electrodes on the commercial cup, the system measures the electrochemical
impedance spectrum of the fluid in the cup. We studied the frequency
sensitivity of the electrochemical impedance spectrum regarding distinct
beverages and the importance of features like amplitude, phase, and real and
imaginary components for beverage classification. The results show that
features from a low-frequency domain (100 Hz to 1000 Hz) provide more
meaningful information for beverage classification than the higher frequency
domain. Twenty beverages, including carbonated drinks and juices, were
classified with nearly perfect accuracy using a supervised machine learning
approach. The same performance was also observed in the freshness recognition,
where four different kinds of milk and fruit juice were studied.
",1.0
268,8,160488,2106.08848,node embedding for graph,"Zhengzheng Tang, Ziyue Qiao, Xuehai Hong, Yang Wang, Fayaz Ali
  Dharejo, Yuanchun Zhou, Yi Du","Data Augmentation for Graph Convolutional Network on Semi-Supervised
  Classification","  Data augmentation aims to generate new and synthetic features from the
original data, which can identify a better representation of data and improve
the performance and generalizability of downstream tasks. However, data
augmentation for graph-based models remains a challenging problem, as graph
data is more complex than traditional data, which consists of two features with
different properties: graph topology and node attributes. In this paper, we
study the problem of graph data augmentation for Graph Convolutional Network
(GCN) in the context of improving the node embeddings for semi-supervised node
classification. Specifically, we conduct cosine similarity based cross
operation on the original features to create new graph features, including new
node attributes and new graph topologies, and we combine them as new pairwise
inputs for specific GCNs. Then, we propose an attentional integrating model to
weighted sum the hidden node embeddings encoded by these GCNs into the final
node embeddings. We also conduct a disparity constraint on these hidden node
embeddings when training to ensure that non-redundant information is captured
from different features. Experimental results on five real-world datasets show
that our method improves the classification accuracy with a clear margin (+2.5%
- +84.2%) than the original GCN model.
",4.0
269,5,7580,1102.2254,matrix completion,Yudong Chen and Huan Xu and Constantine Caramanis and Sujay Sanghavi,"Matrix completion with column manipulation: Near-optimal
  sample-robustness-rank tradeoffs","  This paper considers the problem of matrix completion when some number of the
columns are completely and arbitrarily corrupted, potentially by a malicious
adversary. It is well-known that standard algorithms for matrix completion can
return arbitrarily poor results, if even a single column is corrupted. One
direct application comes from robust collaborative filtering. Here, some number
of users are so-called manipulators who try to skew the predictions of the
algorithm by calibrating their inputs to the system. In this paper, we develop
an efficient algorithm for this problem based on a combination of a trimming
procedure and a convex program that minimizes the nuclear norm and the
$\ell_{1,2}$ norm. Our theoretical results show that given a vanishing fraction
of observed entries, it is nevertheless possible to complete the underlying
matrix even when the number of corrupted columns grows. Significantly, our
results hold without any assumptions on the locations or values of the observed
entries of the manipulated columns. Moreover, we show by an
information-theoretic argument that our guarantees are nearly optimal in terms
of the fraction of sampled entries on the authentic columns, the fraction of
corrupted columns, and the rank of the underlying matrix. Our results therefore
sharply characterize the tradeoffs between sample, robustness and rank in
matrix completion.
",4.0
270,14,115215,2004.03661,text summarization model,Jia-Hong Huang and Marcel Worring,Query-controllable Video Summarization,"  When video collections become huge, how to explore both within and across
videos efficiently is challenging. Video summarization is one of the ways to
tackle this issue. Traditional summarization approaches limit the effectiveness
of video exploration because they only generate one fixed video summary for a
given input video independent of the information need of the user. In this
work, we introduce a method which takes a text-based query as input and
generates a video summary corresponding to it. We do so by modeling video
summarization as a supervised learning problem and propose an end-to-end deep
learning based method for query-controllable video summarization to generate a
query-dependent video summary. Our proposed method consists of a video summary
controller, video summary generator, and video summary output module. To foster
the research of query-controllable video summarization and conduct our
experiments, we introduce a dataset that contains frame-based relevance score
labels. Based on our experimental result, it shows that the text-based query
helps control the video summary. It also shows the text-based query improves
our model performance. Our code and dataset:
https://github.com/Jhhuangkay/Query-controllable-Video-Summarization.
",3.0
271,16,65239,1803.08375,activation function in neutral networks,Abien Fred Agarap,Deep Learning using Rectified Linear Units (ReLU),"  We introduce the use of rectified linear units (ReLU) as the classification
function in a deep neural network (DNN). Conventionally, ReLU is used as an
activation function in DNNs, with Softmax function as their classification
function. However, there have been several studies on using a classification
function other than Softmax, and this study is an addition to those. We
accomplish this by taking the activation of the penultimate layer $h_{n - 1}$
in a neural network, then multiply it by weight parameters $\theta$ to get the
raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$,
i.e. $f(o) = \max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide
class predictions $\hat{y}$ through argmax function, i.e. argmax $f(x)$.
",2.0
272,18,179303,2112.0481,infomation retrieval time complexity,"Chi Thang Duong, Dimitri Percia David, Ljiljana Dolamic, Alain
  Mermoud, Vincent Lenders, Karl Aberer","From Scattered Sources to Comprehensive Technology Landscape: A
  Recommendation-based Retrieval Approach","  Mapping the technology landscape is crucial for market actors to take
informed investment decisions. However, given the large amount of data on the
Web and its subsequent information overload, manually retrieving information is
a seemingly ineffective and incomplete approach. In this work, we propose an
end-to-end recommendation based retrieval approach to support automatic
retrieval of technologies and their associated companies from raw Web data.
This is a two-task setup involving (i) technology classification of entities
extracted from company corpus, and (ii) technology and company retrieval based
on classified technologies. Our proposed framework approaches the first task by
leveraging DistilBERT which is a state-of-the-art language model. For the
retrieval task, we introduce a recommendation-based retrieval technique to
simultaneously support retrieving related companies, technologies related to a
specific company and companies relevant to a technology. To evaluate these
tasks, we also construct a data set that includes company documents and
entities extracted from these documents together with company categories and
technology labels. Experiments show that our approach is able to return 4 times
more relevant companies while outperforming traditional retrieval baseline in
retrieving technologies.
",2.0
273,4,46280,1610.00735,pre-trained language model,Yanshan Wang and Hongfang Liu,MatLM: a Matrix Formulation for Probabilistic Language Models,"  Probabilistic language models are widely used in Information Retrieval (IR)
to rank documents by the probability that they generate the query. However, the
implementation of the probabilistic representations with programming languages
that favor matrix calculations is challenging. In this paper, we utilize matrix
representations to reformulate the probabilistic language models. The matrix
representation is a superstructure for the probabilistic language models to
organize the calculated probabilities and a potential formalism for
standardization of language models and for further mathematical analysis. It
facilitates implementations by matrix friendly programming languages. In this
paper, we consider the matrix formulation of conventional language model with
Dirichlet smoothing, and two language models based on Latent Dirichlet
Allocation (LDA), i.e., LBDM and LDI. We release a Java software
package--MatLM--implementing the proposed models. Code is available at:
https://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM.
",1.0
274,8,153334,2104.07295,node embedding for graph,"Shuiqiao Yang, Sunny Verma, Borui Cai, Jiaojiao Jiang, Kun Yu, Fang
  Chen, Shui Yu",Variational Co-embedding Learning for Attributed Network Clustering,"  Recent works for attributed network clustering utilize graph convolution to
obtain node embeddings and simultaneously perform clustering assignments on the
embedding space. It is effective since graph convolution combines the
structural and attributive information for node embedding learning. However, a
major limitation of such works is that the graph convolution only incorporates
the attribute information from the local neighborhood of nodes but fails to
exploit the mutual affinities between nodes and attributes. In this regard, we
propose a variational co-embedding learning model for attributed network
clustering (VCLANC). VCLANC is composed of dual variational auto-encoders to
simultaneously embed nodes and attributes. Relying on this, the mutual affinity
information between nodes and attributes could be reconstructed from the
embedding space and served as extra self-supervised knowledge for
representation learning. At the same time, trainable Gaussian mixture model is
used as priors to infer the node clustering assignments. To strengthen the
performance of the inferred clusters, we use a mutual distance loss on the
centers of the Gaussian priors and a clustering assignment hardening loss on
the node embeddings. Experimental results on four real-world attributed network
datasets demonstrate the effectiveness of the proposed VCLANC for attributed
network clustering.
",3.0
275,9,182685,2201.06774,language model for long documents,"Snehal Khandve, Vedangi Wagh, Apurva Wani, Isha Joshi, Raviraj Joshi",Hierarchical Neural Network Approaches for Long Document Classification,"  Text classification algorithms investigate the intricate relationships
between words or phrases and attempt to deduce the document's interpretation.
In the last few years, these algorithms have progressed tremendously.
Transformer architecture and sentence encoders have proven to give superior
results on natural language processing tasks. But a major limitation of these
architectures is their applicability for text no longer than a few hundred
words. In this paper, we explore hierarchical transfer learning approaches for
long document classification. We employ pre-trained Universal Sentence Encoder
(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a
hierarchical setup to capture better representations efficiently. Our proposed
models are conceptually simple where we divide the input data into chunks and
then pass this through base models of BERT and USE. Then output representation
for each chunk is then propagated through a shallow neural network comprising
of LSTMs or CNNs for classifying the text data. These extensions are evaluated
on 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its
stand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its
stand-alone counterpart. However, the hierarchical BERT models are still
desirable as it avoids the quadratic complexity of the attention mechanism in
BERT. Along with the hierarchical approaches, this work also provides a
comparison of different deep learning algorithms like USE, BERT, HAN,
Longformer, and BigBird for long document classification. The Longformer
approach consistently performs well on most of the datasets.
",5.0
276,15,49780,1701.08256,relevance feedback for imformation retrieval,"Nattiya Kanhabua, Philipp Kemkes, Wolfgang Nejdl, Tu Ngoc Nguyen,
  Felipe Reis, Nam Khanh Tran",How to Search the Internet Archive Without Indexing It,"  Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.
",3.0
277,19,144482,2101.09163,artificial intelligence for low carbon,"Ye Ouyang (1), Lilei Wang (1), Aidong Yang (1), Maulik Shah (2), David
  Belanger (3 and 4), Tongqing Gao (5), Leping Wei (6), Yaqin Zhang (7) ((1)
  AsiaInfo Technologies, (2) Verizon, (3) AT&T, (4) Stevens Institute of
  Technology, (5) China Mobile, (6) China Telecom, (7) Tsinghua University)",The Next Decade of Telecommunications Artificial Intelligence,"  It has been an exciting journey since the mobile communications and
artificial intelligence were conceived 37 years and 64 years ago. While both
fields evolved independently and profoundly changed communications and
computing industries, the rapid convergence of 5G and deep learning is
beginning to significantly transform the core communication infrastructure,
network management and vertical applications. The paper first outlines the
individual roadmaps of mobile communications and artificial intelligence in the
early stage, with a concentration to review the era from 3G to 5G when AI and
mobile communications started to converge. With regard to telecommunications
artificial intelligence, the paper further introduces in detail the progress of
artificial intelligence in the ecosystem of mobile communications. The paper
then summarizes the classifications of AI in telecom ecosystems along with its
evolution paths specified by various international telecommunications
standardization bodies. Towards the next decade, the paper forecasts the
prospective roadmap of telecommunications artificial intelligence. In line with
3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network
intelligence following 3GPP and ORAN routes respectively, experience and
intention driven network management and operation, network AI signalling
system, intelligent middle-office based BSS, intelligent customer experience
management and policy control driven by BSS and OSS convergence, evolution from
SLA to ELA, and intelligent private network for verticals. The paper is
concluded with the vision that AI will reshape the future B5G or 6G landscape
and we need pivot our R&D, standardizations, and ecosystem to fully take the
unprecedented opportunities.
",1.0
278,14,170130,2109.10886,text summarization model,Alka Khurana and Vasudha Bhatnagar,Investigating Entropy for Extractive Document Summarization,"  Automatic text summarization aims to cut down readers time and cognitive
effort by reducing the content of a text document without compromising on its
essence. Ergo, informativeness is the prime attribute of document summary
generated by an algorithm, and selecting sentences that capture the essence of
a document is the primary goal of extractive document summarization. In this
paper, we employ Shannon entropy to capture informativeness of sentences. We
employ Non-negative Matrix Factorization (NMF) to reveal probability
distributions for computing entropy of terms, topics, and sentences in latent
space. We present an information theoretic interpretation of the computed
entropy, which is the bedrock of the proposed E-Summ algorithm, an unsupervised
method for extractive document summarization. The algorithm systematically
applies information theoretic principle for selecting informative sentences
from important topics in the document. The proposed algorithm is generic and
fast, and hence amenable to use for summarization of documents in real time.
Furthermore, it is domain-, collection-independent and agnostic to the language
of the document. Benefiting from strictly positive NMF factor matrices, E-Summ
algorithm is transparent and explainable too. We use standard ROUGE toolkit for
performance evaluation of the proposed method on four well known public
data-sets. We also perform quantitative assessment of E-Summ summary quality by
computing its semantic similarity w.r.t the original document. Our
investigation reveals that though using NMF and information theoretic approach
for document summarization promises efficient, explainable, and language
independent text summarization, it needs to be bolstered to match the
performance of deep neural methods.
",5.0
279,3,45558,1609.01893,database management system,Ali Ben Ammar,Query Optimization Techniques In Graph Databases,"  Graph databases (GDB) have recently been arisen to overcome the limits of
traditional databases for storing and managing data with graph-like structure.
Today, they represent a requirement for many applications that manage
graph-like data, like social networks. Most of the techniques, applied to
optimize queries in graph databases, have been used in traditional databases,
distribution systems... or they are inspired from graph theory. However, their
reuse in graph databases should take care of the main characteristics of graph
databases, such as dynamic structure, highly interconnected data, and ability
to efficiently access data relationships. In this paper, we survey the query
optimization techniques in graph databases. In particular, we focus on the
features they have introduced to improve querying graph-like data.
",3.0
280,1,213056,2210.06852,advanced search engine,"Anastasios Nentidis, Georgios Katsimpras, Eirini Vandorou, Anastasia
  Krithara, Antonio Miranda-Escalada, Luis Gasco, Martin Krallinger, Georgios
  Paliouras","Overview of BioASQ 2022: The tenth BioASQ challenge on Large-Scale
  Biomedical Semantic Indexing and Question Answering","  This paper presents an overview of the tenth edition of the BioASQ challenge
in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2022.
BioASQ is an ongoing series of challenges that promotes advances in the domain
of large-scale biomedical semantic indexing and question answering. In this
edition, the challenge was composed of the three established tasks a, b, and
Synergy, and a new task named DisTEMIST for automatic semantic annotation and
grounding of diseases from clinical content in Spanish, a key concept for
semantic indexing and search engines of literature and clinical records. This
year, BioASQ received more than 170 distinct systems from 38 teams in total for
the four different tasks of the challenge. As in previous years, the majority
of the competing systems outperformed the strong baselines, indicating the
continuous advancement of the state-of-the-art in this domain.
",2.0
281,0,216689,cs/0207093,learning to rank with partitioned preference,Jan Chomicki,Preference Queries,"  The handling of user preferences is becoming an increasingly important issue
in present-day information systems. Among others, preferences are used for
information filtering and extraction to reduce the volume of data presented to
the user. They are also used to keep track of user profiles and formulate
policies to improve and automate decision making.
  We propose here a simple, logical framework for formulating preferences as
preference formulas. The framework does not impose any restrictions on the
preference relations and allows arbitrary operation and predicate signatures in
preference formulas. It also makes the composition of preference relations
straightforward. We propose a simple, natural embedding of preference formulas
into relational algebra (and SQL) through a single winnow operator
parameterized by a preference formula. The embedding makes possible the
formulation of complex preference queries, e.g., involving aggregation, by
piggybacking on existing SQL constructs. It also leads in a natural way to the
definition of further, preference-related concepts like ranking. Finally, we
present general algebraic laws governing the winnow operator and its
interaction with other relational algebra operators. The preconditions on the
applicability of the laws are captured by logical formulas. The laws provide a
formal foundation for the algebraic optimization of preference queries. We
demonstrate the usefulness of our approach through numerous examples.
",1.0
282,4,84690,1903.08734,pre-trained language model,"Nicol\`o Frisiani, Alexis Laignelet, Batuhan G\""uler","Combination of multiple Deep Learning architectures for Offensive
  Language Detection in Tweets","  This report contains the details regarding our submission to the OffensEval
2019 (SemEval 2019 - Task 6). The competition was based on the Offensive
Language Identification Dataset. We first discuss the details of the classifier
implemented and the type of input data used and pre-processing performed. We
then move onto critically evaluating our performance. We have achieved a
macro-average F1-score of 0.76, 0.68, 0.54, respectively for Task a, Task b,
and Task c, which we believe reflects on the level of sophistication of the
models implemented. Finally, we will be discussing the difficulties encountered
and possible improvements for the future.
",4.0
283,6,76523,1811.00854,query expansion for imformation retrieval,"Adel Rahimi, Mohammad Bahrani","Improving Information Retrieval Results for Persian Documents using
  FarsNet","  In this paper, we propose a new method for query expansion, which uses
FarsNet (Persian WordNet) to find similar tokens related to the query and
expand the semantic meaning of the query. For this purpose, we use synonymy
relations in FarsNet and extract the related synonyms to query words. This
algorithm is used to enhance information retrieval systems and improve search
results. The overall evaluation of this system in comparison to the baseline
method (without using query expansion) shows an improvement of about 9 percent
in Mean Average Precision (MAP).
",5.0
284,11,152384,2104.02553,PageRank for web search,Kirill P. Kalinin and Natalia G. Berloff,Large-scale Sustainable Search on Unconventional Computing Hardware,"  Since the advent of the Internet, quantifying the relative importance of web
pages is at the core of search engine methods. According to one algorithm,
PageRank, the worldwide web structure is represented by the Google matrix,
whose principal eigenvector components assign a numerical value to web pages
for their ranking. Finding such a dominant eigenvector on an ever-growing
number of web pages becomes a computationally intensive task incompatible with
Moore's Law. We demonstrate that special-purpose optical machines such as
networks of optical parametric oscillators, lasers, and gain-dissipative
condensates, may aid in accelerating the reliable reconstruction of principal
eigenvectors of real-life web graphs. We discuss the feasibility of simulating
the PageRank algorithm on large Google matrices using such unconventional
hardware. We offer alternative rankings based on the minimisation of spin
Hamiltonians. Our estimates show that special-purpose optical machines may
provide dramatic improvements in power consumption over classical computing
architectures.
",2.0
285,16,106730,1912.12187,activation function in neutral networks,Fayyaz ul Amir Afsar Minhas and Amina Asif,Learning Neural Activations,"  An artificial neuron is modelled as a weighted summation followed by an
activation function which determines its output. A wide variety of activation
functions such as rectified linear units (ReLU), leaky-ReLU, Swish, MISH, etc.
have been explored in the literature. In this short paper, we explore what
happens when the activation function of each neuron in an artificial neural
network is learned natively from data alone. This is achieved by modelling the
activation function of each neuron as a small neural network whose weights are
shared by all neurons in the original network. We list our primary findings in
the conclusions section. The code for our analysis is available at:
https://github.com/amina01/Learning-Neural-Activations.
",5.0
286,5,7905,1103.1168,matrix completion,"Yangyang Xu, Wotao Yin, Zaiwen Wen, Yin Zhang","An Alternating Direction Algorithm for Matrix Completion with
  Nonnegative Factors","  This paper introduces an algorithm for the nonnegative matrix
factorization-and-completion problem, which aims to find nonnegative low-rank
matrices X and Y so that the product XY approximates a nonnegative data matrix
M whose elements are partially known (to a certain accuracy). This problem
aggregates two existing problems: (i) nonnegative matrix factorization where
all entries of M are given, and (ii) low-rank matrix completion where
nonnegativity is not required. By taking the advantages of both nonnegativity
and low-rankness, one can generally obtain superior results than those of just
using one of the two properties. We propose to solve the non-convex constrained
least-squares problem using an algorithm based on the classic alternating
direction augmented Lagrangian method. Preliminary convergence properties of
the algorithm and numerical simulation results are presented. Compared to a
recent algorithm for nonnegative matrix factorization, the proposed algorithm
produces factorizations of similar quality using only about half of the matrix
entries. On tasks of recovering incomplete grayscale and hyperspectral images,
the proposed algorithm yields overall better qualities than those produced by
two recent matrix-completion algorithms that do not exploit nonnegativity.
",4.0
287,6,57415,1709.03061,query expansion for imformation retrieval,"Douglas Teodoro, Luc Mottin, Julien Gobeill, Arnaud Gaudinat,
  Th\'er\`ese Vachon, Patrick Ruch","Improving average ranking precision in user searches for biomedical
  research datasets","  Availability of research datasets is keystone for health and life science
study reproducibility and scientific progress. Due to the heterogeneity and
complexity of these data, a main challenge to be overcome by research data
management systems is to provide users with the best answers for their search
queries. In the context of the 2016 bioCADDIE Dataset Retrieval Challenge, we
investigate a novel ranking pipeline to improve the search of datasets used in
biomedical experiments. Our system comprises a query expansion model based on
word embeddings, a similarity measure algorithm that takes into consideration
the relevance of the query terms, and a dataset categorisation method that
boosts the rank of datasets matching query constraints. The system was
evaluated using a corpus with 800k datasets and 21 annotated user queries. Our
system provides competitive results when compared to the other challenge
participants. In the official run, it achieved the highest infAP among the
participants, being +22.3% higher than the median infAP of the participant's
best submissions. Overall, it is ranked at top 2 if an aggregated metric using
the best official measures per participant is considered. The query expansion
method showed positive impact on the system's performance increasing our
baseline up to +5.0% and +3.4% for the infAP and infNDCG metrics, respectively.
Our similarity measure algorithm seems to be robust, in particular compared to
Divergence From Randomness framework, having smaller performance variations
under different training conditions. Finally, the result categorization did not
have significant impact on the system's performance. We believe that our
solution could be used to enhance biomedical dataset management systems. In
particular, the use of data driven query expansion methods could be an
alternative to the complexity of biomedical terminologies.
",4.0
288,4,50205,1702.03197,pre-trained language model,"Abdulaziz M. Alayba, Vasile Palade, Matthew England and Rahat Iqbal",Arabic Language Sentiment Analysis on Health Services,"  The social media network phenomenon leads to a massive amount of valuable
data that is available online and easy to access. Many users share images,
videos, comments, reviews, news and opinions on different social networks
sites, with Twitter being one of the most popular ones. Data collected from
Twitter is highly unstructured, and extracting useful information from tweets
is a challenging task. Twitter has a huge number of Arabic users who mostly
post and write their tweets using the Arabic language. While there has been a
lot of research on sentiment analysis in English, the amount of researches and
datasets in Arabic language is limited. This paper introduces an Arabic
language dataset which is about opinions on health services and has been
collected from Twitter. The paper will first detail the process of collecting
the data from Twitter and also the process of filtering, pre-processing and
annotating the Arabic text in order to build a big sentiment analysis dataset
in Arabic. Several Machine Learning algorithms (Naive Bayes, Support Vector
Machine and Logistic Regression) alongside Deep and Convolutional Neural
Networks were utilized in our experiments of sentiment analysis on our health
dataset.
",2.0
289,15,12325,1203.5084,relevance feedback for imformation retrieval,"Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood",A Data Driven Approach to Query Expansion in Question Answering,"  Automated answering of natural language questions is an interesting and
useful problem to solve. Question answering (QA) systems often perform
information retrieval at an initial stage. Information retrieval (IR)
performance, provided by engines such as Lucene, places a bound on overall
system performance. For example, no answer bearing documents are retrieved at
low ranks for almost 40% of questions.
  In this paper, answer texts from previous QA evaluations held as part of the
Text REtrieval Conferences (TREC) are paired with queries and analysed in an
attempt to identify performance-enhancing words. These words are then used to
evaluate the performance of a query expansion method.
  Data driven extension words were found to help in over 70% of difficult
questions. These words can be used to improve and evaluate query expansion
methods. Simple blind relevance feedback (RF) was correctly predicted as
unlikely to help overall performance, and an possible explanation is provided
for its low value in IR for QA.
",2.0
290,11,49183,1701.02595,PageRank for web search,"Alexander Gasnikov, Maxim Zhukovskii, Sergey Kim, Fedor Noskov, Stepan
  Plaunov, Daniil Smirnov","Around power law for PageRank components in Buckley-Osthus model of web
  graph","  In the paper we investigate power law for PageRank components for the
Buckley-Osthus model for web graph. We compare different numerical methods for
PageRank calculation. With the best method we do a lot of numerical
experiments. These experiments confirm the hypothesis about power law. At the
end we discuss real model of web-ranking based on the classical PageRank
approach.
",5.0
291,7,177394,2111.11477,gradient boosting,"Quazi Adibur Rahman Adib, Sidratul Tanzila Tasmi, Md. Shahriar Islam
  Bhuiyan, Md. Mohsin Sarker Raihan and Abdullah Bin Shams","Prediction Model for Mortality Analysis of Pregnant Women Affected With
  COVID-19","  COVID-19 pandemic is an ongoing global pandemic which has caused
unprecedented disruptions in the public health sector and global economy. The
virus, SARS-CoV-2 is responsible for the rapid transmission of coronavirus
disease. Due to its contagious nature, the virus can easily infect an
unprotected and exposed individual from mild to severe symptoms. The study of
the virus effects on pregnant mothers and neonatal is now a concerning issue
globally among civilians and public health workers considering how the virus
will affect the mother and the neonates health. This paper aims to develop a
predictive model to estimate the possibility of death for a COVID-diagnosed
mother based on documented symptoms: dyspnea, cough, rhinorrhea, arthralgia,
and the diagnosis of pneumonia. The machine learning models that have been used
in our study are support vector machine, decision tree, random forest, gradient
boosting, and artificial neural network. The models have provided impressive
results and can accurately predict the mortality of pregnant mothers with a
given input.The precision rate for 3 models(ANN, Gradient Boost, Random Forest)
is 100% The highest accuracy score(Gradient Boosting,ANN) is 95%,highest
recall(Support Vector Machine) is 92.75% and highest f1 score(Gradient
Boosting,ANN) is 94.66%. Due to the accuracy of the model, pregnant mother can
expect immediate medical treatment based on their possibility of death due to
the virus. The model can be utilized by health workers globally to list down
emergency patients, which can ultimately reduce the death rate of COVID-19
diagnosed pregnant mothers.
",3.0
292,15,84028,1903.03705,relevance feedback for imformation retrieval,"Urvashi Oswal, Aniruddha Bhargava, and Robert Nowak",Linear Bandits with Feature Feedback,"  This paper explores a new form of the linear bandit problem in which the
algorithm receives the usual stochastic rewards as well as stochastic feedback
about which features are relevant to the rewards, the latter feedback being the
novel aspect. The focus of this paper is the development of new theory and
algorithms for linear bandits with feature feedback. We show that linear
bandits with feature feedback can achieve regret over time horizon $T$ that
scales like $k\sqrt{T}$, without prior knowledge of which features are relevant
nor the number $k$ of relevant features. In comparison, the regret of
traditional linear bandits is $d\sqrt{T}$, where $d$ is the total number of
(relevant and irrelevant) features, so the improvement can be dramatic if $k\ll
d$. The computational complexity of the new algorithm is proportional to $k$
rather than $d$, making it much more suitable for real-world applications
compared to traditional linear bandits. We demonstrate the performance of the
new algorithm with synthetic and real human-labeled data.
",0.0
293,9,94585,1907.12293,language model for long documents,Weinan E and Yajun Zhou,A mathematical model for universal semantics,"  We characterize the meaning of words with language-independent numerical
fingerprints, through a mathematical analysis of recurring patterns in texts.
Approximating texts by Markov processes on a long-range time scale, we are able
to extract topics, discover synonyms, and sketch semantic fields from a
particular document of moderate length, without consulting external
knowledge-base or thesaurus. Our Markov semantic model allows us to represent
each topical concept by a low-dimensional vector, interpretable as algebraic
invariants in succinct statistical operations on the document, targeting local
environments of individual words. These language-independent semantic
representations enable a robot reader to both understand short texts in a given
language (automated question-answering) and match medium-length texts across
different languages (automated word translation). Our semantic fingerprints
quantify local meaning of words in 14 representative languages across 5 major
language families, suggesting a universal and cost-effective mechanism by which
human languages are processed at the semantic level. Our protocols and source
codes are publicly available on
https://github.com/yajun-zhou/linguae-naturalis-principia-mathematica
",2.0
294,5,2639,902.3846,matrix completion,"Amit Singer, Mihai Cucuringu",Uniqueness of Low-Rank Matrix Completion by Rigidity Theory,"  The problem of completing a low-rank matrix from a subset of its entries is
often encountered in the analysis of incomplete data sets exhibiting an
underlying factor model with applications in collaborative filtering, computer
vision and control. Most recent work had been focused on constructing efficient
algorithms for exact or approximate recovery of the missing matrix entries and
proving lower bounds for the number of known entries that guarantee a
successful recovery with high probability. A related problem from both the
mathematical and algorithmic point of view is the distance geometry problem of
realizing points in a Euclidean space from a given subset of their pairwise
distances. Rigidity theory answers basic questions regarding the uniqueness of
the realization satisfying a given partial set of distances. We observe that
basic ideas and tools of rigidity theory can be adapted to determine uniqueness
of low-rank matrix completion, where inner products play the role that
distances play in rigidity theory. This observation leads to an efficient
randomized algorithm for testing both local and global unique completion.
Crucial to our analysis is a new matrix, which we call the completion matrix,
that serves as the analogue of the rigidity matrix.
",5.0
295,4,179782,2112.07259,pre-trained language model,"Yazheng Yang, Boyuan Pan, Deng Cai, Huan Sun",TopNet: Learning from Neural Topic Model to Generate Long Stories,"  Long story generation (LSG) is one of the coveted goals in natural language
processing. Different from most text generation tasks, LSG requires to output a
long story of rich content based on a much shorter text input, and often
suffers from information sparsity. In this paper, we propose \emph{TopNet} to
alleviate this problem, by leveraging the recent advances in neural topic
modeling to obtain high-quality skeleton words to complement the short input.
In particular, instead of directly generating a story, we first learn to map
the short text input to a low-dimensional topic distribution (which is
pre-assigned by a topic model). Based on this latent topic distribution, we can
use the reconstruction decoder of the topic model to sample a sequence of
inter-related words as a skeleton for the story. Experiments on two benchmark
datasets show that our proposed framework is highly effective in skeleton word
selection and significantly outperforms the state-of-the-art models in both
automatic evaluation and human evaluation.
",5.0
296,9,629,710.478,language model for long documents,"J. M. Almendros-Jim\'enez and A. Becerra-Ter\'on and F. J.
  Enciso-Ba\~nos",Querying XML Documents in Logic Programming,"  Extensible Markup Language (XML) is a simple, very flexible text format
derived from SGML. Originally designed to meet the challenges of large-scale
electronic publishing, XML is also playing an increasingly important role in
the exchange of a wide variety of data on the Web and elsewhere. XPath language
is the result of an effort to provide address parts of an XML document. In
support of this primary purpose, it becomes in a query language against an XML
document. In this paper we present a proposal for the implementation of the
XPath language in logic programming. With this aim we will describe the
representation of XML documents by means of a logic program. Rules and facts
can be used for representing the document schema and the XML document itself.
In particular, we will present how to index XML documents in logic programs:
rules are supposed to be stored in main memory, however facts are stored in
secondary memory by using two kind of indexes: one for each XML tag, and other
for each group of terminal items. In addition, we will study how to query by
means of the XPath language against a logic program representing an XML
document. It evolves the specialization of the logic program with regard to the
XPath expression. Finally, we will also explain how to combine the indexing and
the top-down evaluation of the logic program. To appear in Theory and Practice
of Logic Programming (TPLP)""
",1.0
297,6,71226,1807.08061,query expansion for imformation retrieval,"Surya Kallumadi, Bhaskar Mitra and Tereza Iofciu",A Line in the Sand: Recommendation or Ad-hoc Retrieval?,"  The popular approaches to recommendation and ad-hoc retrieval tasks are
largely distinct in the literature. In this work, we argue that many
recommendation problems can also be cast as ad-hoc retrieval tasks. To
demonstrate this, we build a solution for the RecSys 2018 Spotify challenge by
combining standard ad-hoc retrieval models and using popular retrieval tools
sets. We draw a parallel between the playlist continuation task and the task of
finding good expansion terms for queries in ad-hoc retrieval, and show that
standard pseudo-relevance feedback can be effective as a collaborative
filtering approach. We also use ad-hoc retrieval for content-based
recommendation by treating the input playlist title as a query and associating
all candidate tracks with meta-descriptions extracted from the background data.
The recommendations from these two approaches are further supplemented by a
nearest neighbor search based on track embeddings learned by a popular neural
model. Our final ranked list of recommendations is produced by a learning to
rank model. Our proposed solution using ad-hoc retrieval models achieved a
competitive performance on the music recommendation task at RecSys 2018
challenge---finishing at rank 7 out of 112 participating teams and at rank 5
out of 31 teams for the main and the creative tracks, respectively.
",2.0
298,7,161047,2106.11055,gradient boosting,Mersha Nigus and Dorsewamy,"Performance Evaluation of Classification Models for Household Income,
  Consumption and Expenditure Data Set","  Food security is more prominent on the policy agenda today than it has been
in the past, thanks to recent food shortages at both the regional and global
levels as well as renewed promises from major donor countries to combat chronic
hunger. One field where machine learning can be used is in the classification
of household food insecurity. In this study, we establish a robust methodology
to categorize whether or not a household is being food secure and food insecure
by machine learning algorithms. In this study, we have used ten machine
learning algorithms to classify the food security status of the Household.
Gradient Boosting (GB), Random Forest (RF), Extra Tree (ET), Bagging, K-Nearest
Neighbor (KNN), Decision Tree (DT), Support Vector Machine (SVM), Logistic
Regression (LR), Ada Boost (AB) and Naive Bayes were the classification
algorithms used throughout this study (NB). Then, we perform classification
tasks from developing data set for household food security status by gathering
data from HICE survey data and validating it by Domain Experts. The performance
of all classifiers has better results for all performance metrics. The
performance of the Random Forest and Gradient Boosting models are outstanding
with a testing accuracy of 0.9997 and the other classifier such as Bagging,
Decision tree, Ada Boost, Extra tree, K-nearest neighbor, Logistic Regression,
SVM and Naive Bayes are scored 0.9996, 0.09996, 0.9994, 0.95675, 0.9415,
0.8915, 0.7853 and 0.7595, respectively.
",3.0
299,3,15608,1210.2872,database management system,Tipawan Silwattananusarn and Kulthida Tuamsuk,"Data Mining and Its Applications for Knowledge Management: A Literature
  Review from 2007 to 2012","  Data mining is one of the most important steps of the knowledge discovery in
databases process and is considered as significant subfield in knowledge
management. Research in data mining continues growing in business and in
learning organization over coming decades. This review paper explores the
applications of data mining techniques which have been developed to support
knowledge management process. The journal articles indexed in ScienceDirect
Database from 2007 to 2012 are analyzed and classified. The discussion on the
findings is divided into 4 topics: (i) knowledge resource; (ii) knowledge types
and/or knowledge datasets; (iii) data mining tasks; and (iv) data mining
techniques and applications used in knowledge management. The article first
briefly describes the definition of data mining and data mining functionality.
Then the knowledge management rationale and major knowledge management tools
integrated in knowledge management cycle are described. Finally, the
applications of data mining techniques in the process of knowledge management
are summarized and discussed.
",0.0
300,9,136554,2010.16407,language model for long documents,"Yatin Chaudhary, Pankaj Gupta, Khushbu Saxena, Vivek Kulkarni, Thomas
  Runkler, Hinrich Sch\""utze",TopicBERT for Energy Efficient Document Classification,"  Prior research notes that BERT's computational cost grows quadratically with
sequence length thus leading to longer training times, higher GPU memory
constraints and carbon emissions. While recent work seeks to address these
scalability issues at pre-training, these issues are also prominent in
fine-tuning especially for long sequence tasks like document classification.
Our work thus focuses on optimizing the computational cost of fine-tuning for
document classification. We achieve this by complementary learning of both
topic and language models in a unified framework, named TopicBERT. This
significantly reduces the number of self-attention operations - a main
performance bottleneck. Consequently, our model achieves a 1.4x ($\sim40\%$)
speedup with $\sim40\%$ reduction in $CO_2$ emission while retaining $99.9\%$
performance over 5 datasets.
",1.0
301,0,76594,1811.01282,learning to rank with partitioned preference,Heide Gluesing-Luerssen and Alberto Ravagnani,Partitions of Matrix Spaces With an Application to $q$-Rook Polynomials,"  We study the row-space partition and the pivot partition on the matrix space
$\mathbb{F}_q^{n \times m}$. We show that both these partitions are reflexive
and that the row-space partition is self-dual. Moreover, using various
combinatorial methods, we explicitly compute the Krawtchouk coefficients
associated with these partitions. This establishes MacWilliams-type identities
for the row-space and pivot enumerators of linear rank-metric codes. We then
generalize the Singleton-like bound for rank-metric codes, and introduce two
new concepts of code extremality. Both of them generalize the notion of MRD
codes and are preserved by trace-duality. Moreover, codes that are extremal
according to either notion satisfy strong rigidity properties analogous to
those of MRD codes. As an application of our results to combinatorics, we give
closed formulas for the $q$-rook polynomials associated with Ferrers diagram
boards. Moreover, we exploit connections between matrices over finite fields
and rook placements to prove that the number of matrices of rank $r$ over
$\mathbb{F}_q$ supported on a Ferrers diagram is a polynomial in $q$, whose
degree is strictly increasing in $r$. Finally, we investigate the natural
analogues of the MacWilliams Extension Theorem for the rank, the row-space, and
the pivot partitions.
",0.0
302,17,129720,2008.1265,robustness of neutral networks,Peter Meer,"Are Deep Neural Networks ""Robust""?","  Separating outliers from inliers is the definition of robustness in computer
vision. This essay delineates how deep neural networks are different than
typical robust estimators. Deep neural networks not robust by this traditional
definition.
",5.0
303,8,54286,1706.02216,node embedding for graph,"William L. Hamilton, Rex Ying, Jure Leskovec",Inductive Representation Learning on Large Graphs,"  Low-dimensional embeddings of nodes in large graphs have proved extremely
useful in a variety of prediction tasks, from content recommendation to
identifying protein functions. However, most existing approaches require that
all nodes in the graph are present during training of the embeddings; these
previous approaches are inherently transductive and do not naturally generalize
to unseen nodes. Here we present GraphSAGE, a general, inductive framework that
leverages node feature information (e.g., text attributes) to efficiently
generate node embeddings for previously unseen data. Instead of training
individual embeddings for each node, we learn a function that generates
embeddings by sampling and aggregating features from a node's local
neighborhood. Our algorithm outperforms strong baselines on three inductive
node-classification benchmarks: we classify the category of unseen nodes in
evolving information graphs based on citation and Reddit post data, and we show
that our algorithm generalizes to completely unseen graphs using a multi-graph
dataset of protein-protein interactions.
",5.0
304,7,195175,2205.04333,gradient boosting,"Sajal Saha, Anwar Haque, and Greg Sidebottom","Wavelet-Based Hybrid Machine Learning Model for Out-of-distribution
  Internet Traffic Prediction","  Efficient prediction of internet traffic is essential for ensuring proactive
management of computer networks. Nowadays, machine learning approaches show
promising performance in modeling real-world complex traffic. However, most
existing works assumed that model training and evaluation data came from
identical distribution. But in practice, there is a high probability that the
model will deal with data from a slightly or entirely unknown distribution in
the deployment phase. This paper investigated and evaluated machine learning
performances using eXtreme Gradient Boosting, Light Gradient Boosting Machine,
Stochastic Gradient Descent, Gradient Boosting Regressor, CatBoost Regressor,
and their stacked ensemble model using data from both identical and out-of
distribution. Also, we proposed a hybrid machine learning model integrating
wavelet decomposition for improving out-of-distribution prediction as
standalone models were unable to generalize very well. Our experimental results
show the best performance of the standalone ensemble model with an accuracy of
96.4%, while the hybrid ensemble model improved it by 1% for in-distribution
data. But its performance dropped significantly when tested with three
different datasets having a distribution shift than the training set. However,
our proposed hybrid model considerably reduces the performance gap between
identical and out-of-distribution evaluation compared with the standalone
model, indicating the decomposition technique's effectiveness in the case of
out-of-distribution generalization.
",3.0
305,14,78725,1812.02303,text summarization model,"Tian Shi, Yaser Keneshloo, Naren Ramakrishnan, Chandan K. Reddy",Neural Abstractive Text Summarization with Sequence-to-Sequence Models,"  In the past few years, neural abstractive text summarization with
sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many
interesting techniques have been proposed to improve seq2seq models, making
them capable of handling different challenges, such as saliency, fluency and
human readability, and generate high-quality summaries. Generally speaking,
most of these techniques differ in one of these three categories: network
structure, parameter inference, and decoding/generation. There are also other
concerns, such as efficiency and parallelism for training a model. In this
paper, we provide a comprehensive literature survey on different seq2seq models
for abstractive text summarization from the viewpoint of network structures,
training strategies, and summary generation algorithms. Several models were
first proposed for language modeling and generation tasks, such as machine
translation, and later applied to abstractive text summarization. Hence, we
also provide a brief review of these models. As part of this survey, we also
develop an open source library, namely, Neural Abstractive Text Summarizer
(NATS) toolkit, for the abstractive text summarization. An extensive set of
experiments have been conducted on the widely used CNN/Daily Mail dataset to
examine the effectiveness of several different neural network components.
Finally, we benchmark two models implemented in NATS on the two recently
released datasets, namely, Newsroom and Bytecup.
",5.0
306,0,43536,1606.05859,learning to rank with partitioned preference,"Shenglin Zhao, Tong Zhao, Irwin King and Michael R. Lyu","GT-SEER: Geo-Temporal SEquential Embedding Rank for Point-of-interest
  Recommendation","  Point-of-interest (POI) recommendation is an important application in
location-based social networks (LBSNs), which learns the user preference and
mobility pattern from check-in sequences to recommend POIs. However, previous
POI recommendation systems model check-in sequences based on either tensor
factorization or Markov chain model, which cannot capture contextual check-in
information in sequences. The contextual check-in information implies the
complementary functions among POIs that compose an individual's daily check-in
sequence. In this paper, we exploit the embedding learning technique to capture
the contextual check-in information and further propose the
\textit{{\textbf{SE}}}quential \textit{{\textbf{E}}}mbedding
\textit{{\textbf{R}}}ank (\textit{SEER}) model for POI recommendation. In
particular, the \textit{SEER} model learns user preferences via a pairwise
ranking model under the sequential constraint modeled by the POI embedding
learning method. Furthermore, we incorporate two important factors, i.e.,
temporal influence and geographical influence, into the \textit{SEER} model to
enhance the POI recommendation system. Due to the temporal variance of
sequences on different days, we propose a temporal POI embedding model and
incorporate the temporal POI representations into a temporal preference ranking
model to establish the \textit{T}emporal \textit{SEER} (\textit{T-SEER}) model.
In addition, We incorporate the geographical influence into the \textit{T-SEER}
model and develop the \textit{\textbf{Geo-Temporal}} \textit{{\textbf{SEER}}}
(\textit{GT-SEER}) model.
",1.0
307,11,45783,1609.04556,PageRank for web search,"Dong Nguyen, Thomas Demeester, Dolf Trieschnigg, Djoerd Hiemstra",Resource Selection for Federated Search on the Web,"  A publicly available dataset for federated search reflecting a real web
environment has long been absent, making it difficult for researchers to test
the validity of their federated search algorithms for the web setting. We
present several experiments and analyses on resource selection on the web using
a recently released test collection containing the results from more than a
hundred real search engines, ranging from large general web search engines such
as Google, Bing and Yahoo to small domain-specific engines. First, we
experiment with estimating the size of uncooperative search engines on the web
using query based sampling and propose a new method using the ClueWeb09
dataset. We find the size estimates to be highly effective in resource
selection. Second, we show that an optimized federated search system based on
smaller web search engines can be an alternative to a system using large web
search engines. Third, we provide an empirical comparison of several popular
resource selection methods and find that these methods are not readily suitable
for resource selection on the web. Challenges include the sparse resource
descriptions and extremely skewed sizes of collections.
",1.0
308,6,187627,2202.13869,query expansion for imformation retrieval,"Zhongkai Sun, Sixing Lu, Chengyuan Ma, Xiaohu Liu, Chenlei Guo","Query Expansion and Entity Weighting for Query Reformulation Retrieval
  in Voice Assistant Systems","  Voice assistants such as Alexa, Siri, and Google Assistant have become
increasingly popular worldwide. However, linguistic variations, variability of
speech patterns, ambient acoustic conditions, and other such factors are often
correlated with the assistants misinterpreting the user's query. In order to
provide better customer experience, retrieval based query reformulation (QR)
systems are widely used to reformulate those misinterpreted user queries.
Current QR systems typically focus on neural retrieval model training or direct
entities retrieval for the reformulating. However, these methods rarely focus
on query expansion and entity weighting simultaneously, which may limit the
scope and accuracy of the query reformulation retrieval. In this work, we
propose a novel Query Expansion and Entity Weighting method (QEEW), which
leverages the relationships between entities in the entity catalog (consisting
of users' queries, assistant's responses, and corresponding entities), to
enhance the query reformulation performance. Experiments on Alexa annotated
data demonstrate that QEEW improves all top precision metrics, particularly 6%
improvement in top10 precision, compared with baselines not using query
expansion and weighting; and more than 5% improvement in top10 precision
compared with other baselines using query expansion and weighting.
",4.0
309,14,77415,1811.06567,text summarization model,Chandra Shekhar Yadav,Automatic Text Document Summarization using Semantic-based Analysis,"  Since the advent of the web, the amount of data on wen has been increased
several million folds. In recent years web data generated is more than data
stored for years. One important data format is text. To answer user queries
over the internet, and to overcome the problem of information overload one
possible solution is text document summarization. This not only reduces query
access time, but also optimize the document results according to specific users
requirements. Summarization of text document can be categorized as abstractive
and extractive. Most of the work has been done in the direction of Extractive
summarization. Extractive summarized result is a subset of original documents
with the objective of more content coverage and lea redundancy. Our work is
based on Extractive approaches. In the first approach, we are using some
statistical features and semantic-based features. To include sentiment as a
feature is an idea cached from a view that emotion plays an important role. It
effectively conveys a message. So, it may play a vital role in text document
summarization.
",4.0
310,12,61877,1801.02383,COVID-19 and social media,"Xianwen Wang, Yunxue Cui, Qingchun Li and Xinhui Guo","Social Media Attention Increases Article Visits: An Investigation on
  Article-Level Referral Data of PeerJ","  In order to better understand the effect of social media in the dissemination
of scholarly articles, employing the daily updated referral data of 110 PeerJ
articles collected over a period of 345 days, we analyze the relationship
between social media attention and article visitors directed by social media.
Our results show that social media presence of PeerJ articles is high. About
68.18% of the papers receive at least one tweet from Twitter accounts other
than @PeerJ, the official account of the journal. Social media attention
increases the dissemination of scholarly articles. Altmetrics could not only
act as the complement of traditional citation measures but also play an
important role in increasing the article downloads and promoting the impacts of
scholarly articles. There also exists a significant correlation among the
online attention from different social media platforms. Articles with more
Facebook shares tend to get more tweets. The temporal trends show that social
attention comes immediately following publication but does not last long, so do
the social media directed article views.
",1.0
311,8,103342,1911.0675,node embedding for graph,"Chanyoung Park, Donghyun Kim, Jiawei Han, Hwanjo Yu",Unsupervised Attributed Multiplex Network Embedding,"  Nodes in a multiplex network are connected by multiple types of relations.
However, most existing network embedding methods assume that only a single type
of relation exists between nodes. Even for those that consider the multiplexity
of a network, they overlook node attributes, resort to node labels for
training, and fail to model the global properties of a graph. We present a
simple yet effective unsupervised network embedding method for attributed
multiplex network called DMGI, inspired by Deep Graph Infomax (DGI) that
maximizes the mutual information between local patches of a graph, and the
global representation of the entire graph. We devise a systematic way to
jointly integrate the node embeddings from multiple graphs by introducing 1)
the consensus regularization framework that minimizes the disagreements among
the relation-type specific node embeddings, and 2) the universal discriminator
that discriminates true samples regardless of the relation types. We also show
that the attention mechanism infers the importance of each relation type, and
thus can be useful for filtering unnecessary relation types as a preprocessing
step. Extensive experiments on various downstream tasks demonstrate that DMGI
outperforms the state-of-the-art methods, even though DMGI is fully
unsupervised.
",5.0
312,0,83583,1903.00543,learning to rank with partitioned preference,Aadirupa Saha and Aditya Gopalan,Combinatorial Bandits with Relative Feedback,"  We consider combinatorial online learning with subset choices when only
relative feedback information from subsets is available, instead of bandit or
semi-bandit feedback which is absolute. Specifically, we study two regret
minimisation problems over subsets of a finite ground set $[n]$, with
subset-wise relative preference information feedback according to the
Multinomial logit choice model. In the first setting, the learner can play
subsets of size bounded by a maximum size and receives top-$m$ rank-ordered
feedback, while in the second setting the learner can play subsets of a fixed
size $k$ with a full subset ranking observed as feedback. For both settings, we
devise instance-dependent and order-optimal regret algorithms with regret
$O(\frac{n}{m} \ln T)$ and $O(\frac{n}{k} \ln T)$, respectively. We derive
fundamental limits on the regret performance of online learning with
subset-wise preferences, proving the tightness of our regret guarantees. Our
results also show the value of eliciting more general top-$m$ rank-ordered
feedback over single winner feedback ($m=1$). Our theoretical results are
corroborated with empirical evaluations.
",3.0
313,3,120348,2006.01294,database management system,"Fubao Wu, Han Hee Song, Jiangtao Yin, Lixin Gao, Mario Baldi, Narendra
  Anand",NEMA: Automatic Integration of Large Network Management Databases,"  Network management, whether for malfunction analysis, failure prediction,
performance monitoring and improvement, generally involves large amounts of
data from different sources. To effectively integrate and manage these sources,
automatically finding semantic matches among their schemas or ontologies is
crucial. Existing approaches on database matching mainly fall into two
categories. One focuses on the schema-level matching based on schema properties
such as field names, data types, constraints and schema structures. Network
management databases contain massive tables (e.g., network products, incidents,
security alert and logs) from different departments and groups with nonuniform
field names and schema characteristics. It is not reliable to match them by
those schema properties. The other category is based on the instance-level
matching using general string similarity techniques, which are not applicable
for the matching of large network management databases. In this paper, we
develop a matching technique for large NEtwork MAnagement databases (NEMA)
deploying instance-level matching for effective data integration and
connection. We design matching metrics and scores for both numerical and
non-numerical fields and propose algorithms for matching these fields. The
effectiveness and efficiency of NEMA are evaluated by conducting experiments
based on ground truth field pairs in large network management databases. Our
measurement on large databases with 1,458 fields, each of which contains over
10 million records, reveals that the accuracies of NEMA are up to 95%. It
achieves 2%-10% higher accuracy and 5x-14x speedup over baseline methods.
",4.0
314,4,26483,1405.174,pre-trained language model,"Kutlu Emre Y{\i}lmaz, Ahmet Arslan, Ozgur Yilmazel",Turkish Text Retrieval Experiments Using Lemur Toolkit,"  We used Lemur Toolkit, an open source toolkit designed for Information
Retrieval (IR) research, for our automated indexing and retrieval experiments
on a TREC-like test collection for Turkish. We study and compare three
retrieval models Lemur supports, especially Language modeling approach to IR,
combined with language specific preprocessing techniques. Our experiments show
that all retrieval models benefits from language specific preprocessing in
terms of retrieval quality. Also Language Modeling approach is the best
performing retrieval model when language specific preprocessing applied.
",3.0
315,19,107809,2001.04829,artificial intelligence for low carbon,Gavin H. Graham and Yan Chen,"Bayesian Inversion Of Generative Models For Geologic Storage Of Carbon
  Dioxide","  Carbon capture and storage (CCS) can aid decarbonization of the atmosphere to
limit further global temperature increases. A framework utilizing unsupervised
learning is used to generate a range of subsurface geologic volumes to
investigate potential sites for long-term storage of carbon dioxide. Generative
adversarial networks are used to create geologic volumes, with a further neural
network used to sample the posterior distribution of a trained Generator
conditional to sparsely sampled physical measurements. These generative models
are further conditioned to historic dynamic fluid flow data through Bayesian
inversion to improve the resolution of the forecast of the storage capacity of
injected carbon dioxide.
",5.0
316,8,85137,1903.12287,node embedding for graph,"Adam Lerer, Ledell Wu, Jiajun Shen, Timothee Lacroix, Luca Wehrstedt,
  Abhijit Bose, Alex Peysakhovich",PyTorch-BigGraph: A Large-scale Graph Embedding System,"  Graph embedding methods produce unsupervised node features from graphs that
can then be used for a variety of machine learning tasks. Modern graphs,
particularly in industrial applications, contain billions of nodes and
trillions of edges, which exceeds the capability of existing embedding systems.
We present PyTorch-BigGraph (PBG), an embedding system that incorporates
several modifications to traditional multi-relation embedding systems that
allow it to scale to graphs with billions of nodes and trillions of edges. PBG
uses graph partitioning to train arbitrarily large embeddings on either a
single machine or in a distributed environment. We demonstrate comparable
performance with existing embedding systems on common benchmarks, while
allowing for scaling to arbitrarily large graphs and parallelization on
multiple machines. We train and evaluate embeddings on several large social
network graphs as well as the full Freebase dataset, which contains over 100
million nodes and 2 billion edges.
",5.0
317,19,156293,2105.06564,artificial intelligence for low carbon,"Yingbo Li, Yucong Duan, Anamaria-Beatrice Spulber, Haoyang Che,
  Zakaria Maamar, Zhao Li, Chen Yang, Yu lei","Physical Artificial Intelligence: The Concept Expansion of
  Next-Generation Artificial Intelligence","  Artificial Intelligence has been a growth catalyst to our society and is
cosidered across all idustries as a fundamental technology. However, its
development has been limited to the signal processing domain that relies on the
generated and collected data from other sensors. In recent research, concepts
of Digital Artificial Intelligence and Physicial Artifical Intelligence have
emerged and this can be considered a big step in the theoretical development of
Artifical Intelligence. In this paper we explore the concept of Physicial
Artifical Intelligence and propose two subdomains: Integrated Physicial
Artifical Intelligence and Distributed Physicial Artifical Intelligence. The
paper will also examine the trend and governance of Physicial Artifical
Intelligence.
",1.0
318,18,64064,1802.08988,infomation retrieval time complexity,Baoyang Song,Deep Neural Network for Learning to Rank Query-Text Pairs,"  This paper considers the problem of document ranking in information retrieval
systems by Learning to Rank. We propose ConvRankNet combining a Siamese
Convolutional Neural Network encoder and the RankNet ranking model which could
be trained in an end-to-end fashion. We prove a general result justifying the
linear test-time complexity of pairwise Learning to Rank approach. Experiments
on the OHSUMED dataset show that ConvRankNet outperforms systematically
existing feature-based models.
",5.0
319,13,134466,2010.07773,social network analysis with natrual language processing,"Shubhanker Banerjee, Arun Jayapal and Sajeetha Thavareesan","NUIG-Shubhanker@Dravidian-CodeMix-FIRE2020: Sentiment Analysis of
  Code-Mixed Dravidian text using XLNet","  Social media has penetrated into multilingual societies, however most of them
use English to be a preferred language for communication. So it looks natural
for them to mix their cultural language with English during conversations
resulting in abundance of multilingual data, call this code-mixed data,
available in todays' world.Downstream NLP tasks using such data is challenging
due to the semantic nature of it being spread across multiple languages.One
such Natural Language Processing task is sentiment analysis, for this we use an
auto-regressive XLNet model to perform sentiment analysis on code-mixed
Tamil-English and Malayalam-English datasets.
",3.0
320,9,62052,1801.03872,language model for long documents,"Valentin Kuznetsov, Nils Leif Fischer, Yuyi Guo","The archive solution for distributed workflow management agents of the
  CMS experiment at LHC","  The CMS experiment at the CERN LHC developed the Workflow Management Archive
system to persistently store unstructured framework job report documents
produced by distributed workflow management agents. In this paper we present
its architecture, implementation, deployment, and integration with the CMS and
CERN computing infrastructures, such as central HDFS and Hadoop Spark cluster.
The system leverages modern technologies such as a document oriented database
and the Hadoop eco-system to provide the necessary flexibility to reliably
process, store, and aggregate $\mathcal{O}$(1M) documents on a daily basis. We
describe the data transformation, the short and long term storage layers, the
query language, along with the aggregation pipeline developed to visualize
various performance metrics to assist CMS data operators in assessing the
performance of the CMS computing system.
",0.0
321,1,36674,1509.08396,advanced search engine,Jai Manral and Mohammed Alamgir Hossain,An Innovative Approach for online Meta Search Engine Optimization,"  This paper presents an approach to identify efficient techniques used in Web
Search Engine Optimization (SEO). Understanding SEO factors which can influence
page ranking in search engine is significant for webmasters who wish to attract
large number of users to their website. Different from previous relevant
research, in this study we developed an intelligent Meta search engine which
aggregates results from various search engines and ranks them based on several
important SEO parameters. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction. Initial results generated from Meta search engine
outperformed existing search engines in terms of better retrieved search
results with high precision.
",4.0
322,11,152414,2104.02666,PageRank for web search,Qiwei Ma and Zhaoya Gong,Calibratable Hetero-NodeRank for measuring node influence,"  Node influence metrics have been applied to many applications, including
ranking web pages on internet, or locations on spatial networks. PageRank is a
popular and effective algorithm for estimating node influence. However,
conventional PageRank method considers neither the heterogeneity of network
structures nor additional network information, causing a major impedance to
performance improvement and an underestimation of non-hub nodes' importance. As
these problems are only partially studied, existing solutions are still not
satisfying. This paper addresses the problems by presenting a general
PageRank-based model framework, dubbed Hetero-NodeRank, that accounts for
heterogeneous network topology and incorporates node attribute information to
capture both link- and node-based effects in measuring node influence.
Moreover, the framework enables the calibration of the proposed model against
empirical data, which transforms the original deductive approach into an
inductive one that could be useful for hypothesis testing and causal-effect
analysis. As the original unsupervised task becomes a supervised one,
optimization methods can be leveraged for model calibrations. Experiments on
real data from the national city network of China demonstrated that the
proposed model outperforms several widely used algorithms.
",5.0
323,4,121212,2006.05113,pre-trained language model,"Lukas Muttenthaler, Nora Hollenstein, Maria Barrett",Human brain activity for machine attention,"  Cognitively inspired NLP leverages human-derived data to teach machines about
language processing mechanisms. Recently, neural networks have been augmented
with behavioral data to solve a range of NLP tasks spanning syntax and
semantics. We are the first to exploit neuroscientific data, namely
electroencephalography (EEG), to inform a neural attention model about language
processing of the human brain. The challenge in working with EEG data is that
features are exceptionally rich and need extensive pre-processing to isolate
signals specific to text processing. We devise a method for finding such EEG
features to supervise machine attention through combining theoretically
motivated cropping with random forest tree splits. After this dimensionality
reduction, the pre-processed EEG features are capable of distinguishing two
reading tasks retrieved from a publicly available EEG corpus. We apply these
features to regularise attention on relation classification and show that EEG
is more informative than strong baselines. This improvement depends on both the
cognitive load of the task and the EEG frequency domain. Hence, informing
neural attention models with EEG signals is beneficial but requires further
investigation to understand which dimensions are the most useful across NLP
tasks.
",4.0
324,4,146682,2102.06282,pre-trained language model,"Mads Toftrup, S{\o}ren Asger S{\o}rensen, Manuel R. Ciosici, Ira
  Assent","A reproduction of Apple's bi-directional LSTM models for language
  identification in short strings","  Language Identification is the task of identifying a document's language. For
applications like automatic spell checker selection, language identification
must use very short strings such as text message fragments. In this work, we
reproduce a language identification architecture that Apple briefly sketched in
a blog post. We confirm the bi-LSTM model's performance and find that it
outperforms current open-source language identifiers. We further find that its
language identification mistakes are due to confusion between related
languages.
",5.0
325,1,21269,1307.498,advanced search engine,Bowei Chen and Jun Wang and Ingemar J. Cox and Mohan S. Kankanhalli,"Multi-keyword multi-click advertisement option contracts for sponsored
  search","  In sponsored search, advertisement (abbreviated ad) slots are usually sold by
a search engine to an advertiser through an auction mechanism in which
advertisers bid on keywords. In theory, auction mechanisms have many desirable
economic properties. However, keyword auctions have a number of limitations
including: the uncertainty in payment prices for advertisers; the volatility in
the search engine's revenue; and the weak loyalty between advertiser and search
engine. In this paper we propose a special ad option that alleviates these
problems. In our proposal, an advertiser can purchase an option from a search
engine in advance by paying an upfront fee, known as the option price. He then
has the right, but no obligation, to purchase among the pre-specified set of
keywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks
in a specified period of time. The proposed option is closely related to a
special exotic option in finance that contains multiple underlying assets
(multi-keyword) and is also multi-exercisable (multi-click). This novel
structure has many benefits: advertisers can have reduced uncertainty in
advertising; the search engine can improve the advertisers' loyalty as well as
obtain a stable and increased expected revenue over time. Since the proposed ad
option can be implemented in conjunction with the existing keyword auctions,
the option price and corresponding fixed CPCs must be set such that there is no
arbitrage between the two markets. Option pricing methods are discussed and our
experimental results validate the development. Compared to keyword auctions, a
search engine can have an increased expected revenue by selling an ad option.
",5.0
326,8,89074,1905.10227,node embedding for graph,"Thorben Funke, Tian Guo, Alen Lancic, Nino Antulov-Fantulin",Low-dimensional statistical manifold embedding of directed graphs,"  We propose a novel node embedding of directed graphs to statistical
manifolds, which is based on a global minimization of pairwise relative entropy
and graph geodesics in a non-linear way. Each node is encoded with a
probability density function over a measurable space. Furthermore, we analyze
the connection between the geometrical properties of such embedding and their
efficient learning procedure. Extensive experiments show that our proposed
embedding is better in preserving the global geodesic information of graphs, as
well as outperforming existing embedding models on directed graphs in a variety
of evaluation metrics, in an unsupervised setting.
",5.0
327,9,56812,1708.06555,language model for long documents,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",Long-Short Range Context Neural Networks for Language Modeling,"  The goal of language modeling techniques is to capture the statistical and
structural properties of natural languages from training corpora. This task
typically involves the learning of short range dependencies, which generally
model the syntactic properties of a language and/or long range dependencies,
which are semantic in nature. We propose in this paper a new multi-span
architecture, which separately models the short and long context information
while it dynamically merges them to perform the language modeling task. This is
done through a novel recurrent Long-Short Range Context (LSRC) network, which
explicitly models the local (short) and global (long) context using two
separate hidden states that evolve in time. This new architecture is an
adaptation of the Long-Short Term Memory network (LSTM) to take into account
the linguistic properties. Extensive experiments conducted on the Penn Treebank
(PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a
significant reduction of the perplexity when compared to state-of-the-art
language modeling techniques.
",4.0
328,14,87624,1905.01984,text summarization model,"Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang, Shaoyang Hao, Zhiwen Yu","AI-Powered Text Generation for Harmonious Human-Machine Interaction:
  Current State and Future Directions","  In the last two decades, the landscape of text generation has undergone
tremendous changes and is being reshaped by the success of deep learning. New
technologies for text generation ranging from template-based methods to neural
network-based methods emerged. Meanwhile, the research objectives have also
changed from generating smooth and coherent sentences to infusing personalized
traits to enrich the diversification of newly generated content. With the rapid
development of text generation solutions, one comprehensive survey is urgent to
summarize the achievements and track the state of the arts. In this survey
paper, we present the general systematical framework, illustrate the widely
utilized models and summarize the classic applications of text generation.
",3.0
329,7,66977,1805.00811,gradient boosting,Victoria J. Hodge and Jim Austin,An Evaluation of Classification and Outlier Detection Algorithms,"  This paper evaluates algorithms for classification and outlier detection
accuracies in temporal data. We focus on algorithms that train and classify
rapidly and can be used for systems that need to incorporate new data
regularly. Hence, we compare the accuracy of six fast algorithms using a range
of well-known time-series datasets. The analyses demonstrate that the choice of
algorithm is task and data specific but that we can derive heuristics for
choosing. Gradient Boosting Machines are generally best for classification but
there is no single winner for outlier detection though Gradient Boosting
Machines (again) and Random Forest are better. Hence, we recommend running
evaluations of a number of algorithms using our heuristics.
",3.0
330,14,172646,2110.06388,text summarization model,"Ye Liu, Jian-Guo Zhang, Yao Wan, Congying Xia, Lifang He, Philip S. Yu","HETFORMER: Heterogeneous Transformer with Sparse Attention for Long-Text
  Extractive Summarization","  To capture the semantic graph structure from raw text, most existing
summarization approaches are built on GNNs with a pre-trained model. However,
these methods suffer from cumbersome procedures and inefficient computations
for long-text documents. To mitigate these issues, this paper proposes
HETFORMER, a Transformer-based pre-trained model with multi-granularity sparse
attentions for long-text extractive summarization. Specifically, we model
different types of semantic nodes in raw text as a potential heterogeneous
graph and directly learn heterogeneous relationships (edges) among nodes by
Transformer. Extensive experiments on both single- and multi-document
summarization tasks show that HETFORMER achieves state-of-the-art performance
in Rouge F1 while using less memory and fewer parameters.
",5.0
331,14,214783,2210.136,text summarization model,"Abdulaziz Alhamadani, Xuchao Zhang, Jianfeng He, Chang-Tien Lu",LANS: Large-scale Arabic News Summarization Corpus,"  Text summarization has been intensively studied in many languages, and some
languages have reached advanced stages. Yet, Arabic Text Summarization (ATS) is
still in its developing stages. Existing ATS datasets are either small or lack
diversity. We build, LANS, a large-scale and diverse dataset for Arabic Text
Summarization task. LANS offers 8.4 million articles and their summaries
extracted from newspapers websites metadata between 1999 and 2019. The
high-quality and diverse summaries are written by journalists from 22 major
Arab newspapers, and include an eclectic mix of at least more than 7 topics
from each source. We conduct an intrinsic evaluation on LANS by both automatic
and human evaluations. Human evaluation of 1000 random samples reports 95.4%
accuracy for our collected summaries, and automatic evaluation quantifies the
diversity and abstractness of the summaries. The dataset is publicly available
upon request.
",2.0
332,9,195166,2205.04275,language model for long documents,"Luyu Gao, Jamie Callan",Long Document Re-ranking with Modular Re-ranker,"  Long document re-ranking has been a challenging problem for neural re-rankers
based on deep language models like BERT. Early work breaks the documents into
short passage-like chunks. These chunks are independently mapped to scalar
scores or latent vectors, which are then pooled into a final relevance score.
These encode-and-pool methods however inevitably introduce an information
bottleneck: the low dimension representations. In this paper, we propose
instead to model full query-to-document interaction, leveraging the attention
operation and modular Transformer re-ranker framework. First, document chunks
are encoded independently with an encoder module. An interaction module then
encodes the query and performs joint attention from the query to all document
chunk representations. We demonstrate that the model can use this new degree of
freedom to aggregate important information from the entire document. Our
experiments show that this design produces effective re-ranking on two
classical IR collections Robust04 and ClueWeb09, and a large-scale supervised
collection MS-MARCO document ranking.
",2.0
333,19,218140,cs/0605024,artificial intelligence for low carbon,Shane Legg and Marcus Hutter,A Formal Measure of Machine Intelligence,"  A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this measure formally captures the concept of machine intelligence in the
broadest reasonable sense.
",1.0
334,13,2968,904.3701,social network analysis with natrual language processing,"Guillaume Er\'et\'eo (INRIA Sophia Antipolis), Fabien Gandon (INRIA
  Sophia Antipolis), Olivier Corby (INRIA Sophia Antipolis), Michel Buffa",Semantic Social Network Analysis,"  Social Network Analysis (SNA) tries to understand and exploit the key
features of social networks in order to manage their life cycle and predict
their evolution. Increasingly popular web 2.0 sites are forming huge social
network. Classical methods from social network analysis (SNA) have been applied
to such online networks. In this paper, we propose leveraging semantic web
technologies to merge and exploit the best features of each domain. We present
how to facilitate and enhance the analysis of online social networks,
exploiting the power of semantic social network analysis.
",2.0
335,15,31627,1502.03215,relevance feedback for imformation retrieval,"Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar and Pratyaydipta
  Rudra","A Hybrid Approach for Improved Content-based Image Retrieval using
  Segmentation","  The objective of Content-Based Image Retrieval (CBIR) methods is essentially
to extract, from large (image) databases, a specified number of images similar
in visual and semantic content to a so-called query image. To bridge the
semantic gap that exists between the representation of an image by low-level
features (namely, colour, shape, texture) and its high-level semantic content
as perceived by humans, CBIR systems typically make use of the relevance
feedback (RF) mechanism. RF iteratively incorporates user-given inputs
regarding the relevance of retrieved images, to improve retrieval efficiency.
One approach is to vary the weights of the features dynamically via feature
reweighting. In this work, an attempt has been made to improve retrieval
accuracy by enhancing a CBIR system based on color features alone, through
implicit incorporation of shape information obtained through prior segmentation
of the images. Novel schemes for feature reweighting as well as for
initialization of the relevant set for improved relevance feedback, have also
been proposed for boosting performance of RF- based CBIR. At the same time, new
measures for evaluation of retrieval accuracy have been suggested, to overcome
the limitations of existing measures in the RF context. Results of extensive
experiments have been presented to illustrate the effectiveness of the proposed
approaches.
",3.0
336,12,61007,1712.041,COVID-19 and social media,"D. Asher, J. Caylor, M. Mittrick, J. Richardson, E. Heilman, E.
  Bowman, G. Korniss, B. Szymanski",The Investigation of Social Media Data Thresholds for Opinion Formation,"  The pervasive use of social media has grown to over two billion users to
date, and is commonly utilized as a means to share information and shape world
events. Evidence suggests that passive social media usage (i.e., viewing
without taking action) has an impact on the user's perspective. This empirical
influence over perspective could have significant impact on social events.
Therefore, it is important to understand how social media contributes to the
formation of an individual's perspective. A set of experimental tasks were
designed to investigate empirically derived thresholds for opinion formation as
a result of passive interactions with different social media data types (i.e.,
videos, images, and messages). With a better understanding of how humans
passively interact with social media information, a paradigm can be developed
that allows the exploitation of this interaction and plays a significant role
in future military plans and operations.
",1.0
337,5,20533,1306.0626,matrix completion,Prateek Jain and Inderjit S. Dhillon,Provable Inductive Matrix Completion,"  Consider a movie recommendation system where apart from the ratings
information, side information such as user's age or movie's genre is also
available. Unlike standard matrix completion, in this setting one should be
able to predict inductively on new users/movies. In this paper, we study the
problem of inductive matrix completion in the exact recovery setting. That is,
we assume that the ratings matrix is generated by applying feature vectors to a
low-rank matrix and the goal is to recover back the underlying matrix.
Furthermore, we generalize the problem to that of low-rank matrix estimation
using rank-1 measurements. We study this generic problem and provide conditions
that the set of measurements should satisfy so that the alternating
minimization method (which otherwise is a non-convex method with no convergence
guarantees) is able to recover back the {\em exact} underlying low-rank matrix.
  In addition to inductive matrix completion, we show that two other low-rank
estimation problems can be studied in our framework: a) general low-rank matrix
sensing using rank-1 measurements, and b) multi-label regression with missing
labels. For both the problems, we provide novel and interesting bounds on the
number of measurements required by alternating minimization to provably
converges to the {\em exact} low-rank matrix. In particular, our analysis for
the general low rank matrix sensing problem significantly improves the required
storage and computational cost than that required by the RIP-based matrix
sensing methods \cite{RechtFP2007}. Finally, we provide empirical validation of
our approach and demonstrate that alternating minimization is able to recover
the true matrix for the above mentioned problems using a small number of
measurements.
",4.0
338,15,188827,2203.0542,relevance feedback for imformation retrieval,"Timo Breuer, Melanie Pest, Philipp Schaer","Evaluating Elements of Web-based Data Enrichment for Pseudo-Relevance
  Feedback Retrieval","  In this work, we analyze a pseudo-relevance retrieval method based on the
results of web search engines. By enriching topics with text data from web
search engine result pages and linked contents, we train topic-specific and
cost-efficient classifiers that can be used to search test collections for
relevant documents. Building upon attempts initially made at TREC Common Core
2018 by Grossman and Cormack, we address questions of system performance over
time considering different search engines, queries, and test collections. Our
experimental results show how and to which extent the considered components
affect the retrieval performance. Overall, the analyzed method is robust in
terms of average retrieval performance and a promising way to use web content
for the data enrichment of relevance feedback methods.
",5.0
339,1,199853,2206.06588,advanced search engine,"Chandan K. Reddy, Llu\'is M\`arquez, Fran Valero, Nikhil Rao, Hugo
  Zaragoza, Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, Karthik Subbian","Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving
  Product Search","  Improving the quality of search results can significantly enhance users
experience and engagement with search engines. In spite of several recent
advancements in the fields of machine learning and data mining, correctly
classifying items for a particular user search query has been a long-standing
challenge, which still has a large room for improvement. This paper introduces
the ""Shopping Queries Dataset"", a large dataset of difficult Amazon search
queries and results, publicly released with the aim of fostering research in
improving the quality of search results. The dataset contains around 130
thousand unique queries and 2.6 million manually labeled (query,product)
relevance judgements. The dataset is multilingual with queries in English,
Japanese, and Spanish. The Shopping Queries Dataset is being used in one of the
KDDCup'22 challenges. In this paper, we describe the dataset and present three
evaluation tasks along with baseline results: (i) ranking the results list,
(ii) classifying product results into relevance categories, and (iii)
identifying substitute products for a given query. We anticipate that this data
will become the gold standard for future research in the topic of product
search.
",2.0
340,18,36717,1509.0896,infomation retrieval time complexity,Udayan Khurana and Amol Deshpande,Storing and Analyzing Historical Graph Data at Scale,"  The work on large-scale graph analytics to date has largely focused on the
study of static properties of graph snapshots. However, a static view of
interactions between entities is often an oversimplification of several complex
phenomena like the spread of epidemics, information diffusion, formation of
online communities}, and so on. Being able to find temporal interaction
patterns, visualize the evolution of graph properties, or even simply compare
them across time, adds significant value in reasoning over graphs. However,
because of lack of underlying data management support, an analyst today has to
manually navigate the added temporal complexity of dealing with large evolving
graphs. In this paper, we present a system, called Historical Graph Store, that
enables users to store large volumes of historical graph data and to express
and run complex temporal graph analytical tasks against that data. It consists
of two key components: a Temporal Graph Index (TGI), that compactly stores
large volumes of historical graph evolution data in a partitioned and
distributed fashion; it provides support for retrieving snapshots of the graph
as of any timepoint in the past or evolution histories of individual nodes or
neighborhoods; and a Spark-based Temporal Graph Analysis Framework (TAF), for
expressing complex temporal analytical tasks and for executing them in an
efficient and scalable manner. Our experiments demonstrate our system's
efficient storage, retrieval and analytics across a wide variety of queries on
large volumes of historical graph data.
",2.0
341,0,191536,2204.015,learning to rank with partitioned preference,"Ivan Lyzhin, Aleksei Ustimenko, Andrey Gulin, Liudmila Prokhorenkova",Which Tricks are Important for Learning to Rank?,"  Nowadays, state-of-the-art learning-to-rank (LTR) methods are based on
gradient-boosted decision trees (GBDT). The most well-known algorithm is
LambdaMART that was proposed more than a decade ago. Recently, several other
GBDT-based ranking algorithms were proposed. In this paper, we conduct a
thorough analysis of these methods in a unified setup. In particular, we
address the following questions. Is direct optimization of a smoothed ranking
loss preferable over optimizing a convex surrogate? How to properly construct
and smooth surrogate ranking losses? To address these questions, we compare
LambdaMART with YetiRank and StochasticRank methods and their modifications. We
also improve the YetiRank approach to allow for optimizing specific ranking
loss functions. As a result, we gain insights into learning-to-rank approaches
and obtain a new state-of-the-art algorithm.
",1.0
342,7,112416,2003.02122,gradient boosting,"Aleksei Ustimenko, Liudmila Prokhorenkova",StochasticRank: Global Optimization of Scale-Free Discrete Functions,"  In this paper, we introduce a powerful and efficient framework for direct
optimization of ranking metrics. The problem is ill-posed due to the discrete
structure of the loss, and to deal with that, we introduce two important
techniques: stochastic smoothing and novel gradient estimate based on partial
integration. We show that classic smoothing approaches may introduce bias and
present a universal solution for a proper debiasing. Importantly, we can
guarantee global convergence of our method by adopting a recently proposed
Stochastic Gradient Langevin Boosting algorithm. Our algorithm is implemented
as a part of the CatBoost gradient boosting library and outperforms the
existing approaches on several learning-to-rank datasets. In addition to
ranking metrics, our framework applies to any scale-free discrete loss
function.
",2.0
343,18,2807,903.4132,infomation retrieval time complexity,"Joaqu\'in Go\~ni, I\~nigo Martincorena, Bernat Corominas-Murtra,
  Gonzalo Arrondo, Sergio Ardanza-Trevijano, Pablo Villoslada","Switcher-random-walks: a cognitive-inspired mechanism for network
  exploration","  Semantic memory is the subsystem of human memory that stores knowledge of
concepts or meanings, as opposed to life specific experiences. The organization
of concepts within semantic memory can be understood as a semantic network,
where the concepts (nodes) are associated (linked) to others depending on
perceptions, similarities, etc. Lexical access is the complementary part of
this system and allows the retrieval of such organized knowledge. While
conceptual information is stored under certain underlying organization (and
thus gives rise to a specific topology), it is crucial to have an accurate
access to any of the information units, e.g. the concepts, for efficiently
retrieving semantic information for real-time needings. An example of an
information retrieval process occurs in verbal fluency tasks, and it is known
to involve two different mechanisms: -clustering-, or generating words within a
subcategory, and, when a subcategory is exhausted, -switching- to a new
subcategory. We extended this approach to random-walking on a network
(clustering) in combination to jumping (switching) to any node with certain
probability and derived its analytical expression based on Markov chains.
Results show that this dual mechanism contributes to optimize the exploration
of different network models in terms of the mean first passage time.
Additionally, this cognitive inspired dual mechanism opens a new framework to
better understand and evaluate exploration, propagation and transport phenomena
in other complex systems where switching-like phenomena are feasible.
",2.0
344,1,131519,2009.08958,advanced search engine,Olegs Verhodubs,Keyword Search Engine Enriched by Expert System Features,"  Keyword search engines are essential elements of large information spaces.
The largest information space is the Web, and keyword search engines play
crucial role there. The advent of keyword search engines has provided a quantum
leap in the development of the Web. Since then, the Web has continued to
evolve, and keyword search systems have proven inadequate. A new quantum leap
in the development of keyword search engines is needed. This quantum leap can
be provided with more intellectual keyword search engines. The increased
intelligence of such keyword search engines can be achieved through a
combination of keyword search engines and expert systems. The paper reveals how
it can be done.
",5.0
345,8,73483,1809.0411,node embedding for graph,"Lichao Sun, Lifang He, Zhipeng Huang, Bokai Cao, Congying Xia, Xiaokai
  Wei and Philip S. Yu","Joint Embedding of Meta-Path and Meta-Graph for Heterogeneous
  Information Networks","  Meta-graph is currently the most powerful tool for similarity search on
heterogeneous information networks,where a meta-graph is a composition of
meta-paths that captures the complex structural information. However, current
relevance computing based on meta-graph only considers the complex structural
information, but ignores its embedded meta-paths information. To address this
problem, we proposeMEta-GrAph-based network embedding models, called MEGA and
MEGA++, respectively. The MEGA model uses normalized relevance or similarity
measures that are derived from a meta-graph and its embedded meta-paths between
nodes simultaneously, and then leverages tensor decomposition method to perform
node embedding. The MEGA++ further facilitates the use of coupled tensor-matrix
decomposition method to obtain a joint embedding for nodes, which
simultaneously considers the hidden relations of all meta information of a
meta-graph.Extensive experiments on two real datasets demonstrate thatMEGA and
MEGA++ are more effective than state-of-the-art approaches.
",4.0
346,11,144157,2101.07371,PageRank for web search,"Liang Lyu, Brandon Fain, Kamesh Munagala and Kangning Wang",Centrality with Diversity,"  Graph centrality measures use the structure of a network to quantify central
or ""important"" nodes, with applications in web search, social media analysis,
and graphical data mining generally. Traditional centrality measures such as
the well known PageRank interpret a directed edge as a vote in favor of the
importance of the linked node. We study the case where nodes may belong to
diverse communities or interests and investigate centrality measures that can
identify nodes that are simultaneously important to many such diverse
communities. We propose a family of diverse centrality measures formed as fixed
point solutions to a generalized nonlinear eigenvalue problem. Our measure can
be efficiently computed on large graphs by iterated best response and we study
its normative properties on both random graph models and real-world data. We
find that we are consistently and efficiently able to identify the most
important diverse nodes of a graph, that is, those that are simultaneously
central to multiple communities.
",2.0
347,13,42025,1604.08781,social network analysis with natrual language processing,Joseph Corneli and Miriam Corneli,Teaching natural language to computers,"  ""Natural Language,"" whether spoken and attended to by humans, or processed
and generated by computers, requires networked structures that reflect creative
processes in semantic, syntactic, phonetic, linguistic, social, emotional, and
cultural modules. Being able to produce novel and useful behavior following
repeated practice gets to the root of both artificial intelligence and human
language. This paper investigates the modalities involved in language-like
applications that computers -- and programmers -- engage with, and aims to fine
tune the questions we ask to better account for context, self-awareness, and
embodiment.
",2.0
348,4,146265,2102.04506,pre-trained language model,"Boliang Zhang, Ying Lyu, Ning Ding, Tianhao Shen, Zhaoyang Jia, Kun
  Han, Kevin Knight","A Hybrid Task-Oriented Dialog System with Domain and Task Adaptive
  Pretraining","  This paper describes our submission for the End-to-end Multi-domain Task
Completion Dialog shared task at the 9th Dialog System Technology Challenge
(DSTC-9). Participants in the shared task build an end-to-end task completion
dialog system which is evaluated by human evaluation and a user simulator based
automatic evaluation. Different from traditional pipelined approaches where
modules are optimized individually and suffer from cascading failure, we
propose an end-to-end dialog system that 1) uses Generative Pretraining 2
(GPT-2) as the backbone to jointly solve Natural Language Understanding, Dialog
State Tracking, and Natural Language Generation tasks, 2) adopts Domain and
Task Adaptive Pretraining to tailor GPT-2 to the dialog domain before
finetuning, 3) utilizes heuristic pre/post-processing rules that greatly
simplify the prediction tasks and improve generalizability, and 4) equips a
fault tolerance module to correct errors and inappropriate responses. Our
proposed method significantly outperforms baselines and ties for first place in
the official evaluation. We make our source code publicly available.
",5.0
349,2,45294,1608.0771,random forests,Yangming Zhou and Guoping Qiu,Random Forest for Label Ranking,"  Label ranking aims to learn a mapping from instances to rankings over a
finite number of predefined labels. Random forest is a powerful and one of the
most successful general-purpose machine learning algorithms of modern times. In
this paper, we present a powerful random forest label ranking method which uses
random decision trees to retrieve nearest neighbors. We have developed a novel
two-step rank aggregation strategy to effectively aggregate neighboring
rankings discovered by the random forest into a final predicted ranking.
Compared with existing methods, the new random forest method has many
advantages including its intrinsically scalable tree data structure, highly
parallel-able computational architecture and much superior performance. We
present extensive experimental results to demonstrate that our new method
achieves the highly competitive performance compared with state-of-the-art
methods for datasets with complete ranking and datasets with only partial
ranking information.
",4.0
350,6,96389,1908.10193,query expansion for imformation retrieval,"Hiteshwar Kumar Azad, Akshay Deepak",A novel model for query expansion using pseudo-relevant web knowledge,"  In the field of information retrieval, query expansion (QE) has long been
used as a technique to deal with the fundamental issue of word mismatch between
a user's query and the target information. In the context of the relationship
between the query and expanded terms, existing weighting techniques often fail
to appropriately capture the term-term relationship and term to the whole query
relationship, resulting in low retrieval effectiveness. Our proposed QE
approach addresses this by proposing three weighting models based on (1)
tf-itf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3)
correlation score. Further, to extract the initial set of expanded terms, we
use pseudo-relevant web knowledge consisting of the top N web pages returned by
the three popular search engines namely, Google, Bing, and DuckDuckGo, in
response to the original query. Among the three weighting models, tf-itf scores
each of the individual terms obtained from the web content, kNN-based cosine
similarity scores the expansion terms to obtain the term-term relationship, and
correlation score weighs the selected expansion terms with respect to the whole
query. The proposed model, called web knowledge based query expansion (WKQE),
achieves an improvement of 25.89% on the MAP score and 30.83% on the GMAP score
over the unexpanded queries on the FIRE dataset. A comparative analysis of the
WKQE techniques with other related approaches clearly shows significant
improvement in the retrieval performance. We have also analyzed the effect of
varying the number of pseudo-relevant documents and expansion terms on the
retrieval effectiveness of the proposed model.
",5.0
351,11,49780,1701.08256,PageRank for web search,"Nattiya Kanhabua, Philipp Kemkes, Wolfgang Nejdl, Tu Ngoc Nguyen,
  Felipe Reis, Nam Khanh Tran",How to Search the Internet Archive Without Indexing It,"  Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.
",2.0
352,14,130384,2009.02731,text summarization model,"Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang","Self-Supervised Contrastive Learning for Code Retrieval and
  Summarization via Semantic-Preserving Transformations","  We propose Corder, a self-supervised contrastive learning framework for
source code model. Corder is designed to alleviate the need of labeled data for
code retrieval and code summarization tasks. The pre-trained model of Corder
can be used in two ways: (1) it can produce vector representation of code which
can be applied to code retrieval tasks that do not have labeled data; (2) it
can be used in a fine-tuning process for tasks that might still require label
data such as code summarization. The key innovation is that we train the source
code model by asking it to recognize similar and dissimilar code snippets
through a contrastive learning objective. To do so, we use a set of
semantic-preserving transformation operators to generate code snippets that are
syntactically diverse but semantically equivalent. Through extensive
experiments, we have shown that the code models pretrained by Corder
substantially outperform the other baselines for code-to-code retrieval,
text-to-code retrieval, and code-to-text summarization tasks.
",3.0
353,10,51808,1703.09343,web archive,Martin Klein and Herbert Van de Sompel,Discovering Scholarly Orphans Using ORCID,"  Archival efforts such as (C)LOCKSS and Portico are in place to ensure the
longevity of traditional scholarly resources like journal articles. At the same
time, researchers are depositing a broad variety of other scholarly artifacts
into emerging online portals that are designed to support web-based
scholarship. These web-native scholarly objects are largely neglected by
current archival practices and hence they become scholarly orphans. We
therefore argue for a novel paradigm that is tailored towards archiving these
scholarly orphans. We are investigating the feasibility of using Open
Researcher and Contributor ID (ORCID) as a supporting infrastructure for the
process of discovery of web identities and scholarly orphans for active
researchers. We analyze ORCID in terms of coverage of researchers, subjects,
and location and assess the richness of its profiles in terms of web identities
and scholarly artifacts. We find that ORCID currently lacks in all considered
aspects and hence can only be considered in conjunction with other discovery
sources. However, ORCID is growing fast so there is potential that it could
achieve a satisfactory level of coverage and richness in the near future.
",2.0
354,4,81695,1901.11167,pre-trained language model,"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",A Generalized Language Model in Tensor Space,"  In the literature, tensors have been effectively used for capturing the
context information in language models. However, the existing methods usually
adopt relatively-low order tensors, which have limited expressive power in
modeling language. Developing a higher-order tensor representation is
challenging, in terms of deriving an effective solution and showing its
generality. In this paper, we propose a language model named Tensor Space
Language Model (TSLM), by utilizing tensor networks and tensor decomposition.
In TSLM, we build a high-dimensional semantic space constructed by the tensor
product of word vectors. Theoretically, we prove that such tensor
representation is a generalization of the n-gram language model. We further
show that this high-order tensor representation can be decomposed to a
recursive calculation of conditional probability for language modeling. The
experimental results on Penn Tree Bank (PTB) dataset and WikiText benchmark
demonstrate the effectiveness of TSLM.
",5.0
355,12,74657,1810.01501,COVID-19 and social media,"Derrik E. Asher, Justine Caylor, Casey Doyle, Alexis R. Neigel, Gyorgy
  Korniss, Boleslaw K. Szymanski","Opinion Formation Threshold Estimates from Different Combinations of
  Social Media Data-Types","  Passive consumption of a quantifiable amount of social media information
related to a topic can cause individuals to form opinions. If a substantial
amount of these individuals are motivated to take action from their recently
established opinions, a movement or public opinion shift can be induced
independent of the information's veracity. Given that social media is
ubiquitous in modern society, it is imperative that we understand the threshold
at which social media data results in opinion formation. The present study
estimates population opinion formation thresholds by querying 2222 participants
about the number of various social media data-types (i.e., images, videos,
and/or messages) that they would need to passively consume to form opinions.
Opinion formation is assessed across three dimensions, 1) data-type(s), 2)
context, and 3) source. This work provides a theoretical basis for estimating
the amount of data needed to influence a population through social media
information.
",1.0
356,19,204241,2207.1033,artificial intelligence for low carbon,"Ga\""etan Serr\'e (TAU, Inria, LISN), Eva Boguslawski (RTE, TAU, LISN,
  Inria), Benjamin Donnot (RTE), Adrien Pav\~ao (TAU, LISN, Inria), Isabelle
  Guyon (TAU, LISN, Inria), Antoine Marot (RTE)","Reinforcement learning for Energies of the future and carbon neutrality:
  a Challenge Design","  Current rapid changes in climate increase the urgency to change energy
production and consumption management, to reduce carbon and other green-house
gas production. In this context, the French electricity network management
company RTE (R{\'e}seau de Transport d'{\'E}lectricit{\'e}) has recently
published the results of an extensive study outlining various scenarios for
tomorrow's French power management. We propose a challenge that will test the
viability of such a scenario. The goal is to control electricity transportation
in power networks, while pursuing multiple objectives: balancing production and
consumption, minimizing energetic losses, and keeping people and equipment safe
and particularly avoiding catastrophic failures. While the importance of the
application provides a goal in itself, this challenge also aims to push the
state-of-the-art in a branch of Artificial Intelligence (AI) called
Reinforcement Learning (RL), which offers new possibilities to tackle control
problems. In particular, various aspects of the combination of Deep Learning
and RL called Deep Reinforcement Learning remain to be harnessed in this
application domain. This challenge belongs to a series started in 2019 under
the name ""Learning to run a power network"" (L2RPN). In this new edition, we
introduce new more realistic scenarios proposed by RTE to reach carbon
neutrality by 2050, retiring fossil fuel electricity production, increasing
proportions of renewable and nuclear energy and introducing batteries.
Furthermore, we provide a baseline using state-of-the-art reinforcement
learning algorithm to stimulate the future participants.
",5.0
357,10,210501,2209.12299,web archive,Niklas Deckers and Martin Potthast,WARC-DL: Scalable Web Archive Processing for Deep Learning,"  Web archives have grown to petabytes. In addition to providing invaluable
background knowledge on many social and cultural developments over the last 30
years, they also provide vast amounts of training data for machine learning. To
benefit from recent developments in Deep Learning, the use of web archives
requires a scalable solution for their processing that supports inference with
and training of neural networks. To date, there is no publicly available
library for processing web archives in this way, and some existing applications
use workarounds. This paper presents WARC-DL, a deep learning-enabled pipeline
for web archive processing that scales to petabytes.
",5.0
358,13,15302,1209.4616,social network analysis with natrual language processing,Rumi Ghosh and Kristina Lerman,"Rethinking Centrality: The Role of Dynamical Processes in Social Network
  Analysis","  Many popular measures used in social network analysis, including centrality,
are based on the random walk. The random walk is a model of a stochastic
process where a node interacts with one other node at a time. However, the
random walk may not be appropriate for modeling social phenomena, including
epidemics and information diffusion, in which one node may interact with many
others at the same time, for example, by broadcasting the virus or information
to its neighbors. To produce meaningful results, social network analysis
algorithms have to take into account the nature of interactions between the
nodes. In this paper we classify dynamical processes as conservative and
non-conservative and relate them to well-known measures of centrality used in
network analysis: PageRank and Alpha-Centrality. We demonstrate, by ranking
users in online social networks used for broadcasting information, that
non-conservative Alpha-Centrality generally leads to a better agreement with an
empirical ranking scheme than the conservative PageRank.
",1.0
359,12,191436,2204.0091,COVID-19 and social media,"Kunihiro Miyazaki, Takayuki Uchiba, Haewoon Kwak, Jisun An","Characterizing Spontaneous Ideation Contest on Social Media: Case Study
  on the Name Change of Facebook to Meta","  Collecting good ideas is vital for organizations, especially companies, to
retain their competitiveness. Social media is gathering attention as a place to
extract ideas efficiently; however, the characteristics of ideas and the
posters of ideas on social media are underexamined. Thus, this study aims to
characterize spontaneous ideation contests among social media users by taking
an event of Facebook's name change to Meta as a case study. As a dataset, we
comprehensively collect tweets containing new acronyms of Big Tech companies,
which we treat as an ""idea"" in this work. In the analysis, we especially focus
on the diversity of ideas, which would be the main reason for enlisting social
media for idea generation. As the main results, we discovered that social media
users offered a wider range of ideas than those in mainstream media. The
follow-follower network of the users suggested that the users' position on the
network is related to the preferred ideas. Additionally, we discovered a link
between the amount of user interaction on social media and the diversity of
ideas. This study would promote the use of social media as a part of open
innovation and co-creation processes in the industry.
",1.0
360,7,92284,1906.10991,gradient boosting,"Gil Einziger, Maayan Goldstein, Yaniv Sa'ar, Itai Segall",Verifying Robustness of Gradient Boosted Models,"  Gradient boosted models are a fundamental machine learning technique.
Robustness to small perturbations of the input is an important quality measure
for machine learning models, but the literature lacks a method to prove the
robustness of gradient boosted models. This work introduces VeriGB, a tool for
quantifying the robustness of gradient boosted models. VeriGB encodes the model
and the robustness property as an SMT formula, which enables state of the art
verification tools to prove the model's robustness. We extensively evaluate
VeriGB on publicly available datasets and demonstrate a capability for
verifying large models. Finally, we show that some model configurations tend to
be inherently more robust than others.
",4.0
361,11,3136,905.4162,PageRank for web search,"D.L. Shepelyansky and O.V. Zhirov (CNRS, Toulouse & BINP, Novosibirsk)","Google matrix, dynamical attractors and Ulam networks","  We study the properties of the Google matrix generated by a coarse-grained
Perron-Frobenius operator of the Chirikov typical map with dissipation. The
finite size matrix approximant of this operator is constructed by the Ulam
method. This method applied to the simple dynamical model creates the directed
Ulam networks with approximate scale-free scaling and characteristics being
rather similar to those of the World Wide Web. The simple dynamical attractors
play here the role of popular web sites with a strong concentration of
PageRank. A variation of the Google parameter $\alpha$ or other parameters of
the dynamical map can drive the PageRank of the Google matrix to a delocalized
phase with a strange attractor where the Google search becomes inefficient.
",3.0
362,14,158252,2106.00619,text summarization model,Mohd Khizir Siddiqui and Amreen Ahmad and Om Pal and Tanvir Ahmad,"CoRank: A clustering cum graph ranking approach for extractive
  summarization","  Online information has increased tremendously in today's age of Internet. As
a result, the need has arose to extract relevant content from the plethora of
available information. Researchers are widely using automatic text
summarization techniques for extracting useful and relevant information from
voluminous available information, it also enables users to obtain valuable
knowledge in a limited period of time with minimal effort. The summary obtained
from the automatic text summarization often faces the issues of diversity and
information coverage. Promising results are obtained for automatic text
summarization by the introduction of new techniques based on graph ranking of
sentences, clustering, and optimization. This research work proposes CoRank, a
two-stage sentence selection model involving clustering and then ranking of
sentences. The initial stage involves clustering of sentences using a novel
clustering algorithm, and later selection of salient sentences using CoRank
algorithm. The approach aims to cover two objectives: maximum coverage and
diversity, which is achieved by the extraction of main topics and sub-topics
from the original text. The performance of the CoRank is validated on DUC2001
and DUC 2002 data sets.
",5.0
363,17,177019,2111.09779,robustness of neutral networks,"Sadaf Gulshad, Ivan Sosnovik, Arnold Smeulders",Wiggling Weights to Improve the Robustness of Classifiers,"  Robustness against unwanted perturbations is an important aspect of deploying
neural network classifiers in the real world. Common natural perturbations
include noise, saturation, occlusion, viewpoint changes, and blur deformations.
All of them can be modelled by the newly proposed transform-augmented
convolutional networks. While many approaches for robustness train the network
by providing augmented data to the network, we aim to integrate perturbations
in the network architecture to achieve improved and more general robustness. To
demonstrate that wiggling the weights consistently improves classification, we
choose a standard network and modify it to a transform-augmented network. On
perturbed CIFAR-10 images, the modified network delivers a better performance
than the original network. For the much smaller STL-10 dataset, in addition to
delivering better general robustness, wiggling even improves the classification
of unperturbed, clean images substantially. We conclude that wiggled
transform-augmented networks acquire good robustness even for perturbations not
seen during training.
",5.0
364,18,28056,1408.0034,infomation retrieval time complexity,"Ramtin Pedarsani, Dong Yin, Kangwook Lee, Kannan Ramchandran","PhaseCode: Fast and Efficient Compressive Phase Retrieval based on
  Sparse-Graph-Codes","  We consider the problem of recovering a $K$-sparse complex signal $x$ from
$m$ intensity measurements. We propose the PhaseCode algorithm, and show that
in the noiseless case, PhaseCode can recover an arbitrarily-close-to-one
fraction of the $K$ non-zero signal components using only slightly more than
$4K$ measurements when the support of the signal is uniformly random, with
order-optimal time and memory complexity of $\Theta(K)$. It is known that the
fundamental limit for the number of measurements in compressive phase retrieval
problem is $4K - o(K)$ to recover the signal exactly and with no assumptions on
its support distribution. This shows that under mild relaxation of the
conditions, our algorithm is the first constructive \emph{capacity-approaching}
compressive phase retrieval algorithm: in fact, our algorithm is also
order-optimal in complexity and memory. Next, motivated by some important
practical classes of optical systems, we consider a Fourier-friendly
constrained measurement setting, and show that its performance matches that of
the unconstrained setting. In the Fourier-friendly setting that we consider,
the measurement matrix is constrained to be a cascade of Fourier matrices and
diagonal matrices. We further demonstrate how PhaseCode can be robustified to
noise. Throughout, we provide extensive simulation results that validate the
practical power of our proposed algorithms for the sparse unconstrained and
Fourier-friendly measurement settings, for noiseless and noisy scenarios. A key
contribution of our work is the novel use of coding-theoretic tools like
density evolution methods for the design and analysis of fast and efficient
algorithms for compressive phase-retrieval problems.
",0.0
365,15,32912,1504.02356,relevance feedback for imformation retrieval,"Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Gir\'o-i-Nieto,
  Graham Healy, Kevin McGuinness, Noel O'Connor and Alan F. Smeaton",Exploring EEG for Object Detection and Retrieval,"  This paper explores the potential for using Brain Computer Interfaces (BCI)
as a relevance feedback mechanism in content-based image retrieval. We
investigate if it is possible to capture useful EEG signals to detect if
relevant objects are present in a dataset of realistic and complex images. We
perform several experiments using a rapid serial visual presentation (RSVP) of
images at different rates (5Hz and 10Hz) on 8 users with different degrees of
familiarization with BCI and the dataset. We then use the feedback from the BCI
and mouse-based interfaces to retrieve localized objects in a subset of TRECVid
images. We show that it is indeed possible to detect such objects in complex
images and, also, that users with previous knowledge on the dataset or
experience with the RSVP outperform others. When the users have limited time to
annotate the images (100 seconds in our experiments) both interfaces are
comparable in performance. Comparing our best users in a retrieval task, we
found that EEG-based relevance feedback outperforms mouse-based feedback. The
realistic and complex image dataset differentiates our work from previous
studies on EEG for image retrieval.
",5.0
366,18,197999,2205.15477,infomation retrieval time complexity,"Ala-Eddine Benrazek, Zineddine Kouahla, Brahim Farou, Hamid Seridi,
  Imane Allele","Introduction of a tree-based technique for efficient and real-time label
  retrieval in the object tracking system","  This paper addresses the issue of the real-time tracking quality of moving
objects in large-scale video surveillance systems. During the tracking process,
the system assigns an identifier or label to each tracked object to distinguish
it from other objects. In such a mission, it is essential to keep this
identifier for the same objects, whatever the area, the time of their
appearance, or the detecting camera. This is to conserve as much information
about the tracking object as possible, decrease the number of ID switching
(ID-Sw), and increase the quality of object tracking. To accomplish object
labeling, a massive amount of data collected by the cameras must be searched to
retrieve the most similar (nearest neighbor) object identifier. Although this
task is simple, it becomes very complex in large-scale video surveillance
networks, where the data becomes very large. In this case, the label retrieval
time increases significantly with this increase, which negatively affects the
performance of the real-time tracking system. To avoid such problems, we
propose a new solution to automatically label multiple objects for efficient
real-time tracking using the indexing mechanism. This mechanism organizes the
metadata of the objects extracted during the detection and tracking phase in an
Adaptive BCCF-tree. The main advantage of this structure is: its ability to
index massive metadata generated by multi-cameras, its logarithmic search
complexity, which implicitly reduces the search response time, and its quality
of research results, which ensure coherent labeling of the tracked objects. The
system load is distributed through a new Internet of Video Things
infrastructure-based architecture to improve data processing and real-time
object tracking performance. The experimental evaluation was conducted on a
publicly available dataset generated by multi-camera containing different crowd
activities.
",2.0
367,8,168524,2109.0356,node embedding for graph,"Baoyu Jing, Shengyu Feng, Yuejia Xiang, Xi Chen, Yu Chen and Hanghang
  Tong",X-GOAL: Multiplex Heterogeneous Graph Prototypical Contrastive Learning,"  Graphs are powerful representations for relations among objects, which have
attracted plenty of attention. A fundamental challenge for graph learning is
how to train an effective Graph Neural Network (GNN) encoder without labels,
which are expensive and time consuming to obtain. Contrastive Learning (CL) is
one of the most popular paradigms to address this challenge, which trains GNNs
by discriminating positive and negative node pairs. Despite the success of
recent CL methods, there are still two under-explored problems. First, how to
reduce the semantic error introduced by random topology based data
augmentations. Traditional CL defines positive and negative node pairs via the
node-level topological proximity, which is solely based on the graph topology
regardless of the semantic information of node attributes, and thus some
semantically similar nodes could be wrongly treated as negative pairs. Second,
how to effectively model the multiplexity of the real-world graphs, where nodes
are connected by various relations and each relation could form a homogeneous
graph layer. To solve these problems, we propose a novel multiplex
heterogeneous graph prototypical contrastive leaning (X-GOAL) framework to
extract node embeddings. X-GOAL is comprised of two components: the GOAL
framework, which learns node embeddings for each homogeneous graph layer, and
an alignment regularization, which jointly models different layers by aligning
layer-specific node embeddings. Specifically, the GOAL framework captures the
node-level information by a succinct graph transformation technique, and
captures the cluster-level information by pulling nodes within the same
semantic cluster closer in the embedding space. The alignment regularization
aligns embeddings across layers at both node and cluster levels. We evaluate
X-GOAL on various real-world datasets and downstream tasks to demonstrate its
effectiveness.
",5.0
368,14,116105,2004.08301,text summarization model,"Hiroki Kitano, Koujin Takeda","Belief Propagation for Maximum Coverage on Weighted Bipartite Graph and
  Application to Text Summarization","  We study text summarization from the viewpoint of maximum coverage problem.
In graph theory, the task of text summarization is regarded as maximum coverage
problem on bipartite graph with weighted nodes. In recent study,
belief-propagation based algorithm for maximum coverage on unweighted graph was
proposed using the idea of statistical mechanics. We generalize it to weighted
graph for text summarization. Then we apply our algorithm to weighted biregular
random graph for verification of maximum coverage performance. We also apply it
to bipartite graph representing real document in open text dataset, and check
the performance of text summarization. As a result, our algorithm exhibits
better performance than greedy-type algorithm in some setting of text
summarization.
",5.0
369,5,55198,1707.00622,matrix completion,"Morteza Ashraphijuo, Xiaodong Wang, Vaneet Aggarwal",Rank Determination for Low-Rank Data Completion,"  Recently, fundamental conditions on the sampling patterns have been obtained
for finite completability of low-rank matrices or tensors given the
corresponding ranks. In this paper, we consider the scenario where the rank is
not given and we aim to approximate the unknown rank based on the location of
sampled entries and some given completion. We consider a number of data models,
including single-view matrix, multi-view matrix, CP tensor, tensor-train tensor
and Tucker tensor. For each of these data models, we provide an upper bound on
the rank when an arbitrary low-rank completion is given. We characterize these
bounds both deterministically, i.e., with probability one given that the
sampling pattern satisfies certain combinatorial properties, and
probabilistically, i.e., with high probability given that the sampling
probability is above some threshold. Moreover, for both single-view matrix and
CP tensor, we are able to show that the obtained upper bound is exactly equal
to the unknown rank if the lowest-rank completion is given. Furthermore, we
provide numerical experiments for the case of single-view matrix, where we use
nuclear norm minimization to find a low-rank completion of the sampled data and
we observe that in most of the cases the proposed upper bound on the rank is
equal to the true rank.
",4.0
370,11,27831,1407.5107,PageRank for web search,David F. Gleich,PageRank beyond the Web,"  Google's PageRank method was developed to evaluate the importance of
web-pages via their link structure. The mathematics of PageRank, however, are
entirely general and apply to any graph or network in any domain. Thus,
PageRank is now regularly used in bibliometrics, social and information network
analysis, and for link prediction and recommendation. It's even used for
systems analysis of road networks, as well as biology, chemistry, neuroscience,
and physics. We'll see the mathematics and ideas that unite these diverse
applications.
",3.0
371,12,87292,1904.13355,COVID-19 and social media,"Kai Shu, Xinyi Zhou, Suhang Wang, Reza Zafarani, and Huan Liu",The Role of User Profile for Fake News Detection,"  Consuming news from social media is becoming increasingly popular. Social
media appeals to users due to its fast dissemination of information, low cost,
and easy access. However, social media also enables the widespread of fake
news. Because of the detrimental societal effects of fake news, detecting fake
news has attracted increasing attention. However, the detection performance
only using news contents is generally not satisfactory as fake news is written
to mimic true news. Thus, there is a need for an in-depth understanding on the
relationship between user profiles on social media and fake news. In this
paper, we study the challenging problem of understanding and exploiting user
profiles on social media for fake news detection. In an attempt to understand
connections between user profiles and fake news, first, we measure users'
sharing behaviors on social media and group representative users who are more
likely to share fake and real news; then, we perform a comparative analysis of
explicit and implicit profile features between these user groups, which reveals
their potential to help differentiate fake news from real news. To exploit user
profile features, we demonstrate the usefulness of these user profile features
in a fake news classification task. We further validate the effectiveness of
these features through feature importance analysis. The findings of this work
lay the foundation for deeper exploration of user profile features of social
media and enhance the capabilities for fake news detection.
",2.0
372,10,95212,1908.02819,web archive,"Lulwah M. Alkwai, Michael L. Nelson, Michele C. Weigle","Making Recommendations from Web Archives for ""Lost"" Web Pages","  When a user requests a web page from a web archive, the user will typically
either get an HTTP 200 if the page is available, or an HTTP 404 if the web page
has not been archived. This is because web archives are typically accessed by
URI lookup, and the response is binary: the archive either has the page or it
does not, and the user will not know of other archived web pages that exist and
are potentially similar to the requested web page. In this paper, we propose
augmenting these binary responses with a model for selecting and ranking
recommended web pages in a Web archive. This is to enhance both HTTP 404
responses and HTTP 200 responses by surfacing web pages in the archive that the
user may not know existed. First, we check if the URI is already classified in
DMOZ or Wikipedia. If the requested URI is not found, we use ML to classify the
URI using DMOZ as our ontology and collect candidate URIs to recommended to the
user. Next, we filter the candidates based on if they are present in the
archive. Finally, we rank candidates based on several features, such as
archival quality, web page popularity, temporal similarity, and URI similarity.
We calculated the F1 score for different methods of classifying the requested
web page at the first level. We found that using all-grams from the URI after
removing numerals and the TLD produced the best result with F1=0.59. For
second-level classification, the micro-average F1=0.30. We found that 44.89% of
the correctly classified URIs contained at least one word that exists in a
dictionary and 50.07% of the correctly classified URIs contained long strings
in the domain. In comparison with the URIs from our Wayback access logs, only
5.39% of those URIs contained only words from a dictionary, and 26.74%
contained at least one word from a dictionary. These percentages are low and
may affect the ability for the requested URI to be correctly classified.
",5.0
373,8,152481,2104.02962,node embedding for graph,"Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, Qiang Liu, Liang Wang,
  Mengmeng Ai",DyGCN: Dynamic Graph Embedding with Graph Convolutional Network,"  Graph embedding, aiming to learn low-dimensional representations (aka.
embeddings) of nodes, has received significant attention recently. Recent years
have witnessed a surge of efforts made on static graphs, among which Graph
Convolutional Network (GCN) has emerged as an effective class of models.
However, these methods mainly focus on the static graph embedding. In this
work, we propose an efficient dynamic graph embedding approach, Dynamic Graph
Convolutional Network (DyGCN), which is an extension of GCN-based methods. We
naturally generalizes the embedding propagation scheme of GCN to dynamic
setting in an efficient manner, which is to propagate the change along the
graph to update node embeddings. The most affected nodes are first updated, and
then their changes are propagated to the further nodes and leads to their
update. Extensive experiments conducted on various dynamic graphs demonstrate
that our model can update the node embeddings in a time-saving and
performance-preserving way.
",5.0
374,18,61582,1712.1011,infomation retrieval time complexity,"Su Yan, Wei Lin, Tianshu Wu, Daorui Xiao, Xu Zheng, Bo Wu, Kaipeng Liu","Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in
  E-Commerce Sponsored Search","  On most sponsored search platforms, advertisers bid on some keywords for
their advertisements (ads). Given a search request, ad retrieval module
rewrites the query into bidding keywords, and uses these keywords as keys to
select Top N ads through inverted indexes. In this way, an ad will not be
retrieved even if queries are related when the advertiser does not bid on
corresponding keywords. Moreover, most ad retrieval approaches regard rewriting
and ad-selecting as two separated tasks, and focus on boosting relevance
between search queries and ads. Recently, in e-commerce sponsored search more
and more personalized information has been introduced, such as user profiles,
long-time and real-time clicks. Personalized information makes ad retrieval
able to employ more elements (e.g. real-time clicks) as search signals and
retrieval keys, however it makes ad retrieval more difficult to measure ads
retrieved through different signals. To address these problems, we propose a
novel ad retrieval framework beyond keywords and relevance in e-commerce
sponsored search. Firstly, we employ historical ad click data to initialize a
hierarchical network representing signals, keys and ads, in which personalized
information is introduced. Then we train a model on top of the hierarchical
network by learning the weights of edges. Finally we select the best edges
according to the model, boosting RPM/CTR. Experimental results on our
e-commerce platform demonstrate that our ad retrieval framework achieves good
performance.
",1.0
375,16,166177,2108.04964,activation function in neutral networks,"Lei Wu, Jihao Long","A spectral-based analysis of the separation between two-layer neural
  networks and linear methods","  We propose a spectral-based approach to analyze how two-layer neural networks
separate from linear methods in terms of approximating high-dimensional
functions. We show that quantifying this separation can be reduced to
estimating the Kolmogorov width of two-layer neural networks, and the latter
can be further characterized by using the spectrum of an associated kernel.
Different from previous work, our approach allows obtaining upper bounds, lower
bounds, and identifying explicit hard functions in a united manner. We provide
a systematic study of how the choice of activation functions affects the
separation, in particular the dependence on the input dimension. Specifically,
for nonsmooth activation functions, we extend known results to more activation
functions with sharper bounds. As concrete examples, we prove that any single
neuron can instantiate the separation between neural networks and random
feature models. For smooth activation functions, one surprising finding is that
the separation is negligible unless the norms of inner-layer weights are
polynomially large with respect to the input dimension. By contrast, the
separation for nonsmooth activation functions is independent of the norms of
inner-layer weights.
",4.0
376,11,38605,1512.04633,PageRank for web search,Peter Lofgren,Efficient Algorithms for Personalized PageRank,"  We present new, more efficient algorithms for estimating random walk scores
such as Personalized PageRank from a given source node to one or several target
nodes. These scores are useful for personalized search and recommendations on
networks including social networks, user-item networks, and the web. Past work
has proposed using Monte Carlo or using linear algebra to estimate scores from
a single source to every target, making them inefficient for a single pair. Our
contribution is a new bidirectional algorithm which combines linear algebra and
Monte Carlo to achieve significant speed improvements. On a diverse set of six
graphs, our algorithm is 70x faster than past state-of-the-art algorithms. We
also present theoretical analysis: while past algorithms require $\Omega(n)$
time to estimate a random walk score of typical size $\frac{1}{n}$ on an
$n$-node graph to a given constant accuracy, our algorithm requires only
$O(\sqrt{m})$ expected time for an average target, where $m$ is the number of
edges, and is provably accurate.
  In addition to our core bidirectional estimator for personalized PageRank, we
present an alternative algorithm for undirected graphs, a generalization to
arbitrary walk lengths and Markov Chains, an algorithm for personalized search
ranking, and an algorithm for sampling random paths from a given source to a
given set of targets. We expect our bidirectional methods can be extended in
other ways and will be useful subroutines in other graph analysis problems.
",5.0
377,10,162782,2107.0268,web archive,"Alexander C. Nwala, Michele C. Weigle, Michael L. Nelson","Garbage, Glitter, or Gold: Assigning Multi-dimensional Quality Scores to
  Social Media Seeds for Web Archive Collections","  From popular uprisings to pandemics, the Web is an essential source consulted
by scientists and historians for reconstructing and studying past events.
Unfortunately, the Web is plagued by reference rot which causes important Web
resources to disappear. Web archive collections help reduce the costly effects
of reference rot by saving Web resources that chronicle important
stories/events before they disappear. These collections often begin with URLs
called seeds, hand-selected by experts or scraped from social media. The
quality of social media content varies widely, therefore, we propose a
framework for assigning multi-dimensional quality scores to social media seeds
for Web archive collections about stories and events. We leveraged
contributions from social media research for attributing quality to social
media content and users based on credibility, reputation, and influence. We
combined these with additional contributions from the Web archive research that
emphasizes the importance of considering geographical and temporal constraints
when selecting seeds. Next, we developed the Quality Proxies (QP) framework
which assigns seeds extracted from social media a quality score across 10 major
dimensions: popularity, geographical, temporal, subject expert, retrievability,
relevance, reputation, and scarcity. We instantiated the framework and showed
that seeds can be scored across multiple QP classes that map to different
policies for ranking seeds such as prioritizing seeds from local news,
reputable and/or popular sources, etc. The QP framework is extensible and
robust. Our results showed that Quality Proxies resulted in the selection of
quality seeds with increased precision (by ~0.13) when novelty is and is not
prioritized. These contributions provide an explainable score applicable to
rank and select quality seeds for Web archive collections and other domains.
",3.0
378,0,47531,1611.04218,learning to rank with partitioned preference,"Suriya Gunasekar, Oluwasanmi Koyejo, Joydeep Ghosh",Preference Completion from Partial Rankings,"  We propose a novel and efficient algorithm for the collaborative preference
completion problem, which involves jointly estimating individualized rankings
for a set of entities over a shared set of items, based on a limited number of
observed affinity values. Our approach exploits the observation that while
preferences are often recorded as numerical scores, the predictive quantity of
interest is the underlying rankings. Thus, attempts to closely match the
recorded scores may lead to overfitting and impair generalization performance.
Instead, we propose an estimator that directly fits the underlying preference
order, combined with nuclear norm constraints to encourage low--rank
parameters. Besides (approximate) correctness of the ranking order, the
proposed estimator makes no generative assumption on the numerical scores of
the observations. One consequence is that the proposed estimator can fit any
consistent partial ranking over a subset of the items represented as a directed
acyclic graph (DAG), generalizing standard techniques that can only fit
preference scores. Despite this generality, for supervision representing total
or blockwise total orders, the computational complexity of our algorithm is
within a $\log$ factor of the standard algorithms for nuclear norm
regularization based estimates for matrix completion. We further show promising
empirical results for a novel and challenging application of collaboratively
ranking of the associations between brain--regions and cognitive neuroscience
terms.
",5.0
379,8,178227,2112.00075,node embedding for graph,"Bogumi{\l} Kami\'nski, {\L}ukasz Krai\'nski, Pawe{\l} Pra{\l}at,
  Fran\c{c}ois Th\'eberge","A Multi-purposed Unsupervised Framework for Comparing Embeddings of
  Undirected and Directed Graphs","  Graph embedding is a transformation of nodes of a network into a set of
vectors. A good embedding should capture the underlying graph topology and
structure, node-to-node relationship, and other relevant information about the
graph, its subgraphs, and nodes themselves. If these objectives are achieved,
an embedding is a meaningful, understandable, and often compressed
representation of a network. Unfortunately, selecting the best embedding is a
challenging task and very often requires domain experts. In this paper, we
extend the framework for evaluating graph embeddings that was recently
introduced by the authors. Now, the framework assigns two scores, local and
global, to each embedding that measure the quality of an evaluated embedding
for tasks that require good representation of local and, respectively, global
properties of the network. The best embedding, if needed, can be selected in an
unsupervised way, or the framework can identify a few embeddings that are worth
further investigation. The framework is flexible, scalable, and can deal with
undirected/directed, weighted/unweighted graphs.
",4.0
380,8,176950,2111.09308,node embedding for graph,"Archit Parnami, Mayuri Deshpande, Anant Kumar Mishra, Minwoo Lee","Transformation of Node to Knowledge Graph Embeddings for Faster Link
  Prediction in Social Networks","  Recent advances in neural networks have solved common graph problems such as
link prediction, node classification, node clustering, node recommendation by
developing embeddings of entities and relations into vector spaces. Graph
embeddings encode the structural information present in a graph. The encoded
embeddings then can be used to predict the missing links in a graph. However,
obtaining the optimal embeddings for a graph can be a computationally
challenging task specially in an embedded system. Two techniques which we focus
on in this work are 1) node embeddings from random walk based methods and 2)
knowledge graph embeddings. Random walk based embeddings are computationally
inexpensive to obtain but are sub-optimal whereas knowledge graph embeddings
perform better but are computationally expensive. In this work, we investigate
a transformation model which converts node embeddings obtained from random walk
based methods to embeddings obtained from knowledge graph methods directly
without an increase in the computational cost. Extensive experimentation shows
that the proposed transformation model can be used for solving link prediction
in real-time.
",4.0
381,3,86901,1904.11121,database management system,"Dimitrije Jankov, Shangyu Luo, Binhang Yuan, Zhuhua Cai, Jia Zou,
  Chris Jermaine, Zekai J. Gao","Declarative Recursive Computation on an RDBMS, or, Why You Should Use a
  Database For Distributed Machine Learning","  A number of popular systems, most notably Google's TensorFlow, have been
implemented from the ground up to support machine learning tasks. We consider
how to make a very small set of changes to a modern relational database
management system (RDBMS) to make it suitable for distributed learning
computations. Changes include adding better support for recursion, and
optimization and execution of very large compute plans. We also show that there
are key advantages to using an RDBMS as a machine learning platform. In
particular, learning based on a database management system allows for trivial
scaling to large data sets and especially large models, where different
computational units operate on different parts of a model that may be too large
to fit into RAM.
",5.0
382,6,117060,2004.13481,query expansion for imformation retrieval,"Bhawani Selvaretnam, Mohammed Belkhatir","A Linguistically Driven Framework for Query Expansion via Grammatical
  Constituent Highlighting and Role-Based Concept Weighting","  In this paper, we propose a linguistically-motivated query expansion
framework that recognizes and en-codes significant query constituents that
characterize query intent in order to improve retrieval performance.
Concepts-of-Interest are recognized as the core concepts that represent the
gist of the search goal whilst the remaining query constituents which serve to
specify the search goal and complete the query structure are classified as
descriptive, relational or structural. Acknowledging the need to form
semantically-associated base pairs for the purpose of extracting related
potential expansion concepts, an algorithm which capitalizes on syntactical
dependencies to capture relationships between adjacent and non-adjacent query
concepts is proposed. Lastly, a robust weighting scheme that duly emphasizes
the importance of query constituents based on their linguistic role within the
expanded query is presented. We demonstrate improvements in retrieval
effectiveness in terms of increased mean average precision (MAP) garnered by
the proposed linguistic-based query expansion framework through experimentation
on the TREC ad hoc test collections.
",5.0
383,0,203042,2207.0447,learning to rank with partitioned preference,"Lukas Gienapp and Maik Fr\""obe and Matthias Hagen and Martin Potthast",Sparse Pairwise Re-ranking with Pre-trained Transformers,"  Pairwise re-ranking models predict which of two documents is more relevant to
a query and then aggregate a final ranking from such preferences. This is often
more effective than pointwise re-ranking models that directly predict a
relevance value for each document. However, the high inference overhead of
pairwise models limits their practical application: usually, for a set of $k$
documents to be re-ranked, preferences for all $k^2-k$ comparison pairs
excluding self-comparisons are aggregated. We investigate whether the
efficiency of pairwise re-ranking can be improved by sampling from all pairs.
In an exploratory study, we evaluate three sampling methods and five preference
aggregation methods. The best combination allows for an order of magnitude
fewer comparisons at an acceptable loss of retrieval effectiveness, while
competitive effectiveness is already achieved with about one third of the
comparisons.
",1.0
384,11,4161,911.3823,PageRank for web search,"Leonardo Ermann, Dima D.L. Shepelyansky",Google matrix and Ulam networks of intermittency maps,"  We study the properties of the Google matrix of an Ulam network generated by
intermittency maps. This network is created by the Ulam method which gives a
matrix approximant for the Perron-Frobenius operator of dynamical map. The
spectral properties of eigenvalues and eigenvectors of this matrix are
analyzed. We show that the PageRank of the system is characterized by a power
law decay with the exponent $\beta$ dependent on map parameters and the Google
damping factor $\alpha$. Under certain conditions the PageRank is completely
delocalized so that the Google search in such a situation becomes inefficient.
",4.0
385,16,195629,2205.0657,activation function in neutral networks,"Wentao Huang, Yuesheng Xu, Haizhang Zhang","Convergence of Deep Neural Networks with General Activation Functions
  and Pooling","  Deep neural networks, as a powerful system to represent high dimensional
complex functions, play a key role in deep learning. Convergence of deep neural
networks is a fundamental issue in building the mathematical foundation for
deep learning. We investigated the convergence of deep ReLU networks and deep
convolutional neural networks in two recent researches (arXiv:2107.12530,
2109.13542). Only the Rectified Linear Unit (ReLU) activation was studied
therein, and the important pooling strategy was not considered. In this current
work, we study the convergence of deep neural networks as the depth tends to
infinity for two other important activation functions: the leaky ReLU and the
sigmoid function. Pooling will also be studied. As a result, we prove that the
sufficient condition established in arXiv:2107.12530, 2109.13542 is still
sufficient for the leaky ReLU networks. For contractive activation functions
such as the sigmoid function, we establish a weaker sufficient condition for
uniform convergence of deep neural networks.
",3.0
386,3,104362,1911.11725,database management system,"Nino Arsov, Goran Velinov, Aleksandar S. Dimovski, Bojana Koteska,
  Dragan Sahpaski, Margina Kon-Popovska","Prediction of Horizontal Data Partitioning Through Query Execution Cost
  Estimation","  The excessively increased volume of data in modern data management systems
demands an improved system performance, frequently provided by data
distribution, system scalability and performance optimization techniques.
Optimized horizontal data partitioning has a significant influence of
distributed data management systems. An optimally partitioned schema found in
the early phase of logical database design without loading of real data in the
system and its adaptation to changes of business environment are very important
for a successful implementation, system scalability and performance
improvement. In this paper we present a novel approach for finding an optimal
horizontally partitioned schema that manifests a minimal total execution cost
of a given database workload. Our approach is based on a formal model that
enables abstraction of the predicates in the workload queries, and are
subsequently used to define all relational fragments. This approach has
predictive features acquired by simulation of horizontal partitioning, without
loading any data into the partitions, but instead, altering the statistics in
the database catalogs. We define an optimization problem and employ a genetic
algorithm (GA) to find an approximately optimal horizontally partitioned
schema. The solutions to the optimization problem are evaluated using
PostgreSQL's query optimizer. The initial experimental evaluation of our
approach confirms its efficiency and correctness, and the numbers imply that
the approach is effective in reducing the workload execution cost.
",4.0
387,15,24369,1401.3896,relevance feedback for imformation retrieval,"Oren Kurland, Eyal Krikon","The Opposite of Smoothing: A Language Model Approach to Ranking
  Query-Specific Document Clusters","  Exploiting information induced from (query-specific) clustering of
top-retrieved documents has long been proposed as a means for improving
precision at the very top ranks of the returned results. We present a novel
language model approach to ranking query-specific clusters by the presumed
percentage of relevant documents that they contain. While most previous cluster
ranking approaches focus on the cluster as a whole, our model utilizes also
information induced from documents associated with the cluster. Our model
substantially outperforms previous approaches for identifying clusters
containing a high relevant-document percentage. Furthermore, using the model to
produce document ranking yields precision-at-top-ranks performance that is
consistently better than that of the initial ranking upon which clustering is
performed. The performance also favorably compares with that of a
state-of-the-art pseudo-feedback-based retrieval method.
",2.0
388,1,13384,1206.1494,advanced search engine,"Georg Singer, Ulrich Norbisrath, Dirk Lewandowski",Impact of Gender and Age on performing Search Tasks Online,"  More and more people use the Internet to work on duties of their daily work
routine. To find the right information online, Web search engines are the tools
of their choice. Apart from finding facts, people use Web search engines to
also execute rather complex and time consuming search tasks. So far search
engines follow the one-for-all approach to serve its users and little is known
about the impact of gender and age on people's Web search behavior. In this
article we present a study that examines (1) how female and male web users
carry out simple and complex search tasks and what are the differences between
the two user groups, and (2) how the age of the users impacts their search
performance. The laboratory study was done with 56 ordinary people each
carrying out 12 search tasks. Our findings confirm that age impacts behavior
and search performance significantly, while gender influences were smaller than
expected.
",3.0
389,12,123731,2006.14654,COVID-19 and social media,"Di ""Chelsea"" Sun, Vaishnavi Melkote, Ahmed Sabbir Arif","Exploratory Study of Young Children's Social Media Needs and
  Requirements","  As social media are becoming increasingly popular among young children, it is
important to explore this population's needs and requirements from these
platforms. As a first step to this, we conducted an exploratory design workshop
with children aged between ten and eleven years to find out about their social
media needs and requirements. Through an analysis of the paper prototypes
solicited from the workshop, here we discuss the social media features that are
the most desired by this population.
",1.0
390,0,142368,2012.13569,learning to rank with partitioned preference,"Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao",Dynamic-K Recommendation with Personalized Decision Boundary,"  In this paper, we investigate the recommendation task in the most common
scenario with implicit feedback (e.g., clicks, purchases). State-of-the-art
methods in this direction usually cast the problem as to learn a personalized
ranking on a set of items (e.g., webpages, products). The top-N results are
then provided to users as recommendations, where the N is usually a fixed
number pre-defined by the system according to some heuristic criteria (e.g.,
page size, screen size). There is one major assumption underlying this
fixed-number recommendation scheme, i.e., there are always sufficient relevant
items to users' preferences. Unfortunately, this assumption may not always hold
in real-world scenarios. In some applications, there might be very limited
candidate items to recommend, and some users may have very high relevance
requirement in recommendation. In this way, even the top-1 ranked item may not
be relevant to a user's preference. Therefore, we argue that it is critical to
provide a dynamic-K recommendation, where the K should be different with
respect to the candidate item set and the target user. We formulate this
dynamic-K recommendation task as a joint learning problem with both ranking and
classification objectives. The ranking objective is the same as existing
methods, i.e., to create a ranking list of items according to users' interests.
The classification objective is unique in this work, which aims to learn a
personalized decision boundary to differentiate the relevant items from
irrelevant items. Based on these ideas, we extend two state-of-the-art
ranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding
dynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two
datasets show that the dynamic-K models are more effective than the original
fixed-N recommendation methods.
",1.0
391,1,7229,1101.0198,advanced search engine,"S.K. Jayanthi, S. Sasikala",Link Spam Detection based on DBSpamClust with Fuzzy C-means Clustering,"  Search engine became omnipresent means for ingoing to the web. Spamming
Search engine is the technique to deceiving the ranking in search engine and it
inflates the ranking. Web spammers have taken advantage of the vulnerability of
link based ranking algorithms by creating many artificial references or links
in order to acquire higher-than-deserved ranking n search engines' results.
Link based algorithms such as PageRank, HITS utilizes the structural details of
the hyperlinks for ranking the content in the web. In this paper an algorithm
DBSpamClust is proposed for link spam detection. As showing through experiments
such a method can filter out web spam effectively
",3.0
392,2,29218,1410.2838,random forests,Ender Konukoglu and Melanie Ganz,"Approximate False Positive Rate Control in Selection Frequency for
  Random Forest","  Random Forest has become one of the most popular tools for feature selection.
Its ability to deal with high-dimensional data makes this algorithm especially
useful for studies in neuroimaging and bioinformatics. Despite its popularity
and wide use, feature selection in Random Forest still lacks a crucial
ingredient: false positive rate control. To date there is no efficient,
principled and computationally light-weight solution to this shortcoming. As a
result, researchers using Random Forest for feature selection have to resort to
using heuristically set thresholds on feature rankings. This article builds an
approximate probabilistic model for the feature selection process in random
forest training, which allows us to compute an estimated false positive rate
for a given threshold on selection frequency. Hence, it presents a principled
way to determine thresholds for the selection of relevant features without any
additional computational load. Experimental analysis with synthetic data
demonstrates that the proposed approach can limit false positive rates on the
order of the desired values and keep false negative rates low. Results show
that this holds even in the presence of a complex correlation structure between
features. Its good statistical properties and light-weight computational needs
make this approach widely applicable to feature selection for a wide-range of
applications.
",4.0
393,1,14940,1208.427,advanced search engine,"Kyu-Young Whang, Tae-Seob Yun, Yeon-Mi Yeo, Il-Yeol Song, Hyuk-Yoon
  Kwon, In-Joong Kim","ODYS: A Massively-Parallel Search Engine Using a DB-IR
  Tightly-Integrated Parallel DBMS","  Recently, parallel search engines have been implemented based on scalable
distributed file systems such as Google File System. However, we claim that
building a massively-parallel search engine using a parallel DBMS can be an
attractive alternative since it supports a higher-level (i.e., SQL-level)
interface than that of a distributed file system for easy and less error-prone
application development while providing scalability. In this paper, we propose
a new approach of building a massively-parallel search engine using a DB-IR
tightly-integrated parallel DBMS and demonstrate its commercial-level
scalability and performance. In addition, we present a hybrid (i.e., analytic
and experimental) performance model for the parallel search engine. We have
built a five-node parallel search engine according to the proposed architecture
using a DB-IR tightly-integrated DBMS. Through extensive experiments, we show
the correctness of the model by comparing the projected output with the
experimental results of the five-node engine. Our model demonstrates that ODYS
is capable of handling 1 billion queries per day (81 queries/sec) for 30
billion web pages by using only 43,472 nodes with an average query response
time of 211 ms, which is equivalent to or better than those of commercial
search engines. We also show that, by using twice as many (86,944) nodes, ODYS
can provide an average query response time of 162 ms, which is significantly
lower than those of commercial search engines.
",5.0
394,5,21786,1308.4994,matrix completion,"Dionysios S. Kalogerias, Athina P. Petropulu","Matrix Completion in Colocated MIMO Radar: Recoverability, Bounds &
  Theoretical Guarantees","  It was recently shown that low rank matrix completion theory can be employed
for designing new sampling schemes in the context of MIMO radars, which can
lead to the reduction of the high volume of data typically required for
accurate target detection and estimation. Employing random samplers at each
reception antenna, a partially observed version of the received data matrix is
formulated at the fusion center, which, under certain conditions, can be
recovered using convex optimization. This paper presents the theoretical
analysis regarding the performance of matrix completion in colocated MIMO radar
systems, exploiting the particular structure of the data matrix. Both Uniform
Linear Arrays (ULAs) and arbitrary 2-dimensional arrays are considered for
transmission and reception. Especially for the ULA case, under some mild
assumptions on the directions of arrival of the targets, it is explicitly shown
that the coherence of the data matrix is both asymptotically and approximately
optimal with respect to the number of antennas of the arrays involved and
further, the data matrix is recoverable using a subset of its entries with
minimal cardinality. Sufficient conditions guaranteeing low matrix coherence
and consequently satisfactory matrix completion performance are also presented,
including the arbitrary 2-dimensional array case.
",4.0
395,1,83084,1902.08498,advanced search engine,"Cun Mu, Jun Zhao, Guang Yang, Binwei Yang, Zheng Yan","Fast and Exact Nearest Neighbor Search in Hamming Space on Full-Text
  Search Engines","  A growing interest has been witnessed recently from both academia and
industry in building nearest neighbor search (NNS) solutions on top of
full-text search engines. Compared with other NNS systems, such solutions are
capable of effectively reducing main memory consumption, coherently supporting
multi-model search and being immediately ready for production deployment. In
this paper, we continue the journey to explore specifically how to empower
full-text search engines with fast and exact NNS in Hamming space (i.e., the
set of binary codes). By revisiting three techniques (bit operation, subs-code
filtering and data preprocessing with permutation) in information retrieval
literature, we develop a novel engineering solution for full-text search
engines to efficiently accomplish this special but important NNS task. In the
experiment, we show that our proposed approach enables full-text search engines
to achieve significant speed-ups over its state-of-the-art term match approach
for NNS within binary codes.
",5.0
396,10,49978,1702.01015,web archive,"Helge Holzmann, Vinay Goel, Avishek Anand","ArchiveSpark: Efficient Web Archive Access, Extraction and Derivation","  Web archives are a valuable resource for researchers of various disciplines.
However, to use them as a scholarly source, researchers require a tool that
provides efficient access to Web archive data for extraction and derivation of
smaller datasets. Besides efficient access we identify five other objectives
based on practical researcher needs such as ease of use, extensibility and
reusability.
  Towards these objectives we propose ArchiveSpark, a framework for efficient,
distributed Web archive processing that builds a research corpus by working on
existing and standardized data formats commonly held by Web archiving
institutions. Performance optimizations in ArchiveSpark, facilitated by the use
of a widely available metadata index, result in significant speed-ups of data
processing. Our benchmarks show that ArchiveSpark is faster than alternative
approaches without depending on any additional data stores while improving
usability by seamlessly integrating queries and derivations with external
tools.
",5.0
397,19,213705,2210.09366,artificial intelligence for low carbon,Ananta Nair and Farnoush Banaei-Kashani,"Bridging the Gap between Artificial Intelligence and Artificial General
  Intelligence: A Ten Commandment Framework for Human-Like Intelligence","  The field of artificial intelligence has seen explosive growth and
exponential success. The last phase of development showcased deep learnings
ability to solve a variety of difficult problems across a multitude of domains.
Many of these networks met and exceeded human benchmarks by becoming experts in
the domains in which they are trained. Though the successes of artificial
intelligence have begun to overshadow its failures, there is still much that
separates current artificial intelligence tools from becoming the exceptional
general learners that humans are. In this paper, we identify the ten
commandments upon which human intelligence is systematically and hierarchically
built. We believe these commandments work collectively to serve as the
essential ingredients that lead to the emergence of higher-order cognition and
intelligence. This paper discusses a computational framework that could house
these ten commandments and suggests new architectural modifications that could
lead to the development of smarter, more explainable, and generalizable
artificial systems inspired by a neuromorphic approach.
",1.0
398,6,114596,2004.00293,query expansion for imformation retrieval,"Noemi Mauro, Liliana Ardissono, Laura Di Rocco, Michela Bertolotto and
  Giovanna Guerrini",Impact of Semantic Granularity on Geographic Information Search Support,"  The Information Retrieval research has used semantics to provide accurate
search results, but the analysis of conceptual abstraction has mainly focused
on information integration. We consider session-based query expansion in
Geographical Information Retrieval, and investigate the impact of semantic
granularity (i.e., specificity of concepts representation) on the suggestion of
relevant types of information to search for. We study how different levels of
detail in knowledge representation influence the capability of guiding the user
in the exploration of a complex information space. A comparative analysis of
the performance of a query expansion model, using three spatial ontologies
defined at different semantic granularity levels, reveals that a fine-grained
representation enhances recall. However, precision depends on how closely the
ontologies match the way people conceptualize and verbally describe the
geographic space.
",3.0
399,4,31456,1502.00804,pre-trained language model,"Ronan Cummins, Jiaul Hoque Paik, and Yuanhua Lv",A Polya Urn Document Language Model for Improved Information Retrieval,"  The multinomial language model has been one of the most effective models of
retrieval for over a decade. However, the multinomial distribution does not
model one important linguistic phenomenon relating to term-dependency, that is
the tendency of a term to repeat itself within a document (i.e. word
burstiness). In this article, we model document generation as a random process
with reinforcement (a multivariate Polya process) and develop a Dirichlet
compound multinomial language model that captures word burstiness directly.
  We show that the new reinforced language model can be computed as efficiently
as current retrieval models, and with experiments on an extensive set of TREC
collections, we show that it significantly outperforms the state-of-the-art
language model for a number of standard effectiveness metrics. Experiments also
show that the tuning parameter in the proposed model is more robust than in the
multinomial language model. Furthermore, we develop a constraint for the
verbosity hypothesis and show that the proposed model adheres to the
constraint. Finally, we show that the new language model essentially introduces
a measure closely related to idf which gives theoretical justification for
combining the term and document event spaces in tf-idf type schemes.
",5.0
400,0,22926,1311.0251,learning to rank with partitioned preference,"Andrew Mao, Hossein Azari Soufiani, Yiling Chen, David C. Parkes",Capturing Variation and Uncertainty in Human Judgment,"  The well-studied problem of statistical rank aggregation has been applied to
comparing sports teams, information retrieval, and most recently to data
generated by human judgment. Such human-generated rankings may be substantially
different from traditional statistical ranking data. In this work, we show that
a recently proposed generalized random utility model reveals distinctive
patterns in human judgment across three different domains, and provides a
succinct representation of variance in both population preferences and
imperfect perception. In contrast, we also show that classical statistical
ranking models fail to capture important features from human-generated input.
Our work motivates the use of more flexible ranking models for representing and
describing the collective preferences or decision-making of human participants.
",1.0
401,15,95281,1908.03361,relevance feedback for imformation retrieval,"Bj\""orn Barz, Kai Schr\""oter, Moritz M\""unch, Bin Yang, Andrea Unger,
  Doris Dransch, Joachim Denzler","Enhancing Flood Impact Analysis using Interactive Retrieval of Social
  Media Images","  The analysis of natural disasters such as floods in a timely manner often
suffers from limited data due to a coarse distribution of sensors or sensor
failures. This limitation could be alleviated by leveraging information
contained in images of the event posted on social media platforms, so-called
""Volunteered Geographic Information (VGI)"". To save the analyst from the need
to inspect all images posted online manually, we propose to use content-based
image retrieval with the possibility of relevance feedback for retrieving only
relevant images of the event to be analyzed. To evaluate this approach, we
introduce a new dataset of 3,710 flood images, annotated by domain experts
regarding their relevance with respect to three tasks (determining the flooded
area, inundation depth, water pollution). We compare several image features and
relevance feedback methods on that dataset, mixed with 97,085 distractor
images, and are able to improve the precision among the top 100 retrieval
results from 55% with the baseline retrieval to 87% after 5 rounds of feedback.
",2.0
402,12,172460,2110.05573,COVID-19 and social media,"Kamil Raczycki, Marcin Szyma\'nski, Yahor Yeliseyenka, Piotr
  Szyma\'nski, Tomasz Kajdanowicz","Spatial Data Mining of Public Transport Incidents reported in Social
  Media","  Public transport agencies use social media as an essential tool for
communicating mobility incidents to passengers. However, while the short term,
day-to-day information about transport phenomena is usually posted in social
media with low latency, its availability is short term as the content is rarely
made an aggregated form. Social media communication of transport phenomena
usually lacks GIS annotations as most social media platforms do not allow
attaching non-POI GPS coordinates to posts. As a result, the analysis of
transport phenomena information is minimal. We collected three years of social
media posts of a polish public transport company with user comments. Through
exploration, we infer a six-class transport information typology. We
successfully build an information type classifier for social media posts,
detect stop names in posts, and relate them to GPS coordinates, obtaining a
spatial understanding of long-term aggregated phenomena. We show that our
approach enables citizen science and use it to analyze the impact of three
years of infrastructure incidents on passenger mobility, and the sentiment and
reaction scale towards each of the events. All these results are achieved for
Polish, an under-resourced language when it comes to spatial language
understanding, especially in social media contexts. To improve the situation,
we released two of our annotated data sets: social media posts with incident
type labels and matched stop names and social media comments with the annotated
sentiment. We also opensource the experimental codebase.
",1.0
403,16,174332,2110.13572,activation function in neutral networks,"Lassi Meronen, Martin Trapp, Arno Solin",Periodic Activation Functions Induce Stationarity,"  Neural network models are known to reinforce hidden data biases, making them
unreliable and difficult to interpret. We seek to build models that `know what
they do not know' by introducing inductive biases in the function space. We
show that periodic activation functions in Bayesian neural networks establish a
connection between the prior on the network weights and translation-invariant,
stationary Gaussian process priors. Furthermore, we show that this link goes
beyond sinusoidal (Fourier) activations by also covering triangular wave and
periodic ReLU activation functions. In a series of experiments, we show that
periodic activation functions obtain comparable performance for in-domain data
and capture sensitivity to perturbed inputs in deep neural networks for
out-of-domain detection.
",5.0
404,16,181251,2112.14877,activation function in neutral networks,Tan Bui-Thanh,"A Unified and Constructive Framework for the Universality of Neural
  Networks","  One of the reasons why many neural networks are capable of replicating
complicated tasks or functions is their universal property. Though the past few
decades have seen tremendous advances in theories of neural networks, a single
constructive framework for neural network universality remains unavailable.
This paper is an effort to provide a unified and constructive framework for the
universality of a large class of activations including most of existing ones.
At the heart of the framework is the concept of neural network approximate
identity (nAI). The main result is: {\em any nAI activation function is
universal}. It turns out that most of existing activations are nAI, and thus
universal in the space of continuous functions on compacta. The framework has
the following main properties. First, it is constructive with elementary means
from functional analysis, probability theory, and numerical analysis. Second,
it is the first unified attempt that is valid for most of existing activations.
Third, as a by product, the framework provides the first university proof for
some of the existing activation functions including Mish, SiLU, ELU, GELU, and
etc. Fourth, it provides new proofs for most activation functions. Fifth, it
discovers new activations with guaranteed universality property. Sixth, for a
given activation and error tolerance, the framework provides precisely the
architecture of the corresponding one-hidden neural network with predetermined
number of neurons, and the values of weights/biases. Seventh, the framework
allows us to abstractly present the first universal approximation with
favorable non-asymptotic rate.
",5.0
405,3,21980,1309.1556,database management system,"Yu cao, Xiaoyan Guo, Stephen Todd",Hyper-Graph Based Database Partitioning for Transactional Workloads,"  A common approach to scaling transactional databases in practice is
horizontal partitioning, which increases system scalability, high availability
and self-manageability. Usu- ally it is very challenging to choose or design an
optimal partitioning scheme for a given workload and database. In this
technical report, we propose a fine-grained hyper-graph based database
partitioning system for transactional work- loads. The partitioning system
takes a database, a workload, a node cluster and partitioning constraints as
input and out- puts a lookup-table encoding the final database partitioning
decision. The database partitioning problem is modeled as a multi-constraints
hyper-graph partitioning problem. By deriving a min-cut of the hyper-graph, our
system can min- imize the total number of distributed transactions in the
workload, balance the sizes and workload accesses of the partitions and satisfy
all the partition constraints imposed. Our system is highly interactive as it
allows users to im- pose partition constraints, watch visualized partitioning
ef- fects, and provide feedback based on human expertise and indirect domain
knowledge for generating better partition- ing schemes.
",4.0
406,14,161998,2106.15313,text summarization model,"Kalliath Abdul Rasheed Issam, Shivam Patel, Subalalitha C. N",Topic Modeling Based Extractive Text Summarization,"  Text summarization is an approach for identifying important information
present within text documents. This computational technique aims to generate
shorter versions of the source text, by including only the relevant and salient
information present within the source text. In this paper, we propose a novel
method to summarize a text document by clustering its contents based on latent
topics produced using topic modeling techniques and by generating extractive
summaries for each of the identified text clusters. All extractive
sub-summaries are later combined to generate a summary for any given source
document. We utilize the lesser used and challenging WikiHow dataset in our
approach to text summarization. This dataset is unlike the commonly used news
datasets which are available for text summarization. The well-known news
datasets present their most important information in the first few lines of
their source texts, which make their summarization a lesser challenging task
when compared to summarizing the WikiHow dataset. Contrary to these news
datasets, the documents in the WikiHow dataset are written using a generalized
approach and have lesser abstractedness and higher compression ratio, thus
proposing a greater challenge to generate summaries. A lot of the current
state-of-the-art text summarization techniques tend to eliminate important
information present in source documents in the favor of brevity. Our proposed
technique aims to capture all the varied information present in source
documents. Although the dataset proved challenging, after performing extensive
tests within our experimental setup, we have discovered that our model produces
encouraging ROUGE results and summaries when compared to the other published
extractive and abstractive text summarization models.
",5.0
407,18,5238,1004.446,infomation retrieval time complexity,"Sumalatha Ramachandran, Sharon Joseph, Sujaya Paulraj and Vetriselvi
  Ramaraj","Handling Overload Conditions In High Performance Trustworthy Information
  Retrieval Systems","  Web search engines retrieve a vast amount of information for a given search
query. But the user needs only trustworthy and high-quality information from
this vast retrieved data. The response time of the search engine must be a
minimum value in order to satisfy the user. An optimum level of response time
should be maintained even when the system is overloaded. This paper proposes an
optimal Load Shedding algorithm which is used to handle overload conditions in
real-time data stream applications and is adapted to the Information Retrieval
System of a web search engine. Experiment results show that the proposed
algorithm enables a web search engine to provide trustworthy search results to
the user within an optimum response time, even during overload conditions.
",4.0
408,4,130856,2009.05651,pre-trained language model,"Shivani Shimpi, Shyam Thombre, Snehal Reddy, Ritik Sharma, Srijan
  Singh","Multimodal Depression Severity Prediction from medical bio-markers using
  Machine Learning Tools and Technologies","  Depression has been a leading cause of mental-health illnesses across the
world. While the loss of lives due to unmanaged depression is a subject of
attention, so is the lack of diagnostic tests and subjectivity involved. Using
behavioural cues to automate depression diagnosis and stage prediction in
recent years has relatively increased. However, the absence of labelled
behavioural datasets and a vast amount of possible variations prove to be a
major challenge in accomplishing the task. This paper proposes a novel Custom
CM Ensemble approach and focuses on a paradigm of a cross-platform smartphone
application that takes multimodal inputs from a user through a series of
pre-defined questions, sends it to the Cloud ML architecture and conveys back a
depression quotient, representative of its severity. Our app estimates the
severity of depression based on a multi-class classification model by utilizing
the language, audio, and visual modalities. The given approach attempts to
detect, emphasize, and classify the features of a depressed person based on the
low-level descriptors for verbal and visual features, and context of the
language features when prompted with a question. The model achieved a precision
value of 0.88 and an accuracy of 91.56%. Further optimization reveals the
intramodality and intermodality relevance through the selection of the most
influential features within each modality for decision making.
",1.0
409,8,104760,1912.00536,node embedding for graph,"Bhagya Hettige, Yuan-Fang Li, Weiqing Wang and Wray Buntine",Gaussian Embedding of Large-scale Attributed Graphs,"  Graph embedding methods transform high-dimensional and complex graph contents
into low-dimensional representations. They are useful for a wide range of graph
analysis tasks including link prediction, node classification, recommendation
and visualization. Most existing approaches represent graph nodes as point
vectors in a low-dimensional embedding space, ignoring the uncertainty present
in the real-world graphs. Furthermore, many real-world graphs are large-scale
and rich in content (e.g. node attributes). In this work, we propose GLACE, a
novel, scalable graph embedding method that preserves both graph structure and
node attributes effectively and efficiently in an end-to-end manner. GLACE
effectively models uncertainty through Gaussian embeddings, and supports
inductive inference of new nodes based on their attributes. In our
comprehensive experiments, we evaluate GLACE on real-world graphs, and the
results demonstrate that GLACE significantly outperforms state-of-the-art
embedding methods on multiple graph analysis tasks.
",3.0
410,2,63272,1802.03882,random forests,"Nathan Lay, Adam P. Harrison, Sharon Schreiber, Gitesh Dawer, Adrian
  Barbu",Random Hinge Forest for Differentiable Learning,"  We propose random hinge forests, a simple, efficient, and novel variant of
decision forests. Importantly, random hinge forests can be readily incorporated
as a general component within arbitrary computation graphs that are optimized
end-to-end with stochastic gradient descent or variants thereof. We derive
random hinge forest and ferns, focusing on their sparse and efficient nature,
their min-max margin property, strategies to initialize them for arbitrary
network architectures, and the class of optimizers most suitable for optimizing
random hinge forest. The performance and versatility of random hinge forests
are demonstrated by experiments incorporating a variety of of small and large
UCI machine learning data sets and also ones involving the MNIST, Letter, and
USPS image datasets. We compare random hinge forests with random forests and
the more recent backpropagating deep neural decision forests.
",4.0
411,7,34987,1507.02011,gradient boosting,"Qinxun Bai, Henry Lam, Stan Sclaroff",A Bayesian Approach for Online Classifier Ensemble,"  We propose a Bayesian approach for recursively estimating the classifier
weights in online learning of a classifier ensemble. In contrast with past
methods, such as stochastic gradient descent or online boosting, our approach
estimates the weights by recursively updating its posterior distribution. For a
specified class of loss functions, we show that it is possible to formulate a
suitably defined likelihood function and hence use the posterior distribution
as an approximation to the global empirical loss minimizer. If the stream of
training data is sampled from a stationary process, we can also show that our
approach admits a superior rate of convergence to the expected loss minimizer
than is possible with standard stochastic gradient descent. In experiments with
real-world datasets, our formulation often performs better than
state-of-the-art stochastic gradient descent and online boosting algorithms.
",1.0
412,4,133034,2010.01625,pre-trained language model,"Sheena Panthaplackel, Junyi Jessy Li, Milos Gligoric, Raymond J.
  Mooney","Deep Just-In-Time Inconsistency Detection Between Comments and Source
  Code","  Natural language comments convey key aspects of source code such as
implementation, usage, and pre- and post-conditions. Failure to update comments
accordingly when the corresponding code is modified introduces inconsistencies,
which is known to lead to confusion and software bugs. In this paper, we aim to
detect whether a comment becomes inconsistent as a result of changes to the
corresponding body of code, in order to catch potential inconsistencies
just-in-time, i.e., before they are committed to a code base. To achieve this,
we develop a deep-learning approach that learns to correlate a comment with
code changes. By evaluating on a large corpus of comment/code pairs spanning
various comment types, we show that our model outperforms multiple baselines by
significant margins. For extrinsic evaluation, we show the usefulness of our
approach by combining it with a comment update model to build a more
comprehensive automatic comment maintenance system which can both detect and
resolve inconsistent comments based on code changes.
",4.0
413,15,84120,1903.04263,relevance feedback for imformation retrieval,"Shubhra Kanti Karmaker Santu, Parikshit Sondhi, ChengXiang Zhai",On Application of Learning to Rank for E-Commerce Search,"  E-Commerce (E-Com) search is an emerging important new application of
information retrieval. Learning to Rank (LETOR) is a general effective strategy
for optimizing search engines, and is thus also a key technology for E-Com
search. While the use of LETOR for web search has been well studied, its use
for E-Com search has not yet been well explored. In this paper, we discuss the
practical challenges in applying learning to rank methods to E-Com search,
including the challenges in feature representation, obtaining reliable
relevance judgments, and optimally exploiting multiple user feedback signals
such as click rates, add-to-cart ratios, order rates, and revenue. We study
these new challenges using experiments on industry data sets and report several
interesting findings that can provide guidance on how to optimally apply LETOR
to E-Com search: First, popularity-based features defined solely on product
items are very useful and LETOR methods were able to effectively optimize their
combination with relevance-based features. Second, query attribute sparsity
raises challenges for LETOR, and selecting features to reduce/avoid sparsity is
beneficial. Third, while crowdsourcing is often useful for obtaining relevance
judgments for Web search, it does not work as well for E-Com search due to
difficulty in eliciting sufficiently fine grained relevance judgments. Finally,
among the multiple feedback signals, the order rate is found to be the most
robust training objective, followed by click rate, while add-to-cart ratio
seems least robust, suggesting that an effective practical strategy may be to
initially use click rates for training and gradually shift to using order rates
as they become available.
",3.0
414,1,201486,2206.12926,advanced search engine,"Teddy Lazebnik, Hanna Weitman, Yoav Goldberg, Gal A. Kaminka",Rivendell: Project-Based Academic Search Engine,"  Finding relevant research literature in online databases is a familiar
challenge to all researchers. General search approaches trying to tackle this
challenge fall into two groups: one-time search and life-time search. We
observe that both approaches ignore unique attributes of the research domain
and are affected by concept drift. We posit that in searching for research
papers, a combination of a life-time search engine with an explicitly-provided
context (project) provides a solution to the concept drift problem. We
developed and deployed a project-based meta-search engine for research papers
called Rivendell. Using Rivendell, we conducted experiments with 199 subjects,
comparing project-based search performance to one-time and life-time search
engines, revealing an improvement of up to 12.8 percent in project-based search
compared to life-time search.
",5.0
415,11,152934,2104.05369,PageRank for web search,Jos\'e Devezas and S\'ergio Nunes,Fatigued PageRank,"  Connections among entities are everywhere. From social media interactions to
web page hyperlinks, networks are frequently used to represent such complex
systems. Node ranking is a fundamental task that provides the strategy to
identify central entities according to multiple criteria. Popular node ranking
metrics include degree, closeness or betweenness centralities, as well as HITS
authority or PageRank. In this work, we propose a novel node ranking metric,
where we combine PageRank and the idea of node fatigue, in order to model a
random explorer who wants to optimize coverage - it gets fatigued and avoids
previously visited nodes. We formalize and exemplify the computation of
Fatigued PageRank, evaluating it as a node ranking metric, as well as
query-independent evidence in ad hoc document retrieval. Based on the Simple
English Wikipedia link graph with clickstream transitions from the English
Wikipedia, we find that Fatigued PageRank is able to surpass both indegree and
HITS authority, but only for the top ranking nodes. On the other hand, based on
the TREC Washington Post Corpus, we were unable to outperform the BM25
baseline, obtaining similar performance for all graph-based metrics, except for
indegree, which lowered GMAP and MAP, but increased NDCG@10 and P@10.
",5.0
416,15,217624,cs/0507069,relevance feedback for imformation retrieval,"Jovan Pehcevski (RMIT), James A. Thom (RMIT), Anne-Marie Vercoustre","Users and Assessors in the Context of INEX: Are Relevance Dimensions
  Relevant?","  The main aspects of XML retrieval are identified by analysing and comparing
the following two behaviours: the behaviour of the assessor when judging the
relevance of returned document components; and the behaviour of users when
interacting with components of XML documents. We argue that the two INEX
relevance dimensions, Exhaustivity and Specificity, are not orthogonal
dimensions; indeed, an empirical analysis of each dimension reveals that the
grades of the two dimensions are correlated to each other. By analysing the
level of agreement between the assessor and the users, we aim at identifying
the best units of retrieval. The results of our analysis show that the highest
level of agreement is on highly relevant and on non-relevant document
components, suggesting that only the end points of the INEX 10-point relevance
scale are perceived in the same way by both the assessor and the users. We
propose a new definition of relevance for XML retrieval and argue that its
corresponding relevance scale would be a better choice for INEX.
",3.0
417,15,74940,1810.03519,relevance feedback for imformation retrieval,"Fl\'avio Martins, Jo\~ao Magalh\~aes, Jamie Callan",A Vertical PRF Architecture for Microblog Search,"  In microblog retrieval, query expansion can be essential to obtain good
search results due to the short size of queries and posts. Since information in
microblogs is highly dynamic, an up-to-date index coupled with pseudo-relevance
feedback (PRF) with an external corpus has a higher chance of retrieving more
relevant documents and improving ranking. In this paper, we focus on the
research question:how can we reduce the query expansion computational cost
while maintaining the same retrieval precision as standard PRF? Therefore, we
propose to accelerate the query expansion step of pseudo-relevance feedback.
The hypothesis is that using an expansion corpus organized into verticals for
expanding the query, will lead to a more efficient query expansion process and
improved retrieval effectiveness. Thus, the proposed query expansion method
uses a distributed search architecture and resource selection algorithms to
provide an efficient query expansion process. Experiments on the TREC Microblog
datasets show that the proposed approach can match or outperform standard PRF
in MAP and NDCG@30, with a computational cost that is three orders of magnitude
lower.
",3.0
418,6,136676,2011.00565,query expansion for imformation retrieval,"Muntaha Iqbal, Kamran Amjad, Bilal Tahir, Muhammad Amir Mehmood",CURE: Collection for Urdu Information Retrieval Evaluation and Ranking,"  Urdu is a widely spoken language with 163 million speakers worldwide across
the globe. Information Retrieval (IR) for Urdu entails special consideration of
research community due to its rich morphological features and a large number of
speakers. In general, IR evaluation task is not extensively explored for Urdu.
The most important missing element is the availability of a standardized
evaluation corpus specific to Urdu. In this research work, we propose and
construct a standard test collection of Urdu documents for IR evaluation and
named it Collection for Urdu Retrieval Evaluation (CURE). We select 1,096
unique documents against 50 diverse queries from a large collection of 0.5
million crawled documents using two IR models. The purpose of test collection
is the evaluation of IR models, ranking algorithms, and different natural
language processing techniques. Next, we perform binary relevance judgment on
the selected documents. We also built two other language resources for
lemmatization and query expansion specific to our test collection. Evaluation
of test collection is carried out using four retrieval models as well using the
stop-words list, lemmatization, and query expansion. Furthermore, error
analysis was performed for each query with different NLP techniques. To the
best of our knowledge, this work is the first attempt for preparing a
standardized information retrieval evaluation test collection for the Urdu
language.
",3.0
419,4,208387,2209.01515,pre-trained language model,"Sean Trott, Cameron Jones, Tyler Chang, James Michaelov, Benjamin
  Bergen",Do Large Language Models know what humans know?,"  Humans can attribute mental states to others, a capacity known as Theory of
Mind. However, it is unknown to what extent this ability results from an innate
biological endowment or from experience accrued through child development,
particularly exposure to language describing others' mental states. We test the
viability of the language exposure hypothesis by assessing whether models
exposed to large quantities of human language develop evidence of Theory of
Mind. In pre-registered analyses, we present a linguistic version of the False
Belief Task, widely used to assess Theory of Mind, to both human participants
and a state-of-the-art Large Language Model, GPT-3. Both are sensitive to
others' beliefs, but while the language model significantly exceeds chance
behavior, it does not perform as well as the humans, nor does it explain the
full extent of their behavior -- despite being exposed to more language than a
human would in a lifetime. This suggests that while statistical learning from
language exposure may in part explain how humans develop Theory of Mind, other
mechanisms are also responsible.
",5.0
420,18,36531,1509.06553,infomation retrieval time complexity,"Vidyadhar Rao, Prateek Jain, C.V Jawahar",Diverse Yet Efficient Retrieval using Hash Functions,"  Typical retrieval systems have three requirements: a) Accurate retrieval
i.e., the method should have high precision, b) Diverse retrieval, i.e., the
obtained set of points should be diverse, c) Retrieval time should be small.
However, most of the existing methods address only one or two of the above
mentioned requirements. In this work, we present a method based on randomized
locality sensitive hashing which tries to address all of the above requirements
simultaneously. While earlier hashing approaches considered approximate
retrieval to be acceptable only for the sake of efficiency, we argue that one
can further exploit approximate retrieval to provide impressive trade-offs
between accuracy and diversity. We extend our method to the problem of
multi-label prediction, where the goal is to output a diverse and accurate set
of labels for a given document in real-time. Moreover, we introduce a new
notion to simultaneously evaluate a method's performance for both the precision
and diversity measures. Finally, we present empirical results on several
different retrieval tasks and show that our method retrieves diverse and
accurate images/labels while ensuring $100x$-speed-up over the existing diverse
retrieval approaches.
",3.0
421,16,140305,2012.03351,activation function in neutral networks,Felix Voigtlaender,The universal approximation theorem for complex-valued neural networks,"  We generalize the classical universal approximation theorem for neural
networks to the case of complex-valued neural networks. Precisely, we consider
feedforward networks with a complex activation function $\sigma : \mathbb{C}
\to \mathbb{C}$ in which each neuron performs the operation $\mathbb{C}^N \to
\mathbb{C}, z \mapsto \sigma(b + w^T z)$ with weights $w \in \mathbb{C}^N$ and
a bias $b \in \mathbb{C}$, and with $\sigma$ applied componentwise. We
completely characterize those activation functions $\sigma$ for which the
associated complex networks have the universal approximation property, meaning
that they can uniformly approximate any continuous function on any compact
subset of $\mathbb{C}^d$ arbitrarily well.
  Unlike the classical case of real networks, the set of ""good activation
functions"" which give rise to networks with the universal approximation
property differs significantly depending on whether one considers deep networks
or shallow networks: For deep networks with at least two hidden layers, the
universal approximation property holds as long as $\sigma$ is neither a
polynomial, a holomorphic function, or an antiholomorphic function. Shallow
networks, on the other hand, are universal if and only if the real part or the
imaginary part of $\sigma$ is not a polyharmonic function.
",4.0
422,1,44084,1607.01869,advanced search engine,"Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio
  Silvestri, Ricardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, Gavin
  Owens","Scalable Semantic Matching of Queries to Ads in Sponsored Search
  Advertising","  Sponsored search represents a major source of revenue for web search engines.
This popular advertising model brings a unique possibility for advertisers to
target users' immediate intent communicated through a search query, usually by
displaying their ads alongside organic search results for queries deemed
relevant to their products or services. However, due to a large number of
unique queries it is challenging for advertisers to identify all such relevant
queries. For this reason search engines often provide a service of advanced
matching, which automatically finds additional relevant queries for advertisers
to bid on. We present a novel advanced matching approach based on the idea of
semantic embeddings of queries and ads. The embeddings were learned using a
large data set of user search sessions, consisting of search queries, clicked
ads and search links, while utilizing contextual information such as dwell time
and skipped ads. To address the large-scale nature of our problem, both in
terms of data and vocabulary size, we propose a novel distributed algorithm for
training of the embeddings. Finally, we present an approach for overcoming a
cold-start problem associated with new ads and queries. We report results of
editorial evaluation and online tests on actual search traffic. The results
show that our approach significantly outperforms baselines in terms of
relevance, coverage, and incremental revenue. Lastly, we open-source learned
query embeddings to be used by researchers in computational advertising and
related fields.
",5.0
423,11,20078,1305.0939,PageRank for web search,"Debajyoti Mukhopadhyay, Manoj Sharma, Gajanan Joshi, Trupti Pagare,
  Adarsha Palwe",Intelligent Agent Based Semantic Web in Cloud Computing Environment,"  Considering today's web scenario, there is a need of effective and meaningful
search over the web which is provided by Semantic Web. Existing search engines
are keyword based. They are vulnerable in answering intelligent queries from
the user due to the dependence of their results on information available in web
pages. While semantic search engines provides efficient and relevant results as
the semantic web is an extension of the current web in which information is
given well defined meaning. MetaCrawler is a search tool that uses several
existing search engines and provides combined results by using their own page
ranking algorithm. This paper proposes development of a meta-semantic-search
engine called SemanTelli which works within cloud. SemanTelli fetches results
from different semantic search engines such as Hakia, DuckDuckGo, SenseBot with
the help of intelligent agents that eliminate the limitations of existing
search engines.
",1.0
424,3,218739,cs/0701158,database management system,Jim Gray,Queues Are Databases,"  Message-oriented-middleware (MOM) has become an small industry. MOM offers
queued transaction processing as an advance over pure client-server transaction
processing. This note makes four points: Queued transaction processing is less
general than direct transaction processing. Queued systems are built on top of
direct systems. You cannot build a direct system atop a queued system. It is
difficult to build direct, conversational, or distributed transactions atop a
queued system. Queues are interesting databases with interesting concurrency
control. It is best to build these mechanisms into a standard database system
so other applications can use these interesting features. Queue systems need
DBMS functionality. Queues need security, configuration, performance
monitoring, recovery, and reorganization utilities. Database systems already
have these features. A full-function MOM system duplicates these database
features. Queue managers are simple TP-monitors managing server pools driven by
queues. Database systems are encompassing many server pool features as they
evolve to TP-lite systems.
",3.0
425,1,16706,1212.3906,advanced search engine,Mahyuddin K. M. Nasution,Simple Search Engine Model: Adaptive Properties,"  In this paper we study the relationship between query and search engine by
exploring the adaptive properties based on a simple search engine. We used set
theory and utilized the words and terms for defining singleton space of event
in a search engine model, and then provided the inclusion between one singleton
to another.
",4.0
426,8,77988,1811.10797,node embedding for graph,Dimitris Berberidis and Georgios B. Giannakis,"Node Embedding with Adaptive Similarities for Scalable Learning over
  Graphs","  Node embedding is the task of extracting informative and descriptive features
over the nodes of a graph. The importance of node embeddings for graph
analytics, as well as learning tasks such as node classification, link
prediction and community detection, has led to increased interest on the
problem leading to a number of recent advances. Much like PCA in the feature
domain, node embedding is an inherently \emph{unsupervised} task; in lack of
metadata used for validation, practical methods may require standardization and
limiting the use of tunable hyperparameters. Finally, node embedding methods
are faced with maintaining scalability in the face of large-scale real-world
graphs of ever-increasing sizes. In the present work, we propose an adaptive
node embedding framework that adjusts the embedding process to a given
underlying graph, in a fully unsupervised manner. To achieve this, we adopt the
notion of a tunable node similarity matrix that assigns weights on paths of
different length. The design of the multilength similarities ensures that the
resulting embeddings also inherit interpretable spectral properties. The
proposed model is carefully studied, interpreted, and numerically evaluated
using stochastic block models. Moreover, an algorithmic scheme is proposed for
training the model parameters effieciently and in an unsupervised manner. We
perform extensive node classification, link prediction, and clustering
experiments on many real world graphs from various domains, and compare with
state-of-the-art scalable and unsupervised node embedding alternatives. The
proposed method enjoys superior performance in many cases, while also yielding
interpretable information on the underlying structure of the graph.
",5.0
427,9,200816,2206.10128,language model for long documents,"Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido
  Zuccon, and Daxin Jiang","Bridging the Gap Between Indexing and Retrieval for Differentiable
  Search Index with Query Generation","  The Differentiable Search Index (DSI) is a new, emerging paradigm for
information retrieval. Unlike traditional retrieval architectures where index
and retrieval are two different and separate components, DSI uses a single
transformer model to perform both indexing and retrieval. In this paper, we
identify and tackle an important issue of current DSI models: the data
distribution mismatch that occurs between the DSI indexing and retrieval
processes. Specifically, we argue that, at indexing, current DSI methods learn
to build connections between long document texts and their identifies, but then
at retrieval, short query texts are provided to DSI models to perform the
retrieval of the document identifiers. This problem is further exacerbated when
using DSI for cross-lingual retrieval, where document text and query text are
in different languages. To address this fundamental problem of current DSI
models we propose a simple yet effective indexing framework for DSI called
DSI-QG. In DSI-QG, documents are represented by a number of relevant queries
generated by a query generation model at indexing time. This allows DSI models
to connect a document identifier to a set of query texts when indexing, hence
mitigating data distribution mismatches present between the indexing and the
retrieval phases. Empirical results on popular mono-lingual and cross-lingual
passage retrieval benchmark datasets show that DSI-QG significantly outperforms
the original DSI model.
",1.0
428,7,181527,2201.00703,gradient boosting,"Qixin Zhang, Zengde Deng, Zaiyi Chen, Haoyuan Hu, Yu Yang","Stochastic Continuous Submodular Maximization: Boosting via
  Non-oblivious Function","  In this paper, we revisit Stochastic Continuous Submodular Maximization in
both offline and online settings, which can benefit wide applications in
machine learning and operations research areas. We present a boosting framework
covering gradient ascent and online gradient ascent. The fundamental ingredient
of our methods is a novel non-oblivious function $F$ derived from a
factor-revealing optimization problem, whose any stationary point provides a
$(1-e^{-\gamma})$-approximation to the global maximum of the $\gamma$-weakly
DR-submodular objective function $f\in C^{1,1}_L(\mathcal{X})$. Under the
offline scenario, we propose a boosting gradient ascent method achieving
$(1-e^{-\gamma}-\epsilon^{2})$-approximation after $O(1/\epsilon^2)$
iterations, which improves the $(\frac{\gamma^2}{1+\gamma^2})$ approximation
ratio of the classical gradient ascent algorithm. In the online setting, for
the first time we consider the adversarial delays for stochastic gradient
feedback, under which we propose a boosting online gradient algorithm with the
same non-oblivious function $F$. Meanwhile, we verify that this boosting online
algorithm achieves a regret of $O(\sqrt{D})$ against a
$(1-e^{-\gamma})$-approximation to the best feasible solution in hindsight,
where $D$ is the sum of delays of gradient feedback. To the best of our
knowledge, this is the first result to obtain $O(\sqrt{T})$ regret against a
$(1-e^{-\gamma})$-approximation with $O(1)$ gradient inquiry at each time step,
when no delay exists, i.e., $D=T$. Finally, numerical experiments demonstrate
the effectiveness of our boosting methods.
",1.0
429,8,57935,1709.07604,node embedding for graph,"Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang","A Comprehensive Survey of Graph Embedding: Problems, Techniques and
  Applications","  Graph is an important data representation which appears in a wide diversity
of real-world scenarios. Effective graph analytics provides users a deeper
understanding of what is behind the data, and thus can benefit a lot of useful
applications such as node classification, node recommendation, link prediction,
etc. However, most graph analytics methods suffer the high computation and
space cost. Graph embedding is an effective yet efficient way to solve the
graph analytics problem. It converts the graph data into a low dimensional
space in which the graph structural information and graph properties are
maximally preserved. In this survey, we conduct a comprehensive review of the
literature in graph embedding. We first introduce the formal definition of
graph embedding as well as the related concepts. After that, we propose two
taxonomies of graph embedding which correspond to what challenges exist in
different graph embedding problem settings and how the existing work address
these challenges in their solutions. Finally, we summarize the applications
that graph embedding enables and suggest four promising future research
directions in terms of computation efficiency, problem settings, techniques and
application scenarios.
",5.0
430,16,176610,2111.07683,activation function in neutral networks,Pierre-Jean Meyer,Reachability analysis of neural networks using mixed monotonicity,"  This paper presents a new reachability analysis approach to compute interval
over-approximations of the output set of feedforward neural networks with input
uncertainty. We adapt to neural networks an existing mixed-monotonicity method
for the reachability analysis of dynamical systems and apply it to each partial
network within the main network. This ensures that the intersection of the
obtained results is the tightest interval over-approximation of the output of
each layer that can be obtained using mixed-monotonicity on any partial network
decomposition. Unlike other tools in the literature focusing on small classes
of piecewise-affine or monotone activation functions, the main strength of our
approach is its generality: it can handle neural networks with any
Lipschitz-continuous activation function. In addition, the simplicity of our
framework allows users to very easily add unimplemented activation functions,
by simply providing the function, its derivative and the global argmin and
argmax of the derivative. Our algorithm is compared to five other
interval-based tools (Interval Bound Propagation, ReluVal, Neurify, VeriNet,
CROWN) on both existing benchmarks and two sets of small and large randomly
generated networks for four activation functions (ReLU, TanH, ELU, SiLU).
",3.0
431,14,199804,2206.06383,text summarization model,"Vivian Lai, Alison Smith-Renner, Ke Zhang, Ruijia Cheng, Wenjuan
  Zhang, Joel Tetreault, Alejandro Jaimes",An Exploration of Post-Editing Effectiveness in Text Summarization,"  Automatic summarization methods are efficient but can suffer from low
quality. In comparison, manual summarization is expensive but produces higher
quality. Can humans and AI collaborate to improve summarization performance? In
similar text generation tasks (e.g., machine translation), human-AI
collaboration in the form of ""post-editing"" AI-generated text reduces human
workload and improves the quality of AI output. Therefore, we explored whether
post-editing offers advantages in text summarization. Specifically, we
conducted an experiment with 72 participants, comparing post-editing provided
summaries with manual summarization for summary quality, human efficiency, and
user experience on formal (XSum news) and informal (Reddit posts) text. This
study sheds valuable insights on when post-editing is useful for text
summarization: it helped in some cases (e.g., when participants lacked domain
knowledge) but not in others (e.g., when provided summaries include inaccurate
information). Participants' different editing strategies and needs for
assistance offer implications for future human-AI summarization systems.
",4.0
432,19,138618,2011.09843,artificial intelligence for low carbon,"S. Sairam, Seshadhri Srinivasan, G. Marafioti, B. Subathra, G.
  Mathisen, and Korkut Bekiroglu",Explainable Incipient Fault Detection Systems for Photovoltaic Panels,"  This paper presents an eXplainable Fault Detection and Diagnosis System
(XFDDS) for incipient faults in PV panels. The XFDDS is a hybrid approach that
combines the model-based and data-driven framework. Model-based FDD for PV
panels lacks high fidelity models at low irradiance conditions for detecting
incipient faults. To overcome this, a novel irradiance based three diode model
(IB3DM) is proposed. It is a nine parameter model that provides higher accuracy
even at low irradiance conditions, an important aspect for detecting incipient
faults from noise. To exploit PV data, extreme gradient boosting (XGBoost) is
used due to its ability to detecting incipient faults. Lack of explainability,
feature variability for sample instances, and false alarms are challenges with
data-driven FDD methods. These shortcomings are overcome by hybridization of
XGBoost and IB3DM, and using eXplainable Artificial Intelligence (XAI)
techniques. To combine the XGBoost and IB3DM, a fault-signature metric is
proposed that helps reducing false alarms and also trigger an explanation on
detecting incipient faults. To provide explainability, an eXplainable
Artificial Intelligence (XAI) application is developed. It uses the local
interpretable model-agnostic explanations (LIME) framework and provides
explanations on classifier outputs for data instances. These explanations help
field engineers/technicians for performing troubleshooting and maintenance
operations. The proposed XFDDS is illustrated using experiments on different PV
technologies and our results demonstrate the perceived benefits.
",3.0
433,7,176099,2111.05177,gradient boosting,"Zhengyang Geng and Xin-Yu Zhang and Shaojie Bai and Yisen Wang and
  Zhouchen Lin",On Training Implicit Models,"  This paper focuses on training implicit models of infinite layers.
Specifically, previous works employ implicit differentiation and solve the
exact gradient for the backward propagation. However, is it necessary to
compute such an exact but expensive gradient for training? In this work, we
propose a novel gradient estimate for implicit models, named phantom gradient,
that 1) forgoes the costly computation of the exact gradient; and 2) provides
an update direction empirically preferable to the implicit model training. We
theoretically analyze the condition under which an ascent direction of the loss
landscape could be found, and provide two specific instantiations of the
phantom gradient based on the damped unrolling and Neumann series. Experiments
on large-scale tasks demonstrate that these lightweight phantom gradients
significantly accelerate the backward passes in training implicit models by
roughly 1.7 times, and even boost the performance over approaches based on the
exact gradient on ImageNet.
",1.0
434,9,119091,2005.09069,language model for long documents,"Vivek Gupta, Ankit Saw, Pegah Nokhiz, Praneeth Netrapalli, Piyush Rai,
  Partha Talukdar",P-SIF: Document Embeddings Using Partition Averaging,"  Simple weighted averaging of word vectors often yields effective
representations for sentences which outperform sophisticated seq2seq neural
models in many tasks. While it is desirable to use the same method to represent
documents as well, unfortunately, the effectiveness is lost when representing
long documents involving multiple sentences. One of the key reasons is that a
longer document is likely to contain words from many different topics; hence,
creating a single vector while ignoring all the topical structure is unlikely
to yield an effective document representation. This problem is less acute in
single sentences and other short text fragments where the presence of a single
topic is most likely. To alleviate this problem, we present P-SIF, a
partitioned word averaging model to represent long documents. P-SIF retains the
simplicity of simple weighted word averaging while taking a document's topical
structure into account. In particular, P-SIF learns topic-specific vectors from
a document and finally concatenates them all to represent the overall document.
We provide theoretical justifications on the correctness of P-SIF. Through a
comprehensive set of experiments, we demonstrate P-SIF's effectiveness compared
to simple weighted averaging and many other baselines.
",5.0
435,14,34829,1507.00209,text summarization model,Hai Zhuge,Dimensionality on Summarization,"  Summarization is one of the key features of human intelligence. It plays an
important role in understanding and representation. With rapid and continual
expansion of texts, pictures and videos in cyberspace, automatic summarization
becomes more and more desirable. Text summarization has been studied for over
half century, but it is still hard to automatically generate a satisfied
summary. Traditional methods process texts empirically and neglect the
fundamental characteristics and principles of language use and understanding.
This paper summarizes previous text summarization approaches in a
multi-dimensional classification space, introduces a multi-dimensional
methodology for research and development, unveils the basic characteristics and
principles of language use and understanding, investigates some fundamental
mechanisms of summarization, studies the dimensions and forms of
representations, and proposes a multi-dimensional evaluation mechanisms.
Investigation extends to the incorporation of pictures into summary and to the
summarization of videos, graphs and pictures, and then reaches a general
summarization framework.
",5.0
436,0,23381,1312.0032,learning to rank with partitioned preference,"Thomas Lukasiewicz, Maria Vanina Martinez, Cristian Molinaro, Livia
  Predoiu, and Gerardo I. Simari","Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports
  (Technical Report)","  The use of preferences in query answering, both in traditional databases and
in ontology-based data access, has recently received much attention, due to its
many real-world applications. In this paper, we tackle the problem of top-k
query answering in Datalog+/- ontologies subject to the querying user's
preferences and a collection of (subjective) reports of other users. Here, each
report consists of scores for a list of features, its author's preferences
among the features, as well as other information. Theses pieces of information
of every report are then combined, along with the querying user's preferences
and his/her trust into each report, to rank the query results. We present two
alternative such rankings, along with algorithms for top-k (atomic) query
answering under these rankings. We also show that, under suitable assumptions,
these algorithms run in polynomial time in the data complexity. We finally
present more general reports, which are associated with sets of atoms rather
than single atoms.
",1.0
437,4,197478,2205.13636,pre-trained language model,"Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter
  West, Prithviraj Ammanabrolu, Yejin Choi",Quark: Controllable Text Generation with Reinforced Unlearning,"  Large-scale language models often learn behaviors that are misaligned with
user expectations. Generated text may contain offensive or toxic language,
contain significant repetition, or be of a different sentiment than desired by
the user. We consider the task of unlearning these misalignments by fine-tuning
the language model on signals of what not to do. We introduce Quantized Reward
Konditioning (Quark), an algorithm for optimizing a reward function that
quantifies an (un)wanted property, while not straying too far from the original
model. Quark alternates between (i) collecting samples with the current
language model, (ii) sorting them into quantiles based on reward, with each
quantile identified by a reward token prepended to the language model's input,
and (iii) using a standard language modeling loss on samples from each quantile
conditioned on its reward token, while remaining nearby the original language
model via a KL-divergence penalty. By conditioning on a high-reward token at
generation time, the model generates text that exhibits less of the unwanted
property. For unlearning toxicity, negative sentiment, and repetition, our
experiments show that Quark outperforms both strong baselines and
state-of-the-art reinforcement learning methods like PPO (Schulman et al.
2017), while relying only on standard language modeling primitives.
",3.0
438,5,87039,1904.12058,matrix completion,"Muhan Zhang, Yixin Chen",Inductive Matrix Completion Based on Graph Neural Networks,"  We propose an inductive matrix completion model without using side
information. By factorizing the (rating) matrix into the product of
low-dimensional latent embeddings of rows (users) and columns (items), a
majority of existing matrix completion methods are transductive, since the
learned embeddings cannot generalize to unseen rows/columns or to new matrices.
To make matrix completion inductive, most previous works use content (side
information), such as user's age or movie's genre, to make predictions.
However, high-quality content is not always available, and can be hard to
extract. Under the extreme setting where not any side information is available
other than the matrix to complete, can we still learn an inductive matrix
completion model? In this paper, we propose an Inductive Graph-based Matrix
Completion (IGMC) model to address this problem. IGMC trains a graph neural
network (GNN) based purely on 1-hop subgraphs around (user, item) pairs
generated from the rating matrix and maps these subgraphs to their
corresponding ratings. It achieves highly competitive performance with
state-of-the-art transductive baselines. In addition, IGMC is inductive -- it
can generalize to users/items unseen during the training (given that their
interactions exist), and can even transfer to new tasks. Our transfer learning
experiments show that a model trained out of the MovieLens dataset can be
directly used to predict Douban movie ratings with surprisingly good
performance. Our work demonstrates that: 1) it is possible to train inductive
matrix completion models without using side information while achieving similar
or better performances than state-of-the-art transductive methods; 2) local
graph patterns around a (user, item) pair are effective predictors of the
rating this user gives to the item; and 3) Long-range dependencies might not be
necessary for modeling recommender systems.
",5.0
439,6,191043,2203.1623,query expansion for imformation retrieval,Lorenzo Massai,"Automatic generation of semantic corpora for improving intent estimation
  of taxonomy-driven search engines","  With the increasing demand of intelligent systems capable of operating in
different user contexts (e.g. users on the move) the correct interpretation of
the user-need by such systems has become crucial to give a consistent answer to
the user query. The most effective techniques which are used to address such
task are in the fields of natural language processing and semantic expansion of
terms. Such systems are aimed at estimating the actual meaning of input
queries, addressing the concepts of the words which are expressed within the
user questions. The aim of this paper is to demonstrate which semantic relation
impacts the most in semantic expansion-based retrieval systems and to identify
the best tradeoff between accuracy and noise introduction when combining such
relations. The evaluations are made building a simple natural language
processing system capable of querying any taxonomy-driven domain, making use of
the combination of different semantic expansions as knowledge resources. The
proposed evaluation employs a wide and varied taxonomy as a use-case,
exploiting its labels as basis for the expansions. To build the knowledge
resources several corpora have been produced and integrated as gazetteers into
the NLP infrastructure with the purpose of estimating the pseudo-queries
corresponding to the taxonomy labels, considered as the possible intents.
",5.0
440,0,126111,2007.09079,learning to rank with partitioned preference,"Hadi Hosseini, Vijay Menon, Nisarg Shah, Sujoy Sikdar",Necessarily Optimal One-Sided Matchings,"  We study the classical problem of matching $n$ agents to $n$ objects, where
the agents have ranked preferences over the objects. We focus on two popular
desiderata from the matching literature: Pareto optimality and rank-maximality.
Instead of asking the agents to report their complete preferences, our goal is
to learn a desirable matching from partial preferences, specifically a matching
that is necessarily Pareto optimal (NPO) or necessarily rank-maximal (NRM)
under any completion of the partial preferences. We focus on the top-$k$ model
in which agents reveal a prefix of their preference rankings. We design
efficient algorithms to check if a given matching is NPO or NRM, and to check
whether such a matching exists given top-$k$ partial preferences. We also study
online algorithms for eliciting partial preferences adaptively, and prove
bounds on their competitive ratio.
",3.0
441,3,183496,2201.10442,database management system,"Lixi Zhou, Jiaqing Chen, Amitabh Das, Hong Min, Lei Yu, Ming Zhao, Jia
  Zou","Serving Deep Learning Models with Deduplication from Relational
  Databases","  There are significant benefits to serve deep learning models from relational
databases. First, features extracted from databases do not need to be
transferred to any decoupled deep learning systems for inferences, and thus the
system management overhead can be significantly reduced. Second, in a
relational database, data management along the storage hierarchy is fully
integrated with query processing, and thus it can continue model serving even
if the working set size exceeds the available memory. Applying model
deduplication can greatly reduce the storage space, memory footprint, cache
misses, and inference latency. However, existing data deduplication techniques
are not applicable to the deep learning model serving applications in
relational databases. They do not consider the impacts on model inference
accuracy as well as the inconsistency between tensor blocks and database pages.
This work proposed synergistic storage optimization techniques for duplication
detection, page packing, and caching, to enhance database systems for model
serving. We implemented the proposed approach in netsDB, an object-oriented
relational database. Evaluation results show that our proposed techniques
significantly improved the storage efficiency and the model inference latency,
and serving models from relational databases outperformed existing deep
learning frameworks when the working set size exceeds available memory.
",3.0
442,5,84344,1903.06055,matrix completion,"Yicong He, Fei Wang, Yingsong Li, Jing Qin, Badong Chen","Robust Matrix Completion via Maximum Correntropy Criterion and Half
  Quadratic Optimization","  Robust matrix completion aims to recover a low-rank matrix from a subset of
noisy entries perturbed by complex noises, where traditional methods for matrix
completion may perform poorly due to utilizing $l_2$ error norm in
optimization. In this paper, we propose a novel and fast robust matrix
completion method based on maximum correntropy criterion (MCC). The correntropy
based error measure is utilized instead of using $l_2$-based error norm to
improve the robustness to noises. Using the half-quadratic optimization
technique, the correntropy based optimization can be transformed to a weighted
matrix factorization problem. Then, two efficient algorithms are derived,
including alternating minimization based algorithm and alternating gradient
descend based algorithm. The proposed algorithms do not need to calculate
singular value decomposition (SVD) at each iteration. Further, the adaptive
kernel selection strategy is proposed to accelerate the convergence speed as
well as improve the performance. Comparison with existing robust matrix
completion algorithms is provided by simulations, showing that the new methods
can achieve better performance than existing state-of-the-art algorithms.
",4.0
443,6,76971,1811.03514,query expansion for imformation retrieval,"Ayyoob Imani, Amir Vakili, Ali Montazer and Azadeh Shakery",Deep Neural Networks for Query Expansion using Word Embeddings,"  Query expansion is a method for alleviating the vocabulary mismatch problem
present in information retrieval tasks. Previous works have shown that terms
selected for query expansion by traditional methods such as pseudo-relevance
feedback are not always helpful to the retrieval process. In this paper, we
show that this is also true for more recently proposed embedding-based query
expansion methods. We then introduce an artificial neural network classifier to
predict the usefulness of query expansion terms. This classifier uses term word
embeddings as inputs. We perform experiments on four TREC newswire and web
collections show that using terms selected by the classifier for expansion
significantly improves retrieval performance when compared to competitive
baselines. The results are also shown to be more robust than the baselines.
",5.0
444,15,70492,1807.02299,relevance feedback for imformation retrieval,"Shihao Zou, Guanyu Tao, Jun Wang, Weinan Zhang, Dell Zhang",On the Equilibrium of Query Reformulation and Document Retrieval,"  In this paper, we study jointly query reformulation and document relevance
estimation, the two essential aspects of information retrieval (IR). Their
interactions are modelled as a two-player strategic game: one player, a query
formulator, taking actions to produce the optimal query, is expected to
maximize its own utility with respect to the relevance estimation of documents
produced by the other player, a retrieval modeler; simultaneously, the
retrieval modeler, taking actions to produce the document relevance scores,
needs to optimize its likelihood from the training data with respect to the
refined query produced by the query formulator. Their equilibrium or equilibria
will be reached when both are the best responses to each other. We derive our
equilibrium theory of IR using normal-form representations: when a standard
relevance feedback algorithm is coupled with a retrieval model, they would
share the same objective function and thus form a partnership game; by
contrast, pseudo relevance feedback pursues a rather different objective than
that of retrieval models, therefore the interaction between them would lead to
a general-sum game (though implicitly collaborative). Our game-theoretical
analyses not only yield useful insights into the two major aspects of IR, but
also offer new practical algorithms for achieving the equilibrium state of
retrieval which have been shown to bring consistent performance improvements in
both text retrieval and item recommendation.
",3.0
445,11,4800,1002.3342,PageRank for web search,"B. Georgeot, O. Giraud and D.L. Shepelyansky","Spectral properties of the Google matrix of the World Wide Web and other
  directed networks","  We study numerically the spectrum and eigenstate properties of the Google
matrix of various examples of directed networks such as vocabulary networks of
dictionaries and university World Wide Web networks. The spectra have gapless
structure in the vicinity of the maximal eigenvalue for Google damping
parameter $\alpha$ equal to unity. The vocabulary networks have relatively
homogeneous spectral density, while university networks have pronounced
spectral structures which change from one university to another, reflecting
specific properties of the networks. We also determine specific properties of
eigenstates of the Google matrix, including the PageRank. The fidelity of the
PageRank is proposed as a new characterization of its stability.
",3.0
446,16,209212,2209.06119,activation function in neutral networks,Ravin Kumar,"APTx: better activation function than MISH, SWISH, and ReLU's variants
  used in deep learning","  Activation Functions introduce non-linearity in the deep neural networks.
This nonlinearity helps the neural networks learn faster and efficiently from
the dataset. In deep learning, many activation functions are developed and used
based on the type of problem statement. ReLU's variants, SWISH, and MISH are
goto activation functions. MISH function is considered having similar or even
better performance than SWISH, and much better than ReLU. In this paper, we
propose an activation function named APTx which behaves similar to MISH, but
requires lesser mathematical operations to compute. The lesser computational
requirements of APTx does speed up the model training, and thus also reduces
the hardware requirement for the deep learning model.
",5.0
447,11,64875,1803.05001,PageRank for web search,"Lucas Farach-Colton, Martin Farach-Colton, Leslie Ann Goldberg, John
  Lapinskas, Reut Levi, Moti Medina and Miguel A. Mosteiro",Improved Distortion and Spam Resistance for PageRank,"  Ranking functions are an integral component of web searching, and are thus
subject to link spamming. Quite a lot of practical and theoretical knowledge
has accumulated about which types of ranking functions are susceptible to which
types of spam attacks, but there is no general characterization of the class of
spam attacks nor of the susceptibility of a ranking function to arbitrary spam
attacks.
  Our first contribution is a characterization of the class of spam attacks and
a definition of the spam resistance of a ranking function against all possible
link-spam attacks. We show how this formal notion of spam resistance matches
the intuition of practitioners. This definition allows us to evaluate novel
ranking functions.
  A natural way to resist spam is to design a ranking function that heavily
favors trusted nodes, which are nodes known not be compromised by the spammer.
However, this introduces local distortions into the ranking function. We
formalize this notion of distortion and show that it matches the intuitive
notion used by practitioners. We know of no ranking functions in the literature
that combine high spam resistance and low distortion; we consider several
well-known ranking functions (including the original form of PageRank) and show
that they fail on one or both counts.
  Finally, we introduce a new form of PageRank known as Min-PPR. We prove that
Min-PPR has low distortion and high spam resistance. A secondary benefit is
that Min-PPR comes with an explicit cost function on nodes that shows how
important they are to the spammer; thus a ranker can focus their spam-detection
capacity on these vulnerable nodes. Both Min-PPR and its associated cost
function are straightforward to compute.
",4.0
448,11,119000,2005.08591,PageRank for web search,"Nikitha Rao, Chetan Bansal, Subhabrata Mukherjee and Chandra Maddila",Product Insights: Analyzing Product Intents in Web Search,"  Web search engines are frequently used to access information about products.
This has increased in recent times with the rising popularity of e-commerce.
However, there is limited understanding of what users search for and their
intents when it comes to product search on the web. In this work, we study
search logs from Bing web search engine to characterize user intents and study
user behavior for product search. We propose a taxonomy of product intents by
analyzing product search queries. This is a challenging task given that only
15%-17% of web search queries are about products. We train machine learning
classifiers with query log features to classify queries based on intent with an
overall F1-score of 78%. We further analyze various characteristics of product
search queries in terms of search metrics like dwell time, success, popularity
and session-specific information.
",1.0
449,1,9735,1108.6016,advanced search engine,Jim Gemmell and Benjamin I. P. Rubinstein and Ashok K. Chandra,Improving Entity Resolution with Global Constraints,"  Some of the greatest advances in web search have come from leveraging
socio-economic properties of online user behavior. Past advances include
PageRank, anchor text, hubs-authorities, and TF-IDF. In this paper, we
investigate another socio-economic property that, to our knowledge, has not yet
been exploited: sites that create lists of entities, such as IMDB and Netflix,
have an incentive to avoid gratuitous duplicates. We leverage this property to
resolve entities across the different web sites, and find that we can obtain
substantial improvements in resolution accuracy. This improvement in accuracy
also translates into robustness, which often reduces the amount of training
data that must be labeled for comparing entities across many sites.
Furthermore, the technique provides robustness when resolving sites that have
some duplicates, even without first removing these duplicates. We present
algorithms with very strong precision and recall, and show that max weight
matching, while appearing to be a natural choice turns out to have poor
performance in some situations. The presented techniques are now being used in
the back-end entity resolution system at a major Internet search engine.
",4.0
450,4,55470,1707.03372,pre-trained language model,"Stephen Mussmann, Daniel Levy, Stefano Ermon","Fast Amortized Inference and Learning in Log-linear Models with Randomly
  Perturbed Nearest Neighbor Search","  Inference in log-linear models scales linearly in the size of output space in
the worst-case. This is often a bottleneck in natural language processing and
computer vision tasks when the output space is feasibly enumerable but very
large. We propose a method to perform inference in log-linear models with
sublinear amortized cost. Our idea hinges on using Gumbel random variable
perturbations and a pre-computed Maximum Inner Product Search data structure to
access the most-likely elements in sublinear amortized time. Our method yields
provable runtime and accuracy guarantees. Further, we present empirical
experiments on ImageNet and Word Embeddings showing significant speedups for
sampling, inference, and learning in log-linear models.
",2.0
451,7,117520,2005.00718,gradient boosting,"Zebang Zhang, Kui Zhao, Kai Huang, Quanhui Jia, Yanming Fang, Quan Yu","Large-scale Uncertainty Estimation and Its Application in Revenue
  Forecast of SMEs","  The economic and banking importance of the small and medium enterprise (SME)
sector is well recognized in contemporary society. Business credit loans are
very important for the operation of SMEs, and the revenue is a key indicator of
credit limit management. Therefore, it is very beneficial to construct a
reliable revenue forecasting model. If the uncertainty of an enterprise's
revenue forecasting can be estimated, a more proper credit limit can be
granted. Natural gradient boosting approach, which estimates the uncertainty of
prediction by a multi-parameter boosting algorithm based on the natural
gradient. However, its original implementation is not easy to scale into big
data scenarios, and computationally expensive compared to state-of-the-art
tree-based models (such as XGBoost). In this paper, we propose a Scalable
Natural Gradient Boosting Machines that is simple to implement, readily
parallelizable, interpretable and yields high-quality predictive uncertainty
estimates. According to the characteristics of revenue distribution, we derive
an uncertainty quantification function. We demonstrate that our method can
distinguish between samples that are accurate and inaccurate on revenue
forecasting of SMEs. What's more, interpretability can be naturally obtained
from the model, satisfying the financial needs.
",4.0
452,2,141825,2012.1079,random forests,"Mochen Yang, Edward McFowland III, Gordon Burtch and Gediminas
  Adomavicius","Achieving Reliable Causal Inference with Data-Mined Variables: A Random
  Forest Approach to the Measurement Error Problem","  Combining machine learning with econometric analysis is becoming increasingly
prevalent in both research and practice. A common empirical strategy involves
the application of predictive modeling techniques to 'mine' variables of
interest from available data, followed by the inclusion of those variables into
an econometric framework, with the objective of estimating causal effects.
Recent work highlights that, because the predictions from machine learning
models are inevitably imperfect, econometric analyses based on the predicted
variables are likely to suffer from bias due to measurement error. We propose a
novel approach to mitigate these biases, leveraging the ensemble learning
technique known as the random forest. We propose employing random forest not
just for prediction, but also for generating instrumental variables to address
the measurement error embedded in the prediction. The random forest algorithm
performs best when comprised of a set of trees that are individually accurate
in their predictions, yet which also make 'different' mistakes, i.e., have
weakly correlated prediction errors. A key observation is that these properties
are closely related to the relevance and exclusion requirements of valid
instrumental variables. We design a data-driven procedure to select tuples of
individual trees from a random forest, in which one tree serves as the
endogenous covariate and the other trees serve as its instruments. Simulation
experiments demonstrate the efficacy of the proposed approach in mitigating
estimation biases and its superior performance over three alternative methods
for bias correction.
",4.0
453,6,95222,1908.02938,query expansion for imformation retrieval,"Yue Yin, Chenyan Xiong, Cheng Luo, Zhiyuan Liu",Neural Document Expansion with User Feedback,"  This paper presents a neural document expansion approach (NeuDEF) that
enriches document representations for neural ranking models. NeuDEF harvests
expansion terms from queries which lead to clicks on the document and weights
these expansion terms with learned attention. It is plugged into a standard
neural ranker and learned end-to-end. Experiments on a commercial search log
demonstrate that NeuDEF significantly improves the accuracy of state-of-the-art
neural rankers and expansion methods on queries with different frequencies.
Further studies show the contribution of click queries and learned expansion
weights, as well as the influence of document popularity of NeuDEF's
effectiveness.
",3.0
454,4,24478,1401.4869,pre-trained language model,"Taraka Rama, Karthik Gali, Avinesh PVS",Does Syntactic Knowledge help English-Hindi SMT?,"  In this paper we explore various parameter settings of the state-of-art
Statistical Machine Translation system to improve the quality of the
translation for a `distant' language pair like English-Hindi. We proposed new
techniques for efficient reordering. A slight improvement over the baseline is
reported using these techniques. We also show that a simple pre-processing step
can improve the quality of the translation significantly.
",4.0
455,12,128358,2008.05271,COVID-19 and social media,Gillian Bolsover and Janet Tokitsu Tizon,Social Media and Health Misinformation during the US COVID Crisis,"  Health misinformation has been found to be prevalent on social media,
particularly in new public health crises in which there is limited scientific
information. However, social media can also play a role in limiting and
refuting health misinformation. Using as a case study US President Donald
Trump's controversial comments about the promise and power of UV light- and
disinfectant-based treatments, this data memo examines how these comments were
discussed and responded to on Twitter. We find that these comments fell into
established politically partisan narratives and dominated discussion of both
politics and COVID in the days following. Contestation of the comments was much
more prevalent than support. Supporters attacked media coverage in line with
existing Trump narratives. Contesters responded with humour and shared
mainstream media coverage condemning the comments. These practices would have
strengthened the original misinformation through repetition and done little to
construct a successful refutation for those who might have believed them. This
research adds much-needed knowledge to our understanding of the information
environment surrounding COVID and demonstrates that, despite calls for the
depoliticization of health information in this public health crisis, this is
largely being approached as a political issue along divisive, polarised,
partisan lines.
",5.0
456,11,80114,1901.00678,PageRank for web search,"Bo Song, Xiaobo Jiang, Xinhua Zhuang",Virtual Web Based Personalized PageRank Updating,"  Growing popularity of social networks demands a highly efficient Personalized
PageRank (PPR) updating due to the fast-evolving web graphs of enormous size.
While current researches are focusing on PPR updating under link structure
modification, efficiently updating PPR when node insertion/ deletion involved
remains a challenge. In the previous work called Virtual Web (VW), a few VW
architectures are designed, which results in some highly effective
initializations to significantly accelerate PageRank updating under both link
modification and page insertion/deletion. In the paper, under the general
scenario of link modification and node insertion/deletion we tackle the fast
PPR updating problem. Specifically, we combine VW with the TrackingPPR method
to generate initials, which are then used by the Gauss-Southwell method for
fast PPR updating. The algorithm is named VWPPR method. In extensive
experiments, three real-world datasets are used that contain 1~5.6M nodes and
6.7M~129M links, while a node perturbation of 40k and link perturbation of 1%
are applied. Comparing to the more recent LazyForwardUpdate method, which
handles the general PPR updating problem, the VWPPR method is 3~6 times faster
in terms of running time, or 4.4~10 times faster in terms of iteration numbers.
",4.0
457,19,192354,2204.05397,artificial intelligence for low carbon,"Xiou Ge, Richard T. Goodwin, Haizi Yu, Pablo Romero, Omar Abdelrahman,
  Amruta Sudhalkar, Julius Kusuma, Ryan Cialdella, Nishant Garg, and Lav R.
  Varshney","Accelerated Design and Deployment of Low-Carbon Concrete for Data
  Centers","  Concrete is the most widely used engineered material in the world with more
than 10 billion tons produced annually. Unfortunately, with that scale comes a
significant burden in terms of energy, water, and release of greenhouse gases
and other pollutants; indeed 8% of worldwide carbon emissions are attributed to
the production of cement, a key ingredient in concrete. As such, there is
interest in creating concrete formulas that minimize this environmental burden,
while satisfying engineering performance requirements including compressive
strength. Specifically for computing, concrete is a major ingredient in the
construction of data centers.
  In this work, we use conditional variational autoencoders (CVAEs), a type of
semi-supervised generative artificial intelligence (AI) model, to discover
concrete formulas with desired properties. Our model is trained just using a
small open dataset from the UCI Machine Learning Repository joined with
environmental impact data from standard lifecycle analysis. Computational
predictions demonstrate CVAEs can design concrete formulas with much lower
carbon requirements than existing formulations while meeting design
requirements. Next we report laboratory-based compressive strength experiments
for five AI-generated formulations, which demonstrate that the formulations
exceed design requirements. The resulting formulations were then used by Ozinga
Ready Mix -- a concrete supplier -- to generate field-ready concrete
formulations, based on local conditions and their expertise in concrete design.
Finally, we report on how these formulations were used in the construction of
buildings and structures in a Meta data center in DeKalb, IL, USA. Results from
field experiments as part of this real-world deployment corroborate the
efficacy of AI-generated low-carbon concrete mixes.
",5.0
458,11,7506,1102.0831,PageRank for web search,"G.Madhu (1), Dr.A.Govardhan (2), Dr.T.V.Rajinikanth (3)",Intelligent Semantic Web Search Engines: A Brief Survey,"  The World Wide Web (WWW) allows the people to share the information (data)
from the large database repositories globally. The amount of information grows
billions of databases. We need to search the information will specialize tools
known generically search engine. There are many of search engines available
today, retrieving meaningful information is difficult. However to overcome this
problem in search engines to retrieve meaningful information intelligently,
semantic web technologies are playing a major role. In this paper we present
survey on the search engine generations and the role of search engines in
intelligent web and semantic search technologies.
",2.0
459,4,156533,2105.07706,pre-trained language model,"Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin,
  Kuang-Chih Lee, Jian Xu, Bo Zheng","Towards a Better Tradeoff between Effectiveness and Efficiency in
  Pre-Ranking: A Learnable Feature Selection based Approach","  In real-world search, recommendation, and advertising systems, the
multi-stage ranking architecture is commonly adopted. Such architecture usually
consists of matching, pre-ranking, ranking, and re-ranking stages. In the
pre-ranking stage, vector-product based models with representation-focused
architecture are commonly adopted to account for system efficiency. However, it
brings a significant loss to the effectiveness of the system. In this paper, a
novel pre-ranking approach is proposed which supports complicated models with
interaction-focused architecture. It achieves a better tradeoff between
effectiveness and efficiency by utilizing the proposed learnable Feature
Selection method based on feature Complexity and variational Dropout (FSCD).
Evaluations in a real-world e-commerce sponsored search system for a search
engine demonstrate that utilizing the proposed pre-ranking, the effectiveness
of the system is significantly improved. Moreover, compared to the systems with
conventional pre-ranking models, an identical amount of computational resource
is consumed.
",0.0
460,6,194579,2205.016,query expansion for imformation retrieval,"Sandra Wankm\""uller","A Comparison of Approaches for Imbalanced Classification Problems in the
  Context of Retrieving Relevant Documents for an Analysis","  One of the first steps in many text-based social science studies is to
retrieve documents that are relevant for the analysis from large corpora of
otherwise irrelevant documents. The conventional approach in social science to
address this retrieval task is to apply a set of keywords and to consider those
documents to be relevant that contain at least one of the keywords. But the
application of incomplete keyword lists risks drawing biased inferences. More
complex and costly methods such as query expansion techniques, topic
model-based classification rules, and active as well as passive supervised
learning could have the potential to more accurately separate relevant from
irrelevant documents and thereby reduce the potential size of bias. Yet,
whether applying these more expensive approaches increases retrieval
performance compared to keyword lists at all, and if so, by how much, is
unclear as a comparison of these approaches is lacking. This study closes this
gap by comparing these methods across three retrieval tasks associated with a
data set of German tweets (Linder, 2017), the Social Bias Inference Corpus
(SBIC) (Sap et al., 2020), and the Reuters-21578 corpus (Lewis, 1997). Results
show that query expansion techniques and topic model-based classification rules
in most studied settings tend to decrease rather than increase retrieval
performance. Active supervised learning, however, if applied on a not too small
set of labeled training instances (e.g. 1,000 documents), reaches a
substantially higher retrieval performance than keyword lists.
",3.0
461,4,217389,cs/0502086,pre-trained language model,Pierre-Yves Oudeyer,The Self-Organization of Speech Sounds,"  The speech code is a vehicle of language: it defines a set of forms used by a
community to carry information. Such a code is necessary to support the
linguistic interactions that allow humans to communicate. How then may a speech
code be formed prior to the existence of linguistic interactions? Moreover, the
human speech code is discrete and compositional, shared by all the individuals
of a community but different across communities, and phoneme inventories are
characterized by statistical regularities. How can a speech code with these
properties form? We try to approach these questions in the paper, using the
""methodology of the artificial"". We build a society of artificial agents, and
detail a mechanism that shows the formation of a discrete speech code without
pre-supposing the existence of linguistic capacities or of coordinated
interactions. The mechanism is based on a low-level model of sensory-motor
interactions. We show that the integration of certain very simple and non
language-specific neural devices leads to the formation of a speech code that
has properties similar to the human speech code. This result relies on the
self-organizing properties of a generic coupling between perception and
production within agents, and on the interactions between agents. The
artificial system helps us to develop better intuitions on how speech might
have appeared, by showing how self-organization might have helped natural
selection to find speech.
",3.0
462,0,13929,1206.644,learning to rank with partitioned preference,"Or Sheffet (Carnegie Mellon University), Nina Mishra (Microsoft
  Research), Samuel Ieong (Microsoft Research)",Predicting Preference Flips in Commerce Search,"  Traditional approaches to ranking in web search follow the paradigm of
rank-by-score: a learned function gives each query-URL combination an absolute
score and URLs are ranked according to this score. This paradigm ensures that
if the score of one URL is better than another then one will always be ranked
higher than the other. Scoring contradicts prior work in behavioral economics
that showed that users' preferences between two items depend not only on the
items but also on the presented alternatives. Thus, for the same query, users'
preference between items A and B depends on the presence/absence of item C. We
propose a new model of ranking, the Random Shopper Model, that allows and
explains such behavior. In this model, each feature is viewed as a Markov chain
over the items to be ranked, and the goal is to find a weighting of the
features that best reflects their importance. We show that our model can be
learned under the empirical risk minimization framework, and give an efficient
learning algorithm. Experiments on commerce search logs demonstrate that our
algorithm outperforms scoring-based approaches including regression and
listwise ranking.
",3.0
463,0,57215,1709.01249,learning to rank with partitioned preference,"Pan Li, Olgica Milenkovic",Inhomogeneous Hypergraph Clustering with Applications,"  Hypergraph partitioning is an important problem in machine learning, computer
vision and network analytics. A widely used method for hypergraph partitioning
relies on minimizing a normalized sum of the costs of partitioning hyperedges
across clusters. Algorithmic solutions based on this approach assume that
different partitions of a hyperedge incur the same cost. However, this
assumption fails to leverage the fact that different subsets of vertices within
the same hyperedge may have different structural importance. We hence propose a
new hypergraph clustering technique, termed inhomogeneous hypergraph
partitioning, which assigns different costs to different hyperedge cuts. We
prove that inhomogeneous partitioning produces a quadratic approximation to the
optimal solution if the inhomogeneous costs satisfy submodularity constraints.
Moreover, we demonstrate that inhomogenous partitioning offers significant
performance improvements in applications such as structure learning of
rankings, subspace segmentation and motif clustering.
",1.0
464,7,65202,1803.08,gradient boosting,"Indrayudh Ghosal, Giles Hooker","Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its
  Variance Estimate","  In this paper we propose using the principle of boosting to reduce the bias
of a random forest prediction in the regression setting. From the original
random forest fit we extract the residuals and then fit another random forest
to these residuals. We call the sum of these two random forests a
\textit{one-step boosted forest}. We show with simulated and real data that the
one-step boosted forest has a reduced bias compared to the original random
forest. The paper also provides a variance estimate of the one-step boosted
forest by an extension of the infinitesimal Jackknife estimator. Using this
variance estimate we can construct prediction intervals for the boosted forest
and we show that they have good coverage probabilities. Combining the bias
reduction and the variance estimate we show that the one-step boosted forest
has a significant reduction in predictive mean squared error and thus an
improvement in predictive performance. When applied on datasets from the UCI
database, one-step boosted forest performs better than random forest and
gradient boosting machine algorithms. Theoretically we can also extend such a
boosting process to more than one step and the same principles outlined in this
paper can be used to find variance estimates for such predictors. Such boosting
will reduce bias even further but it risks over-fitting and also increases the
computational burden.
",3.0
465,5,66788,1804.10653,matrix completion,"Ivan Nazarov, Boris Shirokikh, Maria Burkina, Gennady Fedonin and
  Maxim Panov",Sparse Group Inductive Matrix Completion,"  We consider the problem of matrix completion with side information
(\textit{inductive matrix completion}). In real-world applications many
side-channel features are typically non-informative making feature selection an
important part of the problem. We incorporate feature selection into inductive
matrix completion by proposing a matrix factorization framework with
group-lasso regularization on side feature parameter matrices. We demonstrate,
that the theoretical sample complexity for the proposed method is much lower
compared to its competitors in sparse problems, and propose an efficient
optimization algorithm for the resulting low-rank matrix completion problem
with sparsifying regularizers. Experiments on synthetic and real-world datasets
show that the proposed approach outperforms other methods.
",4.0
466,11,31578,1502.02567,PageRank for web search,"Young-Ho Eom, Dima L. Shepelyansky",Opinion formation driven by PageRank node influence on directed networks,"  We study a two states opinion formation model driven by PageRank node
influence and report an extensive numerical study on how PageRank affects
collective opinion formations in large-scale empirical directed networks. In
our model the opinion of a node can be updated by the sum of its neighbor
nodes' opinions weighted by the node influence of the neighbor nodes at each
step. We consider PageRank probability and its sublinear power as node
influence measures and investigate evolution of opinion under various
conditions. First, we observe that all networks reach steady state opinion
after a certain relaxation time. This time scale is decreasing with the
heterogeneity of node influence in the networks. Second, we find that our model
shows consensus and non-consensus behavior in steady state depending on types
of networks: Web graph, citation network of physics articles, and LiveJournal
social network show non-consensus behavior while Wikipedia article network
shows consensus behavior. Third, we find that a more heterogeneous influence
distribution leads to a more uniform opinion state in the cases of Web graph,
Wikipedia, and Livejournal. However, the opposite behavior is observed in the
citation network. Finally we identify that a small number of influential nodes
can impose their own opinion on significant fraction of other nodes in all
considered networks. Our study shows that the effects of heterogeneity of node
influence on opinion formation can be significant and suggests further
investigations on the interplay between node influence and collective opinion
in networks.
",4.0
467,14,115201,2004.03589,text summarization model,"Piji Li, Lidong Bing, Zhongyu Wei, Wai Lam","Salience Estimation with Multi-Attention Learning for Abstractive Text
  Summarization","  Attention mechanism plays a dominant role in the sequence generation models
and has been used to improve the performance of machine translation and
abstractive text summarization. Different from neural machine translation, in
the task of text summarization, salience estimation for words, phrases or
sentences is a critical component, since the output summary is a distillation
of the input text. Although the typical attention mechanism can conduct text
fragment selection from the input text conditioned on the decoder states, there
is still a gap to conduct direct and effective salience detection. To bring
back direct salience estimation for summarization with neural networks, we
propose a Multi-Attention Learning framework which contains two new attention
learning components for salience estimation: supervised attention learning and
unsupervised attention learning. We regard the attention weights as the
salience information, which means that the semantic units with large attention
value will be more important. The context information obtained based on the
estimated salience is incorporated with the typical attention mechanism in the
decoder to conduct summary generation. Extensive experiments on some benchmark
datasets in different languages demonstrate the effectiveness of the proposed
framework for the task of abstractive summarization.
",4.0
468,4,151906,2104.0029,pre-trained language model,"Thamme Gowda, Zhao Zhang, Chris A Mattmann, Jonathan May","Many-to-English Machine Translation Tools, Data, and Pretrained Models","  While there are more than 7000 languages in the world, most translation
research efforts have targeted a few high-resource languages. Commercial
translation systems support only one hundred languages or fewer, and do not
make these models available for transfer to low resource languages. In this
work, we present useful tools for machine translation research: MTData,
NLCodec, and RTG. We demonstrate their usefulness by creating a multilingual
neural machine translation model capable of translating from 500 source
languages to English. We make this multilingual model readily downloadable and
usable as a service, or as a parent model for transfer-learning to even
lower-resource languages.
",5.0
469,2,125415,2007.05721,random forests,"Alvaro H. C. Correia, Robert Peharz, Cassio de Campos",Towards Robust Classification with Deep Generative Forests,"  Decision Trees and Random Forests are among the most widely used machine
learning models, and often achieve state-of-the-art performance in tabular,
domain-agnostic datasets. Nonetheless, being primarily discriminative models
they lack principled methods to manipulate the uncertainty of predictions. In
this paper, we exploit Generative Forests (GeFs), a recent class of deep
probabilistic models that addresses these issues by extending Random Forests to
generative models representing the full joint distribution over the feature
space. We demonstrate that GeFs are uncertainty-aware classifiers, capable of
measuring the robustness of each prediction as well as detecting
out-of-distribution samples.
",4.0
470,12,125464,2007.05924,COVID-19 and social media,"Mousumi Karmakar, Sumit Kumar Banshal, Vivek Kumar Singh","Does presence of social media plugins in a journal website result in
  higher social media attention of its research publications","  Social media platforms have now emerged as an important medium for wider
dissemination of research articles; with authors, readers and publishers
creating different kinds of social media activity about the article. Some
research studies have even shown that articles that get more social media
attention may get higher visibility and citations. These factors are now
persuading journal publishers to integrate social media plugins in their
webpages to facilitate sharing and dissemination of articles in social media
platforms. Many past studies have analyzed several factors (like journal impact
factor, open access, collaboration etc.) that may impact social media attention
of scholarly articles. However, there are no studies to analyze whether the
presence of social media plugin in a journal could result in higher social
media attention of articles published in the journal. This paper aims to bridge
this gap in knowledge by analyzing a sufficiently large-sized sample of 99,749
articles from 100 different journals. Results obtained show that journals that
have social media plugins integrated in their webpages get significantly higher
social media mentions and shares for their articles as compared to journals
that do not provide such plugins. Authors and readers visiting journal webpages
appear to be a major contributor to social media activity around articles
published in such journals. The results suggest that publishing houses should
actively provide social media plugin integration in their journal webpages to
increase social media visibility (altmetric impact) of their articles.
",1.0
471,10,22034,1309.2648,web archive,Hany M. SalahEldeen and Michael L. Nelson,"Resurrecting My Revolution: Using Social Link Neighborhood in Bringing
  Context to the Disappearing Web","  In previous work we reported that resources linked in tweets disappeared at
the rate of 11% in the first year followed by 7.3% each year afterwards. We
also found that in the first year 6.7%, and 14.6% in each subsequent year, of
the resources were archived in public web archives. In this paper we revisit
the same dataset of tweets and find that our prior model still holds and the
calculated error for estimating percentages missing was about 4%, but we found
the rate of archiving produced a higher error of about 11.5%. We also
discovered that resources have disappeared from the archives themselves (7.89%)
as well as reappeared on the live web after being declared missing (6.54%). We
have also tested the availability of the tweets themselves and found that
10.34% have disappeared from the live web. To mitigate the loss of resources on
the live web, we propose the use of a ""tweet signature"". Using the Topsy API,
we extract the top five most frequent terms from the union of all tweets about
a resource, and use these five terms as a query to Google. We found that using
tweet signatures results in discovering replacement resources with 70+% textual
similarity to the missing resource 41% of the time.
",4.0
472,0,69446,1806.05819,learning to rank with partitioned preference,"Chang Li, Branislav Kveton, Tor Lattimore, Ilya Markov, Maarten de
  Rijke, Csaba Szepesvari, and Masrour Zoghi",BubbleRank: Safe Online Learning to Re-Rank via Implicit Click Feedback,"  In this paper, we study the problem of safe online learning to re-rank, where
user feedback is used to improve the quality of displayed lists. Learning to
rank has traditionally been studied in two settings. In the offline setting,
rankers are typically learned from relevance labels created by judges. This
approach has generally become standard in industrial applications of ranking,
such as search. However, this approach lacks exploration and thus is limited by
the information content of the offline training data. In the online setting, an
algorithm can experiment with lists and learn from feedback on them in a
sequential fashion. Bandit algorithms are well-suited for this setting but they
tend to learn user preferences from scratch, which results in a high initial
cost of exploration. This poses an additional challenge of safe exploration in
ranked lists. We propose BubbleRank, a bandit algorithm for safe re-ranking
that combines the strengths of both the offline and online settings. The
algorithm starts with an initial base list and improves it online by gradually
exchanging higher-ranked less attractive items for lower-ranked more attractive
items. We prove an upper bound on the n-step regret of BubbleRank that degrades
gracefully with the quality of the initial base list. Our theoretical findings
are supported by extensive experiments on a large-scale real-world click
dataset.
",1.0
473,15,144249,2101.07918,relevance feedback for imformation retrieval,"HongChien Yu, Zhuyun Dai, Jamie Callan",PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer,"  Most research on pseudo relevance feedback (PRF) has been done in vector
space and probabilistic retrieval models. This paper shows that
Transformer-based rerankers can also benefit from the extra context that PRF
provides. It presents PGT, a graph-based Transformer that sparsifies attention
between graph nodes to enable PRF while avoiding the high computational
complexity of most Transformer architectures. Experiments show that PGT
improves upon non-PRF Transformer reranker, and it is at least as accurate as
Transformer PRF models that use full attention, but with lower computational
costs.
",4.0
474,7,101745,1910.13204,gradient boosting,Bulat Ibragimov and Gleb Gusev,Minimal Variance Sampling in Stochastic Gradient Boosting,"  Stochastic Gradient Boosting (SGB) is a widely used approach to
regularization of boosting models based on decision trees. It was shown that,
in many cases, random sampling at each iteration can lead to better
generalization performance of the model and can also decrease the learning
time. Different sampling approaches were proposed, where probabilities are not
uniform, and it is not currently clear which approach is the most effective. In
this paper, we formulate the problem of randomization in SGB in terms of
optimization of sampling probabilities to maximize the estimation accuracy of
split scoring used to train decision trees. This optimization problem has a
closed-form nearly optimal solution, and it leads to a new sampling technique,
which we call Minimal Variance Sampling (MVS). The method both decreases the
number of examples needed for each iteration of boosting and increases the
quality of the model significantly as compared to the state-of-the art sampling
methods. The superiority of the algorithm was confirmed by introducing MVS as a
new default option for subsampling in CatBoost, a gradient boosting library
achieving state-of-the-art quality on various machine learning tasks.
",3.0
475,3,49008,1701.00622,database management system,"Dietmar Seipel (University of W\""urzburg)",Knowledge Engineering for Hybrid Deductive Databases,"  Modern knowledge base systems frequently need to combine a collection of
databases in different formats: e.g., relational databases, XML databases, rule
bases, ontologies, etc. In the deductive database system DDBASE, we can manage
these different formats of knowledge and reason about them. Even the file
systems on different computers can be part of the knowledge base. Often, it is
necessary to handle different versions of a knowledge base. E.g., we might want
to find out common parts or differences of two versions of a relational
database.
  We will examine the use of abstractions of rule bases by predicate dependency
and rule predicate graphs. Also the proof trees of derived atoms can help to
compare different versions of a rule base. Moreover, it might be possible to
have derivations joining rules with other formalisms of knowledge
representation.
  Ontologies have shown their benefits in many applications of intelligent
systems, and there have been many proposals for rule languages compatible with
the semantic web stack, e.g., SWRL, the semantic web rule language. Recently,
ontologies are used in hybrid systems for specifying the provenance of the
different components.
",3.0
476,9,209967,2209.09631,language model for long documents,"Yakini Tchouka, Jean-Fran\c{c}ois Couchot, Maxime Coulmeau, David
  Laiymani, Philippe Selles, Azzedine Rahmani and Christophe Guyeux","De-Identification of French Unstructured Clinical Notes for Machine
  Learning Tasks","  Unstructured textual data are at the heart of health systems: liaison letters
between doctors, operating reports, coding of procedures according to the
ICD-10 standard, etc. The details included in these documents make it possible
to get to know the patient better, to better manage him or her, to better study
the pathologies, to accurately remunerate the associated medical acts\ldots All
this seems to be (at least partially) within reach of today by artificial
intelligence techniques. However, for obvious reasons of privacy protection,
the designers of these AIs do not have the legal right to access these
documents as long as they contain identifying data. De-identifying these
documents, i.e. detecting and deleting all identifying information present in
them, is a legally necessary step for sharing this data between two
complementary worlds. Over the last decade, several proposals have been made to
de-identify documents, mainly in English. While the detection scores are often
high, the substitution methods are often not very robust to attack. In French,
very few methods are based on arbitrary detection and/or substitution rules. In
this paper, we propose a new comprehensive de-identification method dedicated
to French-language medical documents. Both the approach for the detection of
identifying elements (based on deep learning) and their substitution (based on
differential privacy) are based on the most proven existing approaches. The
result is an approach that effectively protects the privacy of the patients at
the heart of these medical documents. The whole approach has been evaluated on
a French language medical dataset of a French public hospital and the results
are very encouraging.
",1.0
477,14,60370,1711.09357,text summarization model,"Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, Hongyan Li",Generative Adversarial Network for Abstractive Text Summarization,"  In this paper, we propose an adversarial process for abstractive text
summarization, in which we simultaneously train a generative model G and a
discriminative model D. In particular, we build the generator G as an agent of
reinforcement learning, which takes the raw text as input and predicts the
abstractive summarization. We also build a discriminator which attempts to
distinguish the generated summary from the ground truth summary. Extensive
experiments demonstrate that our model achieves competitive ROUGE scores with
the state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show
that our model is able to generate more abstractive, readable and diverse
summaries.
",5.0
478,14,131319,2009.08018,text summarization model,Xiyan Fu and Jun Wang and Zhenglu Yang,Multi-modal Summarization for Video-containing Documents,"  Summarization of multimedia data becomes increasingly significant as it is
the basis for many real-world applications, such as question answering, Web
search, and so forth. Most existing multi-modal summarization works however
have used visual complementary features extracted from images rather than
videos, thereby losing abundant information. Hence, we propose a novel
multi-modal summarization task to summarize from a document and its associated
video. In this work, we also build a baseline general model with effective
strategies, i.e., bi-hop attention and improved late fusion mechanisms to
bridge the gap between different modalities, and a bi-stream summarization
strategy to employ text and video summarization simultaneously. Comprehensive
experiments show that the proposed model is beneficial for multi-modal
summarization and superior to existing methods. Moreover, we collect a novel
dataset and it provides a new resource for future study that results from
documents and videos.
",4.0
479,13,147879,2102.11,social network analysis with natrual language processing,"ElMehdi Boujou, Hamza Chataoui, Abdellah El Mekki, Saad Benjelloun,
  Ikram Chairi, and Ismail Berrada","An open access NLP dataset for Arabic dialects : Data collection,
  labeling, and model construction","  Natural Language Processing (NLP) is today a very active field of research
and innovation. Many applications need however big sets of data for supervised
learning, suitably labelled for the training purpose. This includes
applications for the Arabic language and its national dialects. However, such
open access labeled data sets in Arabic and its dialects are lacking in the
Data Science ecosystem and this lack can be a burden to innovation and research
in this field. In this work, we present an open data set of social data content
in several Arabic dialects. This data was collected from the Twitter social
network and consists on +50K twits in five (5) national dialects. Furthermore,
this data was labeled for several applications, namely dialect detection, topic
detection and sentiment analysis. We publish this data as an open access data
to encourage innovation and encourage other works in the field of NLP for
Arabic dialects and social media. A selection of models were built using this
data set and are presented in this paper along with their performances.
",2.0
480,14,100299,1910.05915,text summarization model,Shengluan Hou and Ruqian Lu,Knowledge-guided Unsupervised Rhetorical Parsing for Text Summarization,"  Automatic text summarization (ATS) has recently achieved impressive
performance thanks to recent advances in deep learning and the availability of
large-scale corpora. To make the summarization results more faithful, this
paper presents an unsupervised approach that combines rhetorical structure
theory, deep neural model and domain knowledge concern for ATS. This
architecture mainly contains three components: domain knowledge base
construction based on representation learning, attentional encoder-decoder
model for rhetorical parsing and subroutine-based model for text summarization.
Domain knowledge can be effectively used for unsupervised rhetorical parsing
thus rhetorical structure trees for each document can be derived. In the
unsupervised rhetorical parsing module, the idea of translation was adopted to
alleviate the problem of data scarcity. The subroutine-based summarization
model purely depends on the derived rhetorical structure trees and can generate
content-balanced results. To evaluate the summary results without golden
standard, we proposed an unsupervised evaluation metric, whose hyper-parameters
were tuned by supervised learning. Experimental results show that, on a
large-scale Chinese dataset, our proposed approach can obtain comparable
performances compared with existing methods.
",5.0
481,13,74086,1809.08513,social network analysis with natrual language processing,Y\'erali Gandica,Population preferences through Wikipedia edits,"  In this work, we are interested in the inner-cultural background shaping
broad people's preferences. Our interest is also to track this human footprint,
as it has the tendency to disappear due to the nowadays globalization. Given
that language is a social construction, it is part of the historical reservoir,
shaping the cultural (and hence collective) identity, then helping the
community to archive accumulated knowledge about its culture and identity. We
assume that the collective interest of a language-speaking community to
document their events, people and any feature important for them, by the online
encyclopedia Wikipedia, can act as a footprint of the whole group's collective
identity. The analysis of the language's preferences into categories among
several languages, could have also applications into the field of Multilingual
Natural Language Processing (MNLP). We, then, report results about the number
of edits, editors, and pages into categories, displayed by the several
languages. Results are shown by several angles, and some extra measures
complement the analysis.
",4.0
482,1,16897,1301.0176,advanced search engine,"Doreswamy, M.N.Vanajakshi",Similarity Measuring Approuch for Engineering Materials Selection,"  Advanced engineering materials design involves the exploration of massive
multidimensional feature spaces, the correlation of materials properties and
the processing parameters derived from disparate sources. The search for
alternative materials or processing property strategies, whether through
analytical, experimental or simulation approaches, has been a slow and arduous
task, punctuated by infrequent and often expected discoveries. A few systematic
efforts have been made to analyze the trends in data as a basis for
classifications and predictions. This is particularly due to the lack of large
amounts of organized data and more importantly the challenging of shifting
through them in a timely and efficient manner. The application of recent
advances in Data Mining on materials informatics is the state of art of
computational and experimental approaches for materials discovery. In this
paper similarity based engineering materials selection model is proposed and
implemented to select engineering materials based on the composite materials
constraints. The result reviewed from this model is sustainable for effective
decision making in advanced engineering materials design applications.
",1.0
483,6,14567,1207.5745,query expansion for imformation retrieval,"Swathi Rajasurya, Tamizhamudhu Muralidharan, Sandhiya Devi, S.
  Swamynathan",Semantic Information Retrieval Using Ontology In University Domain,"  Today's conventional search engines hardly do provide the essential content
relevant to the user's search query. This is because the context and semantics
of the request made by the user is not analyzed to the full extent. So here the
need for a semantic web search arises. SWS is upcoming in the area of web
search which combines Natural Language Processing and Artificial Intelligence.
The objective of the work done here is to design, develop and implement a
semantic search engine- SIEU(Semantic Information Extraction in University
Domain) confined to the university domain. SIEU uses ontology as a knowledge
base for the information retrieval process. It is not just a mere keyword
search. It is one layer above what Google or any other search engines retrieve
by analyzing just the keywords. Here the query is analyzed both syntactically
and semantically. The developed system retrieves the web results more relevant
to the user query through keyword expansion. The results obtained here will be
accurate enough to satisfy the request made by the user. The level of accuracy
will be enhanced since the query is analyzed semantically. The system will be
of great use to the developers and researchers who work on web. The Google
results are re-ranked and optimized for providing the relevant links. For
ranking an algorithm has been applied which fetches more apt results for the
user query.
",3.0
484,0,75840,1810.10226,learning to rank with partitioned preference,"Zhou Zhao, Hanbing Zhan, Lingtao Meng, Jun Xiao, Jun Yu, Min Yang, Fei
  Wu, Deng Cai",Textually Guided Ranking Network for Attentional Image Retweet Modeling,"  Retweet prediction is a challenging problem in social media sites (SMS). In
this paper, we study the problem of image retweet prediction in social media,
which predicts the image sharing behavior that the user reposts the image
tweets from their followees. Unlike previous studies, we learn user preference
ranking model from their past retweeted image tweets in SMS. We first propose
heterogeneous image retweet modeling network (IRM) that exploits users' past
retweeted image tweets with associated contexts, their following relations in
SMS and preference of their followees. We then develop a novel attentional
multi-faceted ranking network learning framework with textually guided
multi-modal neural networks for the proposed heterogenous IRM network to learn
the joint image tweet representations and user preference representations for
prediction task. The extensive experiments on a large-scale dataset from
Twitter site shows that our method achieves better performance than other
state-of-the-art solutions to the problem.
",1.0
485,9,217086,cs/0405044,language model for long documents,Oren Kurland and Lillian Lee,"Corpus structure, language models, and ad hoc information retrieval","  Most previous work on the recently developed language-modeling approach to
information retrieval focuses on document-specific characteristics, and
therefore does not take into account the structure of the surrounding corpus.
We propose a novel algorithmic framework in which information provided by
document-based language models is enhanced by the incorporation of information
drawn from clusters of similar documents. Using this framework, we develop a
suite of new algorithms. Even the simplest typically outperforms the standard
language-modeling approach in precision and recall, and our new interpolation
algorithm posts statistically significant improvements for both metrics over
all three corpora tested.
",1.0
486,12,49104,1701.01737,COVID-19 and social media,"Dominik Wurzer, Yumeng Qin",Spotting Information biases in Chinese and Western Media,"  Newswire and Social Media are the major sources of information in our time.
While the topical demographic of Western Media was subjects of studies in the
past, less is known about Chinese Media. In this paper, we apply event
detection and tracking technology to examine the information overlap and
differences between Chinese and Western - Traditional Media and Social Media.
Our experiments reveal a biased interest of China towards the West, which
becomes particularly apparent when comparing the interest in celebrities.
",1.0
487,2,89048,1905.10101,random forests,"Lin Zhu, Jiaxing Lu, Yihong Chen",HDI-Forest: Highest Density Interval Regression Forest,"  By seeking the narrowest prediction intervals (PIs) that satisfy the
specified coverage probability requirements, the recently proposed
quality-based PI learning principle can extract high-quality PIs that better
summarize the predictive certainty in regression tasks, and has been widely
applied to solve many practical problems. Currently, the state-of-the-art
quality-based PI estimation methods are based on deep neural networks or linear
models. In this paper, we propose Highest Density Interval Regression Forest
(HDI-Forest), a novel quality-based PI estimation method that is instead based
on Random Forest. HDI-Forest does not require additional model training, and
directly reuses the trees learned in a standard Random Forest model. By
utilizing the special properties of Random Forest, HDI-Forest could efficiently
and more directly optimize the PI quality metrics. Extensive experiments on
benchmark datasets show that HDI-Forest significantly outperforms previous
approaches, reducing the average PI width by over 20% while achieving the same
or better coverage probability
",3.0
488,5,31266,1501.06243,matrix completion,Yang Cao and Yao Xie,Poisson Matrix Completion,"  We extend the theory of matrix completion to the case where we make Poisson
observations for a subset of entries of a low-rank matrix. We consider the
(now) usual matrix recovery formulation through maximum likelihood with proper
constraints on the matrix $M$, and establish theoretical upper and lower bounds
on the recovery error. Our bounds are nearly optimal up to a factor on the
order of $\mathcal{O}(\log(d_1 d_2))$. These bounds are obtained by adapting
the arguments used for one-bit matrix completion \cite{davenport20121}
(although these two problems are different in nature) and the adaptation
requires new techniques exploiting properties of the Poisson likelihood
function and tackling the difficulties posed by the locally sub-Gaussian
characteristic of the Poisson distribution. Our results highlight a few
important distinctions of Poisson matrix completion compared to the prior work
in matrix completion including having to impose a minimum signal-to-noise
requirement on each observed entry. We also develop an efficient iterative
algorithm and demonstrate its good performance in recovering solar flare
images.
",4.0
489,3,215442,2210.1628,database management system,Andi Ferhati,"Clustering Graphs -- Applying a Label Propagation Algorithm to Detect
  Communities in Graph Databases","  In the last few decades, Database Management Systems (DBMSs) became powerful
tools for storing large amount of data and executing complex queries over them.
In the recent years, the growing amount of unstructured or semi-structured data
has seen a shift from representing data in the relational model towards
alternative data models. Graph Databases and Graph Database Management Systems
(GDBMSs) have seen an increase in use due to their ability to manage
highly-interconnected, continuously evolving data.
  This thesis is a documentation of the work done in implementing a system to
identify clusters in graph modeled data using a Label Propagation Community
Detection Algorithm. The graph was built using datasets of academic
publications in the field of Computer Science obtained from dblp.org . The
system developed is a FullStack WebApp consisting of a web-based user
interface, an API and the data (nodes, edges, graph) stored in a Graph Database
Management System (GDBMS).
  Described in this document are: - the process of manipulation pre-import and
import of the data in a Graph Database Management System (GDBMS) such as
ArangoDB, creation of nodes, relations (edges) between the nodes and a graph
composed of these nodes and edges; - the GraphQL API implemented in NodeJS to
request data from the Graph Database Management System (GDBMS); - the frontend
interface made with TypeScript and React consisting of the search
functionalities and ability to visualize results in Cytoscape Network Graphs; -
the Label Propagation Community Detection Algorithm execution on the graph, the
found clusters which are stored and visualized to the user whenever requested.
  This thesis hopes to contribute with a practical hands-on approach on the
graph representation, integration and analysis of interconnected data.
",5.0
490,12,83746,1903.01693,COVID-19 and social media,"Hamidreza Alvari, Elham Shaabani, Soumajyoti Sarkar, Ghazaleh Beigi,
  Paulo Shakarian","Less is More: Semi-Supervised Causal Inference for Detecting Pathogenic
  Users in Social Media","  Recent years have witnessed a surge of manipulation of public opinion and
political events by malicious social media actors. These users are referred to
as ""Pathogenic Social Media (PSM)"" accounts. PSMs are key users in spreading
misinformation in social media to viral proportions. These accounts can be
either controlled by real users or automated bots. Identification of PSMs is
thus of utmost importance for social media authorities. The burden usually
falls to automatic approaches that can identify these accounts and protect
social media reputation. However, lack of sufficient labeled examples for
devising and training sophisticated approaches to combat these accounts is
still one of the foremost challenges facing social media firms. In contrast,
unlabeled data is abundant and cheap to obtain thanks to massive user-generated
data. In this paper, we propose a semi-supervised causal inference PSM
detection framework, SemiPsm, to compensate for the lack of labeled data. In
particular, the proposed method leverages unlabeled data in the form of
manifold regularization and only relies on cascade information. This is in
contrast to the existing approaches that use exhaustive feature engineering
(e.g., profile information, network structure, etc.). Evidence from empirical
experiments on a real-world ISIS-related dataset from Twitter suggests
promising results of utilizing unlabeled instances for detecting PSMs.
",1.0
491,10,4974,1003.3661,web archive,"Herbert Van de Sompel, Robert Sanderson, Michael L. Nelson, Lyudmila
  L. Balakireva, Harihar Shankar, Scott Ainsworth",An HTTP-Based Versioning Mechanism for Linked Data,"  Dereferencing a URI returns a representation of the current state of the
resource identified by that URI. But, on the Web representations of prior
states of a resource are also available, for example, as resource versions in
Content Management Systems or archival resources in Web Archives such as the
Internet Archive. This paper introduces a resource versioning mechanism that is
fully based on HTTP and uses datetime as a global version indicator. The
approach allows ""follow your nose"" style navigation both from the current
time-generic resource to associated time-specific version resources as well as
among version resources. The proposed versioning mechanism is congruent with
the Architecture of the World Wide Web, and is based on the Memento framework
that extends HTTP with transparent content negotiation in the datetime
dimension. The paper shows how the versioning approach applies to Linked Data,
and by means of a demonstrator built for DBpedia, it also illustrates how it
can be used to conduct a time-series analysis across versions of Linked Data
descriptions.
",3.0
492,1,213723,2210.0943,advanced search engine,Catherine Chen and Carsten Eickhoff,Evaluating Search Explainability with Psychometrics and Crowdsourcing,"  Information retrieval (IR) systems have become an integral part of our
everyday lives. As search engines, recommender systems, and conversational
agents are employed across various domains from recreational search to clinical
decision support, there is an increasing need for transparent and explainable
systems to guarantee accountable, fair, and unbiased results. Despite many
recent advances towards explainable AI and IR techniques, there is no consensus
on what it means for a system to be explainable. Although a growing body of
literature suggests that explainability is comprised of multiple subfactors,
virtually all existing approaches treat it as a singular notion. In this paper,
we examine explainability in Web search systems, leveraging psychometrics and
crowdsourcing to identify human-centered factors of explainability. Based on
these factors, we establish a continuous-scale evaluation instrument for
explainable search systems that allows researchers and practitioners to
trade-off performance in a more flexible manner than what was previously
possible.
",4.0
493,19,192392,2204.05591,artificial intelligence for low carbon,"Lauren Coan, Bryan Williams, Krishna Adithya Venkatesh, Swati
  Upadhyaya, Silvester Czanner, Rengaraj Venkatesh, Colin E. Willoughby,
  Srinivasan Kavitha, Gabriela Czanner","Automatic detection of glaucoma via fundus imaging and artificial
  intelligence: A review","  Glaucoma is a leading cause of irreversible vision impairment globally and
cases are continuously rising worldwide. Early detection is crucial, allowing
timely intervention which can prevent further visual field loss. To detect
glaucoma, examination of the optic nerve head via fundus imaging can be
performed, at the centre of which is the assessment of the optic cup and disc
boundaries. Fundus imaging is non-invasive and low-cost; however, the image
examination relies on subjective, time-consuming, and costly expert
assessments. A timely question to ask is can artificial intelligence mimic
glaucoma assessments made by experts. Namely, can artificial intelligence
automatically find the boundaries of the optic cup and disc (providing a
so-called segmented fundus image) and then use the segmented image to identify
glaucoma with high accuracy. We conducted a comprehensive review on artificial
intelligence-enabled glaucoma detection frameworks that produce and use
segmented fundus images. We found 28 papers and identified two main approaches:
1) logical rule-based frameworks, based on a set of simplistic decision rules;
and 2) machine learning/statistical modelling based frameworks. We summarise
the state-of-art of the two approaches and highlight the key hurdles to
overcome for artificial intelligence-enabled glaucoma detection frameworks to
be translated into clinical practice.
",1.0
494,11,18770,1303.2438,PageRank for web search,"Guang-Gang Geng, Xiu-Tao Yang, Wei Wang, Chi-Jie Meng",A Taxonomy of Hyperlink Hiding Techniques,"  Hidden links are designed solely for search engines rather than visitors. To
get high search engine rankings, link hiding techniques are usually used for
the profitability of black industries, such as illicit game servers, false
medical services, illegal gambling, and less attractive high-profit industry,
etc. This paper investigates hyperlink hiding techniques on the Web, and gives
a detailed taxonomy. We believe the taxonomy can help develop appropriate
countermeasures. Study on 5,583,451 Chinese sites' home pages indicate that
link hidden techniques are very prevalent on the Web. We also tried to explore
the attitude of Google towards link hiding spam by analyzing the PageRank
values of relative links. The results show that more should be done to punish
the hidden link spam.
",3.0
495,8,148452,2102.13582,node embedding for graph,"Jing Zhu, Xingyu Lu, Mark Heimann, Danai Koutra","Node Proximity Is All You Need: Unified Structural and Positional Node
  and Graph Embedding","  While most network embedding techniques model the relative positions of nodes
in a network, recently there has been significant interest in structural
embeddings that model node role equivalences, irrespective of their distances
to any specific nodes. We present PhUSION, a proximity-based unified framework
for computing structural and positional node embeddings, which leverages
well-established methods for calculating node proximity scores. Clarifying a
point of contention in the literature, we show which step of PhUSION produces
the different kinds of embeddings and what steps can be used by both. Moreover,
by aggregating the PhUSION node embeddings, we obtain graph-level features that
model information lost by previous graph feature learning and kernel methods.
In a comprehensive empirical study with over 10 datasets, 4 tasks, and 35
methods, we systematically reveal successful design choices for node and
graph-level machine learning with embeddings.
",5.0
496,14,140521,2012.04307,text summarization model,"Ale\v{s} \v{Z}agar, Marko Robnik-\v{S}ikonja","Cross-lingual Transfer of Abstractive Summarizer to Less-resource
  Language","  Automatic text summarization extracts important information from texts and
presents the information in the form of a summary. Abstractive summarization
approaches progressed significantly by switching to deep neural networks, but
results are not yet satisfactory, especially for languages where large training
sets do not exist. In several natural language processing tasks, a
cross-lingual model transfer is successfully applied in less-resource
languages. For summarization, the cross-lingual model transfer was not
attempted due to a non-reusable decoder side of neural models that cannot
correct target language generation. In our work, we use a pre-trained English
summarization model based on deep neural networks and sequence-to-sequence
architecture to summarize Slovene news articles. We address the problem of
inadequate decoder by using an additional language model for the evaluation of
the generated text in target language. We test several cross-lingual
summarization models with different amounts of target data for fine-tuning. We
assess the models with automatic evaluation measures and conduct a small-scale
human evaluation. Automatic evaluation shows that the summaries of our best
cross-lingual model are useful and of quality similar to the model trained only
in the target language. Human evaluation shows that our best model generates
summaries with high accuracy and acceptable readability. However, similar to
other abstractive models, our models are not perfect and may occasionally
produce misleading or absurd content.
",4.0
497,0,85531,1904.02345,learning to rank with partitioned preference,"Ayman Elgharabawy, Mukesh Prasad, Chin-Teng Lin",Preference Neural Network,"  This paper proposes a preference neural network (PNN) to address the problem
of indifference preferences orders with new activation function. PNN also
solves the Multi-label ranking problem, where labels may have indifference
preference orders or subgroups are equally ranked. PNN follows a multi-layer
feedforward architecture with fully connected neurons. Each neuron contains a
novel smooth stairstep activation function based on the number of preference
orders. PNN inputs represent data features and output neurons represent label
indexes. The proposed PNN is evaluated using new preference mining dataset that
contains repeated label values which have not experimented before. PNN
outperforms five previously proposed methods for strict label ranking in terms
of accurate results with high computational efficiency.
",0.0
498,8,64128,1802.09612,node embedding for graph,"Jiongqian Liang, Saket Gurukar, Srinivasan Parthasarathy",MILE: A Multi-Level Framework for Scalable Graph Embedding,"  Recently there has been a surge of interest in designing graph embedding
methods. Few, if any, can scale to a large-sized graph with millions of nodes
due to both computational complexity and memory requirements. In this paper, we
relax this limitation by introducing the MultI-Level Embedding (MILE) framework
-- a generic methodology allowing contemporary graph embedding methods to scale
to large graphs. MILE repeatedly coarsens the graph into smaller ones using a
hybrid matching technique to maintain the backbone structure of the graph. It
then applies existing embedding methods on the coarsest graph and refines the
embeddings to the original graph through a graph convolution neural network
that it learns. The proposed MILE framework is agnostic to the underlying graph
embedding techniques and can be applied to many existing graph embedding
methods without modifying them. We employ our framework on several popular
graph embedding techniques and conduct embedding for real-world graphs.
Experimental results on five large-scale datasets demonstrate that MILE
significantly boosts the speed (order of magnitude) of graph embedding while
generating embeddings of better quality, for the task of node classification.
MILE can comfortably scale to a graph with 9 million nodes and 40 million
edges, on which existing methods run out of memory or take too long to compute
on a modern workstation. Our code and data are publicly available with detailed
instructions for adding new base embedding methods:
\url{https://github.com/jiongqian/MILE}.
",5.0
499,6,36465,1509.05567,query expansion for imformation retrieval,"Dipasree Pal, Mandar Mitra and Samar Bhattacharya",Exploring Query Categorisation for Query Expansion: A Study,"  The vocabulary mismatch problem is one of the important challenges facing
traditional keyword-based Information Retrieval Systems. The aim of query
expansion (QE) is to reduce this query-document mismatch by adding related or
synonymous words or phrases to the query.
  Several existing query expansion algorithms have proved their merit, but they
are not uniformly beneficial for all kinds of queries. Our long-term goal is to
formulate methods for applying QE techniques tailored to individual queries,
rather than applying the same general QE method to all queries. As an initial
step, we have proposed a taxonomy of query classes (from a QE perspective) in
this report. We have discussed the properties of each query class with
examples. We have also discussed some QE strategies that might be effective for
each query category.
  In future work, we intend to test the proposed techniques using standard
datasets, and to explore automatic query categorisation methods.
",5.0
500,16,182071,2201.037,activation function in neutral networks,Marco Maronese and Claudio Destri and Enrico Prati,Quantum activation functions for quantum neural networks,"  The field of artificial neural networks is expected to strongly benefit from
recent developments of quantum computers. In particular, quantum machine
learning, a class of quantum algorithms which exploit qubits for creating
trainable neural networks, will provide more power to solve problems such as
pattern recognition, clustering and machine learning in general. The building
block of feed-forward neural networks consists of one layer of neurons
connected to an output neuron that is activated according to an arbitrary
activation function. The corresponding learning algorithm goes under the name
of Rosenblatt perceptron. Quantum perceptrons with specific activation
functions are known, but a general method to realize arbitrary activation
functions on a quantum computer is still lacking. Here we fill this gap with a
quantum algorithm which is capable to approximate any analytic activation
functions to any given order of its power series. Unlike previous proposals
providing irreversible measurement--based and simplified activation functions,
here we show how to approximate any analytic function to any required accuracy
without the need to measure the states encoding the information. Thanks to the
generality of this construction, any feed-forward neural network may acquire
the universal approximation properties according to Hornik's theorem. Our
results recast the science of artificial neural networks in the architecture of
gate-model quantum computers.
",4.0
501,13,213620,2210.08994,social network analysis with natrual language processing,"Seng-Beng Ho, Zhaoxia Wang, Boon-Kiat Quek, Erik Cambria","Knowledge Representation for Conceptual, Motivational, and Affective
  Processes in Natural Language Communication","  Natural language communication is an intricate and complex process. The
speaker usually begins with an intention and motivation of what is to be
communicated, and what effects are expected from the communication, while
taking into consideration the listener's mental model to concoct an appropriate
sentence. The listener likewise has to interpret what the speaker means, and
respond accordingly, also with the speaker's mental state in mind. To do this
successfully, conceptual, motivational, and affective processes have to be
represented appropriately to drive the language generation and understanding
processes. Language processing has succeeded well with the big data approach in
applications such as chatbots and machine translation. However, in human-robot
collaborative social communication and in using natural language for delivering
precise instructions to robots, a deeper representation of the conceptual,
motivational, and affective processes is needed. This paper capitalizes on the
UGALRS (Unified General Autonomous and Language Reasoning System) framework and
the CD+ (Conceptual Representation Plus) representational scheme to illustrate
how social communication through language is supported by a knowledge
representational scheme that handles conceptual, motivational, and affective
processes in a deep and general way. Though a small set of concepts,
motivations, and emotions is treated in this paper, its main contribution is in
articulating a general framework of knowledge representation and processing to
link these aspects together in serving the purpose of natural language
communication for an intelligent system.
",1.0
502,11,24590,1401.6092,PageRank for web search,"Christopher Engstr\""om, Sergei Silvestrov",PageRank for evolving link structures,"  In this article we will look at the PageRank algorithm used as part of the
ranking process of different Internet pages in search engines by for example
Google. This article has its main focus in the understanding of the behavior of
PageRank as the system dynamically changes either by contracting or expanding
such as when adding or subtracting nodes or links or groups of nodes or links.
In particular we will take a look at link structures consisting of a line of
nodes or a complete graph where every node links to all others.
  We will look at PageRank as the solution of a linear system of equations and
do our examination in both the ordinary normalized version of PageRank as well
as the non-normalized version found by solving the linear system. We will see
that it is possible to find explicit formulas for the PageRank in some simple
link structures and using these formulas take a more in-depth look at the
behavior of the ranking as the system changes.
",2.0
503,4,26931,1406.0079,pre-trained language model,Shashishekar Ramakrishna and Adrian Paschke,"Bridging the gap between Legal Practitioners and Knowledge Engineers
  using semi-formal KR","  The use of Structured English as a computation independent knowledge
representation format for non-technical users in business rules representation
has been proposed in OMGs Semantics and Business Vocabulary Representation
(SBVR). In the legal domain we face a similar problem. Formal representation
languages, such as OASIS LegalRuleML and legal ontologies (LKIF, legal OWL2
ontologies etc.) support the technical knowledge engineer and the automated
reasoning. But, they can be hardly used directly by the legal domain experts
who do not have a computer science background. In this paper we adapt the SBVR
Structured English approach for the legal domain and implement a
proof-of-concept, called KR4IPLaw, which enables legal domain experts to
represent their knowledge in Structured English in a computational independent
and hence, for them, more usable way. The benefit of this approach is that the
underlying pre-defined semantics of the Structured English approach makes
transformations into formal languages such as OASIS LegalRuleML and OWL2
ontologies possible. We exemplify our approach in the domain of patent law.
",4.0
504,15,167733,2108.13454,relevance feedback for imformation retrieval,"HongChien Yu, Chenyan Xiong, Jamie Callan","Improving Query Representations for Dense Retrieval with Pseudo
  Relevance Feedback","  Dense retrieval systems conduct first-stage retrieval using embedded
representations and simple similarity metrics to match a query to documents.
Its effectiveness depends on encoded embeddings to capture the semantics of
queries and documents, a challenging task due to the shortness and ambiguity of
search queries. This paper proposes ANCE-PRF, a new query encoder that uses
pseudo relevance feedback (PRF) to improve query representations for dense
retrieval. ANCE-PRF uses a BERT encoder that consumes the query and the top
retrieved documents from a dense retrieval model, ANCE, and it learns to
produce better query embeddings directly from relevance labels. It also keeps
the document index unchanged to reduce overhead. ANCE-PRF significantly
outperforms ANCE and other recent dense retrieval systems on several datasets.
Analysis shows that the PRF encoder effectively captures the relevant and
complementary information from PRF documents, while ignoring the noise with its
learned attention mechanism.
",4.0
505,10,22125,1309.4016,web archive,"Yasmin AlNoamany, Ahmed AlSum, Michele C. Weigle, and Michael L.
  Nelson",Who and What Links to the Internet Archive,"  The Internet Archive's (IA) Wayback Machine is the largest and oldest public
web archive and has become a significant repository of our recent history and
cultural heritage. Despite its importance, there has been little research about
how it is discovered and used. Based on web access logs, we analyze what users
are looking for, why they come to IA, where they come from, and how pages link
to IA. We find that users request English pages the most, followed by the
European languages. Most human users come to web archives because they do not
find the requested pages on the live web. About 65% of the requested archived
pages no longer exist on the live web. We find that more than 82% of human
sessions connect to the Wayback Machine via referrals from other web sites,
while only 15% of robots have referrers. Most of the links (86%) from websites
are to individual archived pages at specific points in time, and of those 83%
no longer exist on the live web.
",4.0
506,7,43911,1607.0011,gradient boosting,Iman Alodah and Jennifer Neville,"Combining Gradient Boosting Machines with Collective Inference to
  Predict Continuous Values","  Gradient boosting of regression trees is a competitive procedure for learning
predictive models of continuous data that fits the data with an additive
non-parametric model. The classic version of gradient boosting assumes that the
data is independent and identically distributed. However, relational data with
interdependent, linked instances is now common and the dependencies in such
data can be exploited to improve predictive performance. Collective inference
is one approach to exploit relational correlation patterns and significantly
reduce classification error. However, much of the work on collective learning
and inference has focused on discrete prediction tasks rather than continuous.
%target values has not got that attention in terms of collective inference. In
this work, we investigate how to combine these two paradigms together to
improve regression in relational domains. Specifically, we propose a boosting
algorithm for learning a collective inference model that predicts a continuous
target variable. In the algorithm, we learn a basic relational model,
collectively infer the target values, and then iteratively learn relational
models to predict the residuals. We evaluate our proposed algorithm on a real
network dataset and show that it outperforms alternative boosting methods.
However, our investigation also revealed that the relational features interact
together to produce better predictions.
",4.0
507,18,107951,2001.05568,infomation retrieval time complexity,"Rafael G. L. D'Oliveira, Salim El Rouayheb, Daniel Heinlein, David
  Karpuk","Notes on Communication and Computation in Secure Distributed Matrix
  Multiplication","  We consider the problem of secure distributed matrix multiplication in which
a user wishes to compute the product of two matrices with the assistance of
honest but curious servers. In this paper, we answer the following question: Is
it beneficial to offload the computations if security is a concern? We answer
this question in the affirmative by showing that by adjusting the parameters in
a polynomial code we can obtain a trade-off between the user's and the servers'
computational time. Indeed, we show that if the computational time complexity
of an operation in $\mathbb{F}_q$ is at most $\mathcal{Z}_q$ and the
computational time complexity of multiplying two $n\times n$ matrices is
$\mathcal{O}(n^\omega \mathcal{Z}_q)$ then, by optimizing the trade-off, the
user together with the servers can compute the multiplication in
$\mathcal{O}(n^{4-\frac{6}{\omega+1}} \mathcal{Z}_q)$ time. We also show that
if the user is only concerned in optimizing the download rate, a common
assumption in the literature, then the problem can be converted into a simple
private information retrieval problem by means of a scheme we call Private
Oracle Querying. However, this comes at large upload and computational costs
for both the user and the servers.
",2.0
508,18,52053,1704.01049,infomation retrieval time complexity,Alexander Eckrot and Carina Geldhauser and Jan Jurczyk,"A simulated annealing approach to optimal storing in a multi-level
  warehouse","  We propose a simulated annealing algorithm specifically tailored to optimise
total retrieval times in a multi-level warehouse under complex pre-batched
picking constraints. Experiments on real data from a picker-to-parts order
picking process in the warehouse of a European manufacturer show that optimal
storage assignments do not necessarily display features presumed in heuristics,
such as clustering of positively correlated items or ordering of items by
picking frequency.
  In an experiment run on more than 4000 batched orders with 1 to 150 items per
batch, the storage assignment suggested by the algorithm produces a 21\%
reduction in the total retrieval time with respect to a frequency-based storage
assignment.
",2.0
509,0,218145,cs/0605035,learning to rank with partitioned preference,Filip Radlinski and Thorsten Joachims,Query Chains: Learning to Rank from Implicit Feedback,"  This paper presents a novel approach for using clickthrough data to learn
ranked retrieval functions for web search results. We observe that users
searching the web often perform a sequence, or chain, of queries with a similar
information need. Using query chains, we generate new types of preference
judgments from search engine logs, thus taking advantage of user intelligence
in reformulating queries. To validate our method we perform a controlled user
study comparing generated preference judgments to explicit relevance judgments.
We also implemented a real-world search engine to test our approach, using a
modified ranking SVM to learn an improved ranking function from preference
data. Our results demonstrate significant improvements in the ranking given by
the search engine. The learned rankings outperform both a static ranking
function, as well as one trained without considering query chains.
",1.0
510,4,76182,1810.12387,pre-trained language model,"Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun,
  Fen Lin, Leyu Lin",Language Modeling with Sparse Product of Sememe Experts,"  Most language modeling methods rely on large-scale data to statistically
learn the sequential patterns of words. In this paper, we argue that words are
atomic language units but not necessarily atomic semantic units. Inspired by
HowNet, we use sememes, the minimum semantic units in human languages, to
represent the implicit semantics behind words for language modeling, named
Sememe-Driven Language Model (SDLM). More specifically, to predict the next
word, SDLM first estimates the sememe distribution gave textual context.
Afterward, it regards each sememe as a distinct semantic expert, and these
experts jointly identify the most probable senses and the corresponding word.
In this way, SDLM enables language models to work beyond word-level
manipulation to fine-grained sememe-level semantics and offers us more powerful
tools to fine-tune language models and improve the interpretability as well as
the robustness of language models. Experiments on language modeling and the
downstream application of headline gener- ation demonstrate the significant
effect of SDLM. Source code and data used in the experiments can be accessed at
https:// github.com/thunlp/SDLM-pytorch.
",3.0
511,13,116436,2004.10151,social network analysis with natrual language processing,"Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua
  Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May,
  Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian",Experience Grounds Language,"  Language understanding research is held back by a failure to relate language
to the physical world it describes and to the social interactions it
facilitates. Despite the incredible effectiveness of language processing models
to tackle tasks after being trained on text alone, successful linguistic
communication relies on a shared experience of the world. It is this shared
experience that makes utterances meaningful.
  Natural language processing is a diverse field, and progress throughout its
development has come from new representational theories, modeling techniques,
data collection paradigms, and tasks. We posit that the present success of
representation learning approaches trained on large, text-only corpora requires
the parallel tradition of research on the broader physical and social context
of language to address the deeper questions of communication.
",3.0
512,2,89323,1905.11028,random forests,"Hanyuan Hang, Xiaoyu Liu, and Ingo Steinwart",Best-scored Random Forest Classification,"  We propose an algorithm named best-scored random forest for binary
classification problems. The terminology ""best-scored"" means to select the one
with the best empirical performance out of a certain number of purely random
tree candidates as each single tree in the forest. In this way, the resulting
forest can be more accurate than the original purely random forest. From the
theoretical perspective, within the framework of regularized empirical risk
minimization penalized on the number of splits, we establish almost optimal
convergence rates for the proposed best-scored random trees under certain
conditions which can be extended to the best-scored random forest. In addition,
we present a counterexample to illustrate that in order to ensure the
consistency of the forest, every dimension must have the chance to be split. In
the numerical experiments, for the sake of efficiency, we employ an adaptive
random splitting criterion. Comparative experiments with other state-of-art
classification methods demonstrate the accuracy of our best-scored random
forest.
",4.0
513,18,2514,901.4272,infomation retrieval time complexity,"Khalid Hachemi (GIPSA-lab), Hassane. Alla (GIPSA-lab)",Dynamic Control of a Flow-Rack Automated Storage and Retrieval System,"  In this paper we propose a control scheme based on coloured Petri net (CPN)
for a flow-rack automated storage and retrieval system. The AS/RS is modelled
using Coloured Petri nets, the developed model has been used to capture and
provide the rack state. We introduce in the control system an optimization
module as a decision process which performs a real-time optimization working on
a discrete events time scale. The objective is to find bin locations for the
retrieval requests by minimizing the total number of retrieval cycles for a
batch of requests and thereby increase the system throughput. By solving the
optimization model, the proposed method gives according to customers request
and the rack state, the best bin locations for retrieval, i.e. allowing at the
same time to satisfy the customers request and carrying out the minimum of
retrieval cycles.
",4.0
514,4,35634,1508.01774,pre-trained language model,"Siddharth Sigtia, Emmanouil Benetos, Simon Dixon",An End-to-End Neural Network for Polyphonic Piano Music Transcription,"  We present a supervised neural network model for polyphonic piano music
transcription. The architecture of the proposed model is analogous to speech
recognition systems and comprises an acoustic model and a music language model.
The acoustic model is a neural network used for estimating the probabilities of
pitches in a frame of audio. The language model is a recurrent neural network
that models the correlations between pitch combinations over time. The proposed
model is general and can be used to transcribe polyphonic music without
imposing any constraints on the polyphony. The acoustic and language model
predictions are combined using a probabilistic graphical model. Inference over
the output variables is performed using the beam search algorithm. We perform
two sets of experiments. We investigate various neural network architectures
for the acoustic models and also investigate the effect of combining acoustic
and music language model predictions using the proposed architecture. We
compare performance of the neural network based acoustic models with two
popular unsupervised acoustic models. Results show that convolutional neural
network acoustic models yields the best performance across all evaluation
metrics. We also observe improved performance with the application of the music
language models. Finally, we present an efficient variant of beam search that
improves performance and reduces run-times by an order of magnitude, making the
model suitable for real-time applications.
",3.0
515,2,66230,1804.05753,random forests,Taylor Pospisil and Ann B. Lee,RFCDE: Random Forests for Conditional Density Estimation,"  Random forests is a common non-parametric regression technique which performs
well for mixed-type data and irrelevant covariates, while being robust to
monotonic variable transformations. Existing random forest implementations
target regression or classification. We introduce the RFCDE package for fitting
random forest models optimized for nonparametric conditional density
estimation, including joint densities for multiple responses. This enables
analysis of conditional probability distributions which is useful for
propagating uncertainty and of joint distributions that describe relationships
between multiple responses and covariates. RFCDE is released under the MIT
open-source license and can be accessed at https://github.com/tpospisi/rfcde .
Both R and Python versions, which call a common C++ library, are available.
",3.0
516,3,67943,1805.0852,database management system,"Mark Raasveldt, Hannes M\""uhleisen",MonetDBLite: An Embedded Analytical Database,"  While traditional RDBMSes offer a lot of advantages, they require significant
effort to setup and to use. Because of these challenges, many data scientists
and analysts have switched to using alternative data management solutions.
These alternatives, however, lack features that are standard for RDBMSes, e.g.
out-of-core query execution. In this paper, we introduce the embedded
analytical database MonetDBLite. MonetDBLite is designed to be both highly
efficient and easy to use in conjunction with standard analytical tools. It can
be installed using standard package managers, and requires no configuration or
server management. It is designed for OLAP scenarios, and offers
near-instantaneous data transfer between the database and analytical tools, all
the while maintaining the transactional guarantees and ACID properties of a
standard relational system. These properties make MonetDBLite highly suitable
as a storage engine for data used in analytics, machine learning and
classification tasks.
",5.0
517,3,82423,1902.03948,database management system,"Rebecca Wild, Matthew Hubbell, Jeremy Kepner",Scaling Big Data Platform for Big Data Pipeline,"  Monitoring and Managing High Performance Computing (HPC) systems and
environments generate an ever growing amount of data. Making sense of this data
and generating a platform where the data can be visualized for system
administrators and management to proactively identify system failures or
understand the state of the system requires the platform to be as efficient and
scalable as the underlying database tools used to store and analyze the data.
In this paper we will show how we leverage Accumulo, d4m, and Unity to generate
a 3D visualization platform to monitor and manage the Lincoln Laboratory
Supercomputer systems and how we have had to retool our approach to scale with
our systems.
",4.0
518,7,34490,1506.0482,gradient boosting,"Alina Beygelzimer, Elad Hazan, Satyen Kale and Haipeng Luo",Online Gradient Boosting,"  We extend the theory of boosting for regression problems to the online
learning setting. Generalizing from the batch setting for boosting, the notion
of a weak learning algorithm is modeled as an online learning algorithm with
linear loss functions that competes with a base class of regression functions,
while a strong learning algorithm is an online learning algorithm with convex
loss functions that competes with a larger class of regression functions. Our
main result is an online gradient boosting algorithm which converts a weak
online learning algorithm into a strong one where the larger class of functions
is the linear span of the base class. We also give a simpler boosting algorithm
that converts a weak online learning algorithm into a strong one where the
larger class of functions is the convex hull of the base class, and prove its
optimality.
",4.0
519,12,96030,1908.08147,COVID-19 and social media,"Nagendra Kumar, Rakshita Nagalla, Tanya Marwah, Manish Singh",Sentiment Dynamics in Social Media News Channels,"  Social media is currently one of the most important means of news
communication. Since people are consuming a large fraction of their daily news
through social media, most of the traditional news channels are using social
media to catch the attention of users. Each news channel has its own strategies
to attract more users. In this paper, we analyze how the news channels use
sentiment to garner users' attention in social media. We compare the sentiment
of social media news posts of television, radio and print media, to show the
differences in the ways these channels cover the news. We also analyze users'
reactions and opinion sentiment on news posts with different sentiments. We
perform our experiments on a dataset extracted from Facebook Pages of five
popular news channels. Our dataset contains 0.15 million news posts and 1.13
billion users reactions. The results of our experiments show that the sentiment
of user opinion has a strong correlation with the sentiment of the news post
and the type of information source. Our study also illustrates the differences
among the social media news channels of different types of news sources.
",0.0
520,18,30507,1412.5694,infomation retrieval time complexity,"Ramtin Pedarsani, Kangwook Lee, Kannan Ramchandran","Capacity-Approaching PhaseCode for Low-Complexity Compressive Phase
  Retrieval","  In this paper, we tackle the general compressive phase retrieval problem. The
problem is to recover a K-sparse complex vector of length n, $x\in
\mathbb{C}^n$, from the magnitudes of m linear measurements, $y=|Ax|$, where $A
\in \mathbb{C}^{m \times n}$ can be designed, and the magnitudes are taken
component-wise for vector $Ax\in \mathbb{C}^m$. We propose a variant of the
PhaseCode algorithm, and show that, using an irregular left-degree sparse-graph
code construction, the algorithm can recover almost all the K non-zero signal
components using only slightly more than 4K measurements under some mild
assumptions, with optimal time and memory complexity of ${\cal O}(K)$. It is
known that the fundamental limit for the number of measurements in compressive
phase retrieval problem is $4K - o(K)$. To the best of our knowledge, this is
the first constructive capacity-approaching compressive phase retrieval
algorithm. As a second contribution, we propose another variant of the
PhaseCode algorithm that is based on a Compressive Sensing framework involving
sparse-graph codes. Our proposed algorithm is an instance of a more powerful
""separation"" architecture that can be used to address the compressive
phase-retrieval problem in general. This modular design features a compressive
sensing outer layer, and a trigonometric-based phase-retrieval inner layer. The
compressive-sensing layer operates as a linear phase-aware compressive
measurement subsystem, while the trig-based phase-retrieval layer provides the
desired abstraction between the actually targeted nonlinear phase-retrieval
problem and the induced linear compressive-sensing problem. Invoking this
architecture based on the use of sparse-graph codes for the compressive sensing
layer, we show that we can exactly recover a signal from only the magnitudes of
its linear measurements using only slightly more than 6K measurements.
",0.0
521,15,29258,1410.3462,relevance feedback for imformation retrieval,Xirong Li,Tag Relevance Fusion for Social Image Retrieval,"  Due to the subjective nature of social tagging, measuring the relevance of
social tags with respect to the visual content is crucial for retrieving the
increasing amounts of social-networked images. Witnessing the limit of a single
measurement of tag relevance, we introduce in this paper tag relevance fusion
as an extension to methods for tag relevance estimation. We present a
systematic study, covering tag relevance fusion in early and late stages, and
in supervised and unsupervised settings. Experiments on a large present-day
benchmark set show that tag relevance fusion leads to better image retrieval.
Moreover, unsupervised tag relevance fusion is found to be practically as
effective as supervised tag relevance fusion, but without the need of any
training efforts. This finding suggests the potential of tag relevance fusion
for real-world deployment.
",4.0
522,5,30274,1412.2113,matrix completion,"Suriya Gunasekar, Makoto Yamada, Dawei Yin, Yi Chang",Consistent Collective Matrix Completion under Joint Low Rank Structure,"  We address the collective matrix completion problem of jointly recovering a
collection of matrices with shared structure from partial (and potentially
noisy) observations. To ensure well--posedness of the problem, we impose a
joint low rank structure, wherein each component matrix is low rank and the
latent space of the low rank factors corresponding to each entity is shared
across the entire collection. We first develop a rigorous algebra for
representing and manipulating collective--matrix structure, and identify
sufficient conditions for consistent estimation of collective matrices. We then
propose a tractable convex estimator for solving the collective matrix
completion problem, and provide the first non--trivial theoretical guarantees
for consistency of collective matrix completion. We show that under reasonable
assumptions stated in Section 3.1, with high probability, the proposed
estimator exactly recovers the true matrices whenever sample complexity
requirements dictated by Theorem 1 are met. The sample complexity requirement
derived in the paper are optimum up to logarithmic factors, and significantly
improve upon the requirements obtained by trivial extensions of standard matrix
completion. Finally, we propose a scalable approximate algorithm to solve the
proposed convex program, and corroborate our results through simulated
experiments.
",5.0
523,0,12523,1204.1688,learning to rank with partitioned preference,"John C. Duchi, Lester Mackey, Michael I. Jordan",The asymptotics of ranking algorithms,"  We consider the predictive problem of supervised ranking, where the task is
to rank sets of candidate items returned in response to queries. Although there
exist statistical procedures that come with guarantees of consistency in this
setting, these procedures require that individuals provide a complete ranking
of all items, which is rarely feasible in practice. Instead, individuals
routinely provide partial preference information, such as pairwise comparisons
of items, and more practical approaches to ranking have aimed at modeling this
partial preference data directly. As we show, however, such an approach raises
serious theoretical challenges. Indeed, we demonstrate that many commonly used
surrogate losses for pairwise comparison data do not yield consistency;
surprisingly, we show inconsistency even in low-noise settings. With these
negative results as motivation, we present a new approach to supervised ranking
based on aggregation of partial preferences, and we develop $U$-statistic-based
empirical risk minimization procedures. We present an asymptotic analysis of
these new procedures, showing that they yield consistency results that parallel
those available for classification. We complement our theoretical results with
an experiment studying the new procedures in a large-scale web-ranking task.
",1.0
524,8,209515,2209.07603,node embedding for graph,"Aleksandar Tom\v{c}i\'c and Milo\v{s} Savi\'c and Milo\v{s}
  Radovanovi\'c",Hub-aware Random Walk Graph Embedding Methods for Classification,"  In the last two decades we are witnessing a huge increase of valuable big
data structured in the form of graphs or networks. To apply traditional machine
learning and data analytic techniques to such data it is necessary to transform
graphs into vector-based representations that preserve the most essential
structural properties of graphs. For this purpose, a large number of graph
embedding methods have been proposed in the literature. Most of them produce
general-purpose embeddings suitable for a variety of applications such as node
clustering, node classification, graph visualisation and link prediction. In
this paper, we propose two novel graph embedding algorithms based on random
walks that are specifically designed for the node classification problem.
Random walk sampling strategies of the proposed algorithms have been designed
to pay special attention to hubs -- high-degree nodes that have the most
critical role for the overall connectedness in large-scale graphs. The proposed
methods are experimentally evaluated by analyzing the classification
performance of three classification algorithms trained on embeddings of
real-world networks. The obtained results indicate that our methods
considerably improve the predictive power of examined classifiers compared to
currently the most popular random walk method for generating general-purpose
graph embeddings (node2vec).
",4.0
525,11,22932,1311.0339,PageRank for web search,"Sonali Gupta, Komal Kumar Bhatia","A Novel Term Weighing Scheme Towards Efficient Crawl of Textual
  Databases","  The Hidden Web is the vast repository of informational databases available
only through search form interfaces, accessible by therein typing a set of
keywords in the search forms. Typically, a Hidden Web crawler is employed to
autonomously discover and download pages from the Hidden Web. Traditional
hidden web crawlers do not provide the search engines with an optimal search
experience because of the excessive number of search requests posed through the
form interface so as to exhaustively crawl and retrieve the contents of the
target hidden web database. Here in our work, we provide a framework to
investigate the problem of optimal search and curtail it by proposing an
effective query term selection approach based on the frequency & distribution
of terms in the document database. The paper focuses on developing a
term-weighing scheme called VarDF (acronym for variable document frequency)
that can ease the identification of optimal terms to be used as queries on the
interface for maximizing the achieved coverage of the crawler which in turn
will facilitate the search engine to have a diversified and expanded index. We
experimentally evaluate the effectiveness of our approach on a manually created
database of documents in the area of Information Retrieval.
",1.0
526,4,198396,2206.00962,pre-trained language model,"Juuso Eronen, Michal Ptaszynski, Fumito Masui, Masaki Arata, Gniewosz
  Leliwa, Michal Wroczynski","Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language
  Detection","  We study the selection of transfer languages for automatic abusive language
detection. Instead of preparing a dataset for every language, we demonstrate
the effectiveness of cross-lingual transfer learning for zero-shot abusive
language detection. This way we can use existing data from higher-resource
languages to build better detection systems for low-resource languages. Our
datasets are from seven different languages from three language families. We
measure the distance between the languages using several language similarity
measures, especially by quantifying the World Atlas of Language Structures. We
show that there is a correlation between linguistic similarity and classifier
performance. This discovery allows us to choose an optimal transfer language
for zero shot abusive language detection.
",3.0
527,16,97168,1909.03069,activation function in neutral networks,"MohamadAli Torkamani, Shiv Shankar, Amirmohammad Rooshenas, Phillip
  Wallis","Differential Equation Units: Learning Functional Forms of Activation
  Functions from Data","  Most deep neural networks use simple, fixed activation functions, such as
sigmoids or rectified linear units, regardless of domain or network structure.
We introduce differential equation units (DEUs), an improvement to modern
neural networks, which enables each neuron to learn a particular nonlinear
activation function from a family of solutions to an ordinary differential
equation. Specifically, each neuron may change its functional form during
training based on the behavior of the other parts of the network. We show that
using neurons with DEU activation functions results in a more compact network
capable of achieving comparable, if not superior, performance when is compared
to much larger networks.
",4.0
528,10,130022,2009.00611,web archive,"Krutarth Patel, Cornelia Caragea, Mark Phillips, Nathaniel Fox",Identifying Documents In-Scope of a Collection from Web Archives,"  Web archive data usually contains high-quality documents that are very useful
for creating specialized collections of documents, e.g., scientific digital
libraries and repositories of technical reports. In doing so, there is a
substantial need for automatic approaches that can distinguish the documents of
interest for a collection out of the huge number of documents collected by web
archiving institutions. In this paper, we explore different learning models and
feature representations to determine the best performing ones for identifying
the documents of interest from the web archived data. Specifically, we study
both machine learning and deep learning models and ""bag of words"" (BoW)
features extracted from the entire document or from specific portions of the
document, as well as structural features that capture the structure of
documents. We focus our evaluation on three datasets that we created from three
different Web archives. Our experimental results show that the BoW classifiers
that focus only on specific portions of the documents (rather than the full
text) outperform all compared methods on all three datasets.
",3.0
529,3,216828,cs/0305056,database management system,"R. Bartoldus, G. Dubois-Felsmann, Y. Kolomensky, A. Salnikov",Configuration Database for BaBar On-line,"  The configuration database is one of the vital systems in the BaBar on-line
system. It provides services for the different parts of the data acquisition
system and control system, which require run-time parameters. The original
design and implementation of the configuration database played a significant
role in the successful BaBar operations since the beginning of experiment.
Recent additions to the design of the configuration database provide better
means for the management of data and add new tools to simplify main
configuration tasks. We describe the design of the configuration database, its
implementation with the Objectivity/DB object-oriented database, and our
experience collected during the years of operation.
",5.0
530,11,12498,1204.1413,PageRank for web search,"Pushpa R. Suri, Harmunish Taneja","An integrated ranking algorithm for efficient information computing in
  social networks","  Social networks have ensured the expanding disproportion between the face of
WWW stored traditionally in search engine repositories and the actual ever
changing face of Web. Exponential growth of web users and the ease with which
they can upload contents on web highlights the need of content controls on
material published on the web. As definition of search is changing,
socially-enhanced interactive search methodologies are the need of the hour.
Ranking is pivotal for efficient web search as the search performance mainly
depends upon the ranking results. In this paper new integrated ranking model
based on fused rank of web object based on popularity factor earned over only
valid interlinks from multiple social forums is proposed. This model identifies
relationships between web objects in separate social networks based on the
object inheritance graph. Experimental study indicates the effectiveness of
proposed Fusion based ranking algorithm in terms of better search results.
",2.0
531,2,42345,1605.03391,random forests,"Marvin N. Wright, Theresa Dankowski and Andreas Ziegler","Unbiased split variable selection for random survival forests using
  maximally selected rank statistics","  The most popular approach for analyzing survival data is the Cox regression
model. The Cox model may, however, be misspecified, and its proportionality
assumption may not always be fulfilled. An alternative approach for survival
prediction is random forests for survival outcomes. The standard split
criterion for random survival forests is the log-rank test statistics, which
favors splitting variables with many possible split points. Conditional
inference forests avoid this split variable selection bias. However, linear
rank statistics are utilized by default in conditional inference forests to
select the optimal splitting variable, which cannot detect non-linear effects
in the independent variables. An alternative is to use maximally selected rank
statistics for the split point selection. As in conditional inference forests,
splitting variables are compared on the p-value scale. However, instead of the
conditional Monte-Carlo approach used in conditional inference forests, p-value
approximations are employed. We describe several p-value approximations and the
implementation of the proposed random forest approach. A simulation study
demonstrates that unbiased split variable selection is possible. However, there
is a trade-off between unbiased split variable selection and runtime. In
benchmark studies of prediction performance on simulated and real datasets the
new method performs better than random survival forests if informative
dichotomous variables are combined with uninformative variables with more
categories and better than conditional inference forests if non-linear
covariate effects are included. In a runtime comparison the method proves to be
computationally faster than both alternatives, if a simple p-value
approximation is used.
",4.0
532,10,162380,2107.00893,web archive,"Michael V\""olske, Janek Bevendorff, Johannes Kiesel, Benno Stein, Maik
  Fr\""obe, Matthias Hagen, Martin Potthast",Web Archive Analytics,"  Web archive analytics is the exploitation of publicly accessible web pages
and their evolution for research purposes -- to the extent organizationally
possible for researchers. In order to better understand the complexity of this
task, the first part of this paper puts the entirety of the world's captured,
created, and replicated data (the ""Global Datasphere"") in relation to other
important data sets such as the public internet and its web pages, or what is
preserved thereof by the Internet Archive.
  Recently, the Webis research group, a network of university chairs to which
the authors belong, concluded an agreement with the Internet Archive to
download a substantial part of its web archive for research purposes. The
second part of the paper in hand describes our infrastructure for processing
this data treasure: We will eventually host around 8 PB of web archive data
from the Internet Archive and Common Crawl, with the goal of supplementing
existing large scale web corpora and forming a non-biased subset of the 30 PB
web archive at the Internet Archive.
",3.0
533,1,14747,1208.1011,advanced search engine,Dirk Lewandowski,Credibility in Web Search Engines,"  Web search engines apply a variety of ranking signals to achieve user
satisfaction, i.e., results pages that provide the best-possible results to the
user. While these ranking signals implicitly consider credibility (e.g., by
measuring popularity), explicit measures of credibility are not applied. In
this chapter, credibility in Web search engines is discussed in a broad
context: credibility as a measure for including documents in a search engine's
index, credibility as a ranking signal, credibility in the context of universal
search results, and the possibility of using credibility as an explicit measure
for ranking purposes. It is found that while search engines-at least to a
certain extent-show credible results to their users, there is no fully
integrated credibility framework for Web search engines.
",4.0
534,6,96957,1909.01772,query expansion for imformation retrieval,"Tommaso Teofili, Niyati Chhaya",Affect Enriched Word Embeddings for News Information Retrieval,"  Distributed representations of words have shown to be useful to improve the
effectiveness of IR systems in many sub-tasks like query expansion, retrieval
and ranking. Algorithms like word2vec, GloVe and others are also key factors in
many improvements in different NLP tasks. One common issue with such embedding
models is that words like happy and sad appear in similar contexts and hence
are wrongly clustered close in the embedding space. In this paper we leverage
Aff2Vec, a set of word embeddings models which include affect information, in
order to better capture the affect aspect in news text to achieve better
results in information retrieval tasks, also such embeddings are less hit by
the synonym/antonym issue. We evaluate their effectiveness on two IR related
tasks (query expansion and ranking) over the New York Times dataset (TREC-core
'17) comparing them against other word embeddings based models and classic
ranking models.
",2.0
535,10,91572,1906.07104,web archive,"Sawood Alam, Michele C. Weigle, Michael L. Nelson, Martin Klein, and
  Herbert Van de Sompel",Supporting Web Archiving via Web Packaging,"  We describe challenges related to web archiving, replaying archived web
resources, and verifying their authenticity. We show that Web Packaging has
significant potential to help address these challenges and identify areas in
which changes are needed in order to fully realize that potential.
",5.0
536,16,187187,2202.12065,activation function in neutral networks,Vipul Bansal,Activation Functions: Dive into an optimal activation function,"  Activation functions have come up as one of the essential components of
neural networks. The choice of adequate activation function can impact the
accuracy of these methods. In this study, we experiment for finding an optimal
activation function by defining it as a weighted sum of existing activation
functions and then further optimizing these weights while training the network.
The study uses three activation functions, ReLU, tanh, and sin, over three
popular image datasets, MNIST, FashionMNIST, and KMNIST. We observe that the
ReLU activation function can easily overlook other activation functions. Also,
we see that initial layers prefer to have ReLU or LeakyReLU type of activation
functions, but deeper layers tend to prefer more convergent activation
functions.
",5.0
537,19,166252,2108.05349,artificial intelligence for low carbon,Carlos Gershenson,"Intelligence as information processing: brains, swarms, and computers","  There is no agreed definition of intelligence, so it is problematic to simply
ask whether brains, swarms, computers, or other systems are intelligent or not.
To compare the potential intelligence exhibited by different cognitive systems,
I use the common approach used by artificial intelligence and artificial life:
Instead of studying the substrate of systems, let us focus on their
organization. This organization can be measured with information. Thus, I apply
an informationist epistemology to describe cognitive systems, including brains
and computers. This allows me to frame the usefulness and limitations of the
brain-computer analogy in different contexts. I also use this perspective to
discuss the evolution and ecology of intelligence.
",0.0
538,1,14498,1207.4701,advanced search engine,"Chenyang Li, Mingyi Hong, Randy Cogill and Alfredo Garcia",An Adaptive Online Ad Auction Scoring Algorithm for Revenue Maximization,"  Sponsored search becomes an easy platform to match potential consumers'
intent with merchants' advertising. Advertisers express their willingness to
pay for each keyword in terms of bids to the search engine. When a user's query
matches the keyword, the search engine evaluates the bids and allocates slots
to the advertisers that are displayed along side the unpaid algorithmic search
results. The advertiser only pays the search engine when its ad is clicked by
the user and the price-per-click is determined by the bids of other competing
advertisers.
",5.0
539,9,109261,2002.00761,language model for long documents,"Ahmed El-Kishky, Francisco Guzm\'an","Massively Multilingual Document Alignment with Cross-lingual
  Sentence-Mover's Distance","  Document alignment aims to identify pairs of documents in two distinct
languages that are of comparable content or translations of each other. Such
aligned data can be used for a variety of NLP tasks from training cross-lingual
representations to mining parallel data for machine translation. In this paper
we develop an unsupervised scoring function that leverages cross-lingual
sentence embeddings to compute the semantic distance between documents in
different languages. These semantic distances are then used to guide a document
alignment algorithm to properly pair cross-lingual web documents across a
variety of low, mid, and high-resource language pairs. Recognizing that our
proposed scoring function and other state of the art methods are
computationally intractable for long web documents, we utilize a more tractable
greedy algorithm that performs comparably. We experimentally demonstrate that
our distance metric performs better alignment than current baselines
outperforming them by 7% on high-resource language pairs, 15% on mid-resource
language pairs, and 22% on low-resource language pairs.
",1.0
540,4,9519,1108.1275,pre-trained language model,R. A. Blythe,Neutral evolution: A null model for language dynamics,"  We review the task of aligning simple models for language dynamics with
relevant empirical data, motivated by the fact that this is rarely attempted in
practice despite an abundance of abstract models. We propose that one way to
meet this challenge is through the careful construction of null models. We
argue in particular that rejection of a null model must have important
consequences for theories about language dynamics if modelling is truly to be
worthwhile. Our main claim is that the stochastic process of neutral evolution
(also known as genetic drift or random copying) is a viable null model for
language dynamics. We survey empirical evidence in favour and against neutral
evolution as a mechanism behind historical language changes, highlighting the
theoretical implications in each case.
",3.0
541,12,93810,1907.0724,COVID-19 and social media,"Ganesh Nalluru, Rahul Pandey, and Hemant Purohit","Relevancy Classification of Multimodal Social Media Streams for
  Emergency Services","  Social media has become an integral part of our daily lives. During
time-critical events, the public shares a variety of posts on social media
including reports for resource needs, damages, and help offerings for the
affected community. Such posts can be relevant and may contain valuable
situational awareness information. However, the information overload of social
media challenges the timely processing and extraction of relevant information
by the emergency services. Furthermore, the growing usage of multimedia content
in the social media posts in recent years further adds to the challenge in
timely mining relevant information from social media. In this paper, we present
a novel method for multimodal relevancy classification of social media posts,
where relevancy is defined with respect to the information needs of emergency
management agencies. Specifically, we experiment with the combination of
semantic textual features with the image features to efficiently classify a
relevant multimodal social media post. We validate our method using an
evaluation of classifying the data from three real-world crisis events. Our
experiments demonstrate that features based on the proposed hybrid framework of
exploiting both textual and image content improve the performance of
identifying relevant posts. In the light of these experiments, the application
of the proposed classification method could reduce cognitive load on emergency
services, in filtering multimodal public posts at large scale.
",1.0
542,18,46322,1610.01366,infomation retrieval time complexity,"Giambattista Amati and Simone Angelini and Marco Bianchi and Luca
  Costantini and Giuseppe Marcone",A cumulative approach to quantification for sentiment analysis,"  We estimate sentiment categories proportions for retrieval within large
retrieval sets. In general, estimates are produced by counting the
classification outcomes and then by adjusting such category sizes taking into
account misclassification error matrix. However, both the accuracy of the
classifier and the precision of the retrieval produce a large number of errors
that makes difficult the application of an aggregative approach to sentiment
analysis as a reliable and efficient estimation of proportions for sentiment
categories.
  The challenge for real time analytics during retrieval is thus to overcome
misclassification errors, and more importantly, to apply sentiment
classification or any other similar post-processing analytics at retrieval
time. We present a non-aggregative approach that can be applied to very large
retrieval sets of queries.
",1.0
543,6,163870,2107.07773,query expansion for imformation retrieval,"Yizhi Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu",More Robust Dense Retrieval with Contrastive Dual Learning,"  Dense retrieval conducts text retrieval in the embedding space and has shown
many advantages compared to sparse retrieval. Existing dense retrievers
optimize representations of queries and documents with contrastive training and
map them to the embedding space. The embedding space is optimized by aligning
the matched query-document pairs and pushing the negative documents away from
the query. However, in such training paradigm, the queries are only optimized
to align to the documents and are coarsely positioned, leading to an
anisotropic query embedding space. In this paper, we analyze the embedding
space distributions and propose an effective training paradigm, Contrastive
Dual Learning for Approximate Nearest Neighbor (DANCE) to learn fine-grained
query representations for dense retrieval. DANCE incorporates an additional
dual training object of query retrieval, inspired by the classic information
retrieval training axiom, query likelihood. With contrastive learning, the dual
training object of DANCE learns more tailored representations for queries and
documents to keep the embedding space smooth and uniform, thriving on the
ranking performance of DANCE on the MS MARCO document retrieval task. Different
from ANCE that only optimized with the document retrieval task, DANCE
concentrates the query embeddings closer to document representations while
making the document distribution more discriminative. Such concentrated query
embedding distribution assigns more uniform negative sampling probabilities to
queries and helps to sufficiently optimize query representations in the query
retrieval task. Our codes are released at https://github.com/thunlp/DANCE.
",2.0
544,18,105422,1912.04107,infomation retrieval time complexity,"S\'ebastien D\'ejean, Radu Tudor Ionescu, Josiane Mothe, Md Zia Ullah",Forward and Backward Feature Selection for Query Performance Prediction,"  The goal of query performance prediction (QPP) is to automatically estimate
the effectiveness of a search result for any given query, without relevance
judgements. Post-retrieval features have been shown to be more effective for
this task while being more expensive to compute than pre-retrieval features.
Combining multiple post-retrieval features is even more effective, but
state-of-the-art QPP methods are impossible to interpret because of the
black-box nature of the employed machine learning models. However,
interpretation is useful for understanding the predictive model and providing
more answers about its behavior. Moreover, combining many post-retrieval
features is not applicable to real-world cases, since the query running time is
of utter importance. In this paper, we investigate a new framework for feature
selection in which the trained model explains well the prediction. We introduce
a step-wise (forward and backward) model selection approach where different
subsets of query features are used to fit different models from which the
system selects the best one. We evaluate our approach on four TREC collections
using standard QPP features. We also develop two QPP features to address the
issue of query-drift in the query feedback setting. We found that: (1) our
model based on a limited number of selected features is as good as more complex
models for QPP and better than non-selective models; (2) our model is more
efficient than complex models during inference time since it requires fewer
features; (3) the predictive model is readable and understandable; and (4) one
of our new QPP features is consistently selected across different collections,
proving its usefulness.
",2.0
545,19,117043,2004.13351,artificial intelligence for low carbon,"Kai Yang, Yong Zhou, Zhanpeng Yang, Yuanming Shi",Communication-Efficient Edge AI Inference Over Wireless Networks,"  Given the fast growth of intelligent devices, it is expected that a large
number of high-stake artificial intelligence (AI) applications, e.g., drones,
autonomous cars, tactile robots, will be deployed at the edge of wireless
networks in the near future. As such, the intelligent communication networks
will be designed to leverage advanced wireless techniques and edge computing
technologies to support AI-enabled applications at various end devices with
limited communication, computation, hardware and energy resources. In this
article, we shall present the principles of efficient deployment of model
inference at network edge to provide low-latency and energy-efficient AI
services. This includes the wireless distributed computing framework for
low-latency device distributed model inference as well as the wireless
cooperative transmission strategy for energy-efficient edge cooperative model
inference. The communication efficiency of edge inference systems is further
improved by building up a smart radio propagation environment via intelligent
reflecting surface.
",0.0
546,12,12089,1203.1647,COVID-19 and social media,Sheng Yu and Subhash Kak,A Survey of Prediction Using Social Media,"  Social media comprises interactive applications and platforms for creating,
sharing and exchange of user-generated contents. The past ten years have
brought huge growth in social media, especially online social networking
services, and it is changing our ways to organize and communicate. It
aggregates opinions and feelings of diverse groups of people at low cost.
Mining the attributes and contents of social media gives us an opportunity to
discover social structure characteristics, analyze action patterns
qualitatively and quantitatively, and sometimes the ability to predict future
human related events. In this paper, we firstly discuss the realms which can be
predicted with current social media, then overview available predictors and
techniques of prediction, and finally discuss challenges and possible future
directions.
",1.0
547,1,36789,1510.00819,advanced search engine,Jai Manral,Intelligent Search Optimization using Artificial Fuzzy Logics,"  Information on the web is prodigious; searching relevant information is
difficult making web users to rely on search engines for finding relevant
information on the web. Search engines index and categorize web pages according
to their contents using crawlers and rank them accordingly. For given user
query they retrieve millions of webpages and display them to users according to
web-page rank. Every search engine has their own algorithms based on certain
parameters for ranking web-pages. Search Engine Optimization (SEO) is that
technique by which webmasters try to improve ranking of their websites by
optimizing it according to search engines ranking parameters. It is the aim of
this research to identify the most popular SEO techniques used by search
engines for ranking web-pages and to establish their importance for indexing
and categorizing web data. The research tries to establish that using more SEO
parameters in ranking algorithms helps in retrieving better search results thus
increasing user satisfaction.
  In the accomplished research, a web based Meta search engine is proposed to
aggregates search results from different search engines and rank web-pages
based on new page ranking algorithm which will assign heuristic page rank to
web-pages based on SEO parameters such as title tag, Meta description, sitemap
etc. The research also provides insight into techniques which webmasters can
use for better ranking their websites in Google and Bing.
  Initial results has shown that using certain SEO parameters in present
ranking algorithm helps in retrieving more useful results for user queries.
These results generated from Meta search engine outperformed existing search
engines in terms of better retrieved search results and high precision.
",5.0
548,9,162666,2107.02192,language model for long documents,"Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein,
  Anima Anandkumar, Bryan Catanzaro",Long-Short Transformer: Efficient Transformers for Language and Vision,"  Transformers have achieved success in both language and vision domains.
However, it is prohibitively expensive to scale them to long sequences such as
long documents or high-resolution images, because self-attention mechanism has
quadratic time and memory complexities with respect to the input sequence
length. In this paper, we propose Long-Short Transformer (Transformer-LS), an
efficient self-attention mechanism for modeling long sequences with linear
complexity for both language and vision tasks. It aggregates a novel long-range
attention with dynamic projection to model distant correlations and a
short-term attention to capture fine-grained local correlations. We propose a
dual normalization strategy to account for the scale mismatch between the two
attention mechanisms. Transformer-LS can be applied to both autoregressive and
bidirectional models without additional complexity. Our method outperforms the
state-of-the-art models on multiple tasks in language and vision domains,
including the Long Range Arena benchmark, autoregressive language modeling, and
ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on
enwik8 using half the number of parameters than previous method, while being
faster and is able to handle 3x as long sequences compared to its
full-attention version on the same hardware. On ImageNet, it can obtain the
state-of-the-art results (e.g., a moderate size of 55.8M model solely trained
on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more
scalable on high-resolution images. The source code and models are released at
https://github.com/NVIDIA/transformer-ls .
",5.0
549,11,72999,1809.00939,PageRank for web search,"Ziliang Lai, Chris Liu, Eric Lo, Ben Kao, and Siu-Ming Yiu",Decentralized Search on Decentralized Web,"  Decentralized Web, or DWeb, is envisioned as a promising future of the Web.
Being decentralized, there are no dedicated web servers in DWeb; Devices that
retrieve web contents also serve their cached data to peer devices with
straight privacy-preserving mechanisms. The fact that contents in DWeb are
distributed, replicated, and decentralized lead to a number of key advantages
over the conventional web. These include better resiliency against network
partitioning and distributed-denial-of-service attacks (DDoS), and better
browsing experiences in terms of shorter latency and higher throughput.
Moreover, DWeb provides tamper-proof contents because each content piece is
uniquely identified by a cryptographic hash. DWeb also clicks well with future
Internet architectures, such as Named Data Networking (NDN).Search engines have
been an inseparable element of the Web. Contemporary (""Web 2.0"") search
engines, however, provide centralized services. They are thus subject to DDoS
attacks, insider threat, and ethical issues like search bias and censorship. As
the web moves from being centralized to being decentralized, search engines
ought to follow. We propose QueenBee, a decentralized search engine for DWeb.
QueenBee is so named because worker bees and honeycomb are a common metaphor
for distributed architectures, with the queen being the one that holds the
colony together. QueenBee aims to revolutionize the search engine business
model by offering incentives to both content providers and peers that
participate in QueenBee's page indexing and ranking operations.
",1.0
550,1,23650,1312.4036,advanced search engine,Apoorv Narang and Srikanta Bedathur,"Mind Your Language: Effects of Spoken Query Formulation on Retrieval
  Effectiveness","  Voice search is becoming a popular mode for interacting with search engines.
As a result, research has gone into building better voice transcription
engines, interfaces, and search engines that better handle inherent verbosity
of queries. However, when one considers its use by non- native speakers of
English, another aspect that becomes important is the formulation of the query
by users. In this paper, we present the results of a preliminary study that we
conducted with non-native English speakers who formulate queries for given
retrieval tasks. Our results show that the current search engines are sensitive
in their rankings to the query formulation, and thus highlights the need for
developing more robust ranking methods.
",5.0
551,7,122854,2006.11014,gradient boosting,Andrei V. Konstantinov and Lev V. Utkin,Gradient boosting machine with partially randomized decision trees,"  The gradient boosting machine is a powerful ensemble-based machine learning
method for solving regression problems. However, one of the difficulties of its
using is a possible discontinuity of the regression function, which arises when
regions of training data are not densely covered by training points. In order
to overcome this difficulty and to reduce the computational complexity of the
gradient boosting machine, we propose to apply the partially randomized trees
which can be regarded as a special case of the extremely randomized trees
applied to the gradient boosting. The gradient boosting machine with the
partially randomized trees is illustrated by means of many numerical examples
using synthetic and real data.
",4.0
552,6,114410,2003.13624,query expansion for imformation retrieval,"Jeffrey Dalton, Chenyan Xiong, Jamie Callan",TREC CAsT 2019: The Conversational Assistance Track Overview,"  The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to
facilitate Conversational Information Seeking (CIS) research and to create a
large-scale reusable test collection for conversational search systems. The
document corpus is 38,426,252 passages from the TREC Complex Answer Retrieval
(CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty
information seeking dialogues (30 train, 50 test) are an average of 9 to 10
questions long. Relevance assessments are provided for 30 training topics and
20 test topics. This year 21 groups submitted a total of 65 runs using varying
methods for conversational query understanding and ranking. Methods include
traditional retrieval based methods, feature based learning-to-rank, neural
models, and knowledge enhanced methods. A common theme through the runs is the
use of BERT-based neural reranking methods. Leading methods also employed
document expansion, conversational query expansion, and generative language
models for conversational query rewriting (GPT-2). The results show a gap
between automatic systems and those using the manually resolved utterances,
with a 35% relative improvement of manual rewrites over the best automatic
system.
",1.0
553,11,8132,1103.5231,PageRank for web search,"Linyuan Lu, Yi-Cheng Zhang, Chi Ho Yeung, Tao Zhou","Leaders in Social Networks, the Delicious Case","  Finding pertinent information is not limited to search engines. Online
communities can amplify the influence of a small number of power users for the
benefit of all other users. Users' information foraging in depth and breadth
can be greatly enhanced by choosing suitable leaders. For instance in
delicious.com, users subscribe to leaders' collection which lead to a deeper
and wider reach not achievable with search engines. To consolidate such
collective search, it is essential to utilize the leadership topology and
identify influential users. Google's PageRank, as a successful search algorithm
in the World Wide Web, turns out to be less effective in networks of people. We
thus devise an adaptive and parameter-free algorithm, the LeaderRank, to
quantify user influence. We show that LeaderRank outperforms PageRank in terms
of ranking effectiveness, as well as robustness against manipulations and noisy
data. These results suggest that leaders who are aware of their clout may
reinforce the development of social networks, and thus the power of collective
search.
",4.0
554,0,151066,2103.12982,learning to rank with partitioned preference,"Rui Li, Yunjiang Jiang, Wenyun Yang, Guoyu Tang, Songlin Wang, Chaoyi
  Ma, Wei He, Xi Xiong, Yun Xiao, Eric Yihong Zhao","From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in
  E-commerce Search","  We introduce deep learning models to the two most important stages in product
search at JD.com, one of the largest e-commerce platforms in the world.
Specifically, we outline the design of a deep learning system that retrieves
semantically relevant items to a query within milliseconds, and a pairwise deep
re-ranking system, which learns subtle user preferences. Compared to
traditional search systems, the proposed approaches are better at semantic
retrieval and personalized ranking, achieving significant improvements.
",1.0
555,5,43762,1606.08009,matrix completion,"Ashkan Esmaeili, Arash Amini, and Farokh Marvasti",Fast Methods for Recovering Sparse Parameters in Linear Low Rank Models,"  In this paper, we investigate the recovery of a sparse weight vector
(parameters vector) from a set of noisy linear combinations. However, only
partial information about the matrix representing the linear combinations is
available. Assuming a low-rank structure for the matrix, one natural solution
would be to first apply a matrix completion on the data, and then to solve the
resulting compressed sensing problem. In big data applications such as massive
MIMO and medical data, the matrix completion step imposes a huge computational
burden. Here, we propose to reduce the computational cost of the completion
task by ignoring the columns corresponding to zero elements in the sparse
vector. To this end, we employ a technique to initially approximate the support
of the sparse vector. We further propose to unify the partial matrix completion
and sparse vector recovery into an augmented four-step problem. Simulation
results reveal that the augmented approach achieves the best performance, while
both proposed methods outperform the natural two-step technique with
substantially less computational requirements.
",4.0
556,1,157030,2105.10124,advanced search engine,"Jianghong Zhou, Eugene Agichtein",RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search,"  To support complex search tasks, where the initial information requirements
are complex or may change during the search, a search engine must adapt the
information delivery as the user's information requirements evolve. To support
this dynamic ranking paradigm effectively, search result ranking must
incorporate both the user feedback received, and the information displayed so
far. To address this problem, we introduce a novel reinforcement learning-based
approach, RLIrank. We first build an adapted reinforcement learning framework
to integrate the key components of the dynamic search. Then, we implement a new
Learning to Rank (LTR) model for each iteration of the dynamic search, using a
recurrent Long Short Term Memory neural network (LSTM), which estimates the
gain for each next result, learning from each previously ranked document. To
incorporate the user's feedback, we develop a word-embedding variation of the
classic Rocchio Algorithm, to help guide the ranking towards the high-value
documents. Those innovations enable RLIrank to outperform the previously
reported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the
methods in 2016 TREC Dynamic Domain after multiple search iterations, advancing
the state of the art for dynamic search.
",4.0
557,4,147249,2102.08357,pre-trained language model,"Shengjie Luo, Kaiyuan Gao, Shuxin Zheng, Guolin Ke, Di He, Liwei Wang,
  Tie-Yan Liu",Revisiting Language Encoding in Learning Multilingual Representations,"  Transformer has demonstrated its great power to learn contextual word
representations for multiple languages in a single model. To process
multilingual sentences in the model, a learnable vector is usually assigned to
each language, which is called ""language embedding"". The language embedding can
be either added to the word embedding or attached at the beginning of the
sentence. It serves as a language-specific signal for the Transformer to
capture contextual representations across languages. In this paper, we revisit
the use of language embedding and identify several problems in the existing
formulations. By investigating the interaction between language embedding and
word embedding in the self-attention module, we find that the current methods
cannot reflect the language-specific word correlation well. Given these
findings, we propose a new approach called Cross-lingual Language Projection
(XLP) to replace language embedding. For a sentence, XLP projects the word
embeddings into language-specific semantic space, and then the projected
embeddings will be fed into the Transformer model to process with their
language-specific meanings. In such a way, XLP achieves the purpose of
appropriately encoding ""language"" in a multilingual Transformer model.
Experimental results show that XLP can freely and significantly boost the model
performance on extensive multilingual benchmark datasets. Codes and models will
be released at https://github.com/lsj2408/XLP.
",5.0
558,18,60932,1712.03278,infomation retrieval time complexity,"Milad Bakhshizadeh, Arian Maleki, Shirin Jalali",Using Black-box Compression Algorithms for Phase Retrieval,"  Compressive phase retrieval refers to the problem of recovering a structured
$n$-dimensional complex-valued vector from its phase-less under-determined
linear measurements. The non-linearity of measurements makes designing
theoretically-analyzable efficient phase retrieval algorithms challenging. As a
result, to a great extent, algorithms designed in this area are developed to
take advantage of simple structures such as sparsity and its convex
generalizations. The goal of this paper is to move beyond simple models through
employing compression codes. Such codes are typically developed to take
advantage of complex signal models to represent the signals as efficiently as
possible. In this work, it is shown how an existing compression code can be
treated as a black box and integrated into an efficient solution for phase
retrieval. First, COmpressive PhasE Retrieval (COPER) optimization, a
computationally-intensive compression-based phase retrieval method, is
proposed. COPER provides a theoretical framework for studying compression-based
phase retrieval. The number of measurements required by COPER is connected to
$\kappa$, the $\alpha$-dimension (closely related to the rate-distortion
dimension) of the given family of compression codes. To finds the solution of
COPER, an efficient iterative algorithm called gradient descent for COPER
(GD-COPER) is proposed. It is proven that under some mild conditions on the
initialization, if the number of measurements is larger than $ C \kappa^2
\log^2 n$, where $C$ is a constant, GD-COPER obtains an accurate estimate of
the input vector in polynomial time. In the simulation results, JPEG2000 is
integrated in GD-COPER to confirm the superb performance of the resulting
algorithm on real-world images.
",0.0
559,19,155374,2105.02198,artificial intelligence for low carbon,"Tyler Millhouse, Melanie Moses, Melanie Mitchell","Foundations of Intelligence in Natural and Artificial Systems: A
  Workshop Report","  In March of 2021, the Santa Fe Institute hosted a workshop as part of its
Foundations of Intelligence in Natural and Artificial Systems project. This
project seeks to advance the field of artificial intelligence by promoting
interdisciplinary research on the nature of intelligence. During the workshop,
speakers from diverse disciplines gathered to develop a taxonomy of
intelligence, articulating their own understanding of intelligence and how
their research has furthered that understanding. In this report, we summarize
the insights offered by each speaker and identify the themes that emerged
during the talks and subsequent discussions.
",1.0
560,19,170481,2109.12584,artificial intelligence for low carbon,"Mirza Yusuf, Praatibh Surana, Gauri Gupta and Krithika Ramesh","Curb Your Carbon Emissions: Benchmarking Carbon Emissions in Machine
  Translation","  In recent times, there has been definitive progress in the field of NLP, with
its applications growing as the utility of our language models increases with
advances in their performance. However, these models require a large amount of
computational power and data to train, consequently leading to large carbon
footprints. Therefore, it is imperative that we study the carbon efficiency and
look for alternatives to reduce the overall environmental impact of training
models, in particular large language models. In our work, we assess the
performance of models for machine translation, across multiple language pairs
to assess the difference in computational power required to train these models
for each of these language pairs and examine the various components of these
models to analyze aspects of our pipeline that can be optimized to reduce these
carbon emissions.
",2.0
561,12,151094,2103.13187,COVID-19 and social media,Shreyash Arya,The Influence of Social Networks on Human Society,"  This report gives a brief overview of the origin of social networks and their
most popular manifestation in the modern era - the Online Social Networks
(OSNs) or social media. It further discusses the positive and negative
implications of OSNs on human society. The coupling of Data Science and social
media (social media mining) is then put forward as a powerful tool to overcome
the current challenges and pave the path for futuristic advancements
",1.0
562,4,83369,1902.10339,pre-trained language model,"Wenhu Chen, Yu Su, Yilin Shen, Zhiyu Chen, Xifeng Yan, William Wang","How Large a Vocabulary Does Text Classification Need? A Variational
  Approach to Vocabulary Selection","  With the rapid development in deep learning, deep neural networks have been
widely adopted in many real-life natural language applications. Under deep
neural networks, a pre-defined vocabulary is required to vectorize text inputs.
The canonical approach to select pre-defined vocabulary is based on the word
frequency, where a threshold is selected to cut off the long tail distribution.
However, we observed that such simple approach could easily lead to under-sized
vocabulary or over-sized vocabulary issues. Therefore, we are interested in
understanding how the end-task classification accuracy is related to the
vocabulary size and what is the minimum required vocabulary size to achieve a
specific performance. In this paper, we provide a more sophisticated
variational vocabulary dropout (VVD) based on variational dropout to perform
vocabulary selection, which can intelligently select the subset of the
vocabulary to achieve the required performance. To evaluate different
algorithms on the newly proposed vocabulary selection problem, we propose two
new metrics: Area Under Accuracy-Vocab Curve and Vocab Size under X\% Accuracy
Drop. Through extensive experiments on various NLP classification tasks, our
variational framework is shown to significantly outperform the frequency-based
and other selection baselines on these metrics.
",3.0
563,1,217335,cs/0501086,advanced search engine,Peter M. Kruse and Andre Naujoks and Dietmar Roesner and Manuela Kunze,Clever Search: A WordNet Based Wrapper for Internet Search Engines,"  This paper presents an approach to enhance search engines with information
about word senses available in WordNet. The approach exploits information about
the conceptual relations within the lexical-semantic net. In the wrapper for
search engines presented, WordNet information is used to specify user's request
or to classify the results of a publicly available web search engine, like
google, yahoo, etc.
",4.0
564,13,98160,1909.08349,social network analysis with natrual language processing,"Gaurav Verma, Balaji Vasan Srinivasan","A Lexical, Syntactic, and Semantic Perspective for Understanding Style
  in Text","  With a growing interest in modeling inherent subjectivity in natural
language, we present a linguistically-motivated process to understand and
analyze the writing style of individuals from three perspectives: lexical,
syntactic, and semantic. We discuss the stylistically expressive elements
within each of these levels and use existing methods to quantify the linguistic
intuitions related to some of these elements. We show that such a multi-level
analysis is useful for developing a well-knit understanding of style - which is
independent of the natural language task at hand, and also demonstrate its
value in solving three downstream tasks: authors' style analysis, authorship
attribution, and emotion prediction. We conduct experiments on a variety of
datasets, comprising texts from social networking sites, user reviews, legal
documents, literary books, and newswire. The results on the aforementioned
tasks and datasets illustrate that such a multi-level understanding of style,
which has been largely ignored in recent works, models style-related
subjectivity in text and can be leveraged to improve performance on multiple
downstream tasks both qualitatively and quantitatively.
",1.0
565,18,147952,2102.11345,infomation retrieval time complexity,"Alberto Purpura, Karolina Buchner, Gianmaria Silvello, Gian Antonio
  Susto",Neural Feature Selection for Learning to Rank,"  LEarning TO Rank (LETOR) is a research area in the field of Information
Retrieval (IR) where machine learning models are employed to rank a set of
items. In the past few years, neural LETOR approaches have become a competitive
alternative to traditional ones like LambdaMART. However, neural architectures
performance grew proportionally to their complexity and size. This can be an
obstacle for their adoption in large-scale search systems where a model size
impacts latency and update time. For this reason, we propose an
architecture-agnostic approach based on a neural LETOR model to reduce the size
of its input by up to 60% without affecting the system performance. This
approach also allows to reduce a LETOR model complexity and, therefore, its
training and inference time up to 50%.
",3.0
566,10,22124,1309.4009,web archive,"Yasmin AlNoamany, Michele C. Weigle, Michael L. Nelson",Access Patterns for Robots and Humans in Web Archives,"  Although user access patterns on the live web are well-understood, there has
been no corresponding study of how users, both humans and robots, access web
archives. Based on samples from the Internet Archive's public Wayback Machine,
we propose a set of basic usage patterns: Dip (a single access), Slide (the
same page at different archive times), Dive (different pages at approximately
the same archive time), and Skim (lists of what pages are archived, i.e.,
TimeMaps). Robots are limited almost exclusively to Dips and Skims, but human
accesses are more varied between all four types. Robots outnumber humans 10:1
in terms of sessions, 5:4 in terms of raw HTTP accesses, and 4:1 in terms of
megabytes transferred. Robots almost always access TimeMaps (95% of accesses),
but humans predominately access the archived web pages themselves (82% of
accesses). In terms of unique archived web pages, there is no overall
preference for a particular time, but the recent past (within the last year)
shows significant repeat accesses.
",5.0
567,18,58233,1710.00454,infomation retrieval time complexity,"Amanpreet Singh, Karthik Venkatesan and Simranjyot Singh Gill",Building a Structured Query Engine,"  Finding patterns in data and being able to retrieve information from those
patterns is an important task in Information retrieval. Complex search
requirements which are not fulfilled by simple string matching and require
exploring certain patterns in data demand a better query engine that can
support searching via structured queries. In this article, we built a
structured query engine which supports searching data through structured
queries on the lines of ElasticSearch. We will show how we achieved real time
indexing and retrieving of data through a RESTful API and how complex queries
can be created and processed using efficient data structures we created for
storing the data in structured way. Finally, we will conclude with an example
of movie recommendation system built on top of this query engine.
",1.0
568,5,116861,2004.1243,matrix completion,Manolis C. Tsakiris,Low-rank matrix completion theory via Plucker coordinates,"  Despite the popularity of low-rank matrix completion, the majority of its
theory has been developed under the assumption of random observation patterns,
whereas very little is known about the practically relevant case of non-random
patterns. Specifically, a fundamental yet largely open question is to describe
patterns that allow for unique or finitely many completions. This paper
provides two such families of patterns for any rank. A key to achieving this is
a novel formulation of low-rank matrix completion in terms of Plucker
coordinates, the latter a traditional tool in computer vision. This connection
is of potential significance to a wide family of matrix and subspace learning
problems with incomplete data.
",4.0
569,11,217395,cs/0503011,PageRank for web search,"Sandeep Pandey, Sourashis Roy, Christopher Olston, Junghoo Cho, and
  Soumen Chakrabarti","Shuffling a Stacked Deck: The Case for Partially Randomized Ranking of
  Search Engine Results","  In-degree, PageRank, number of visits and other measures of Web page
popularity significantly influence the ranking of search results by modern
search engines. The assumption is that popularity is closely correlated with
quality, a more elusive concept that is difficult to measure directly.
Unfortunately, the correlation between popularity and quality is very weak for
newly-created pages that have yet to receive many visits and/or in-links.
Worse, since discovery of new content is largely done by querying search
engines, and because users usually focus their attention on the top few
results, newly-created but high-quality pages are effectively ``shut out,'' and
it can take a very long time before they become popular.
  We propose a simple and elegant solution to this problem: the introduction of
a controlled amount of randomness into search result ranking methods. Doing so
offers new pages a chance to prove their worth, although clearly using too much
randomness will degrade result quality and annul any benefits achieved. Hence
there is a tradeoff between exploration to estimate the quality of new pages
and exploitation of pages already known to be of high quality. We study this
tradeoff both analytically and via simulation, in the context of an economic
objective function based on aggregate result quality amortized over time. We
show that a modest amount of randomness leads to improved search results.
",2.0
570,14,85303,1904.00788,text summarization model,"Soheil Esmaeilzadeh, Gao Xian Peh, Angela Xu",Neural Abstractive Text Summarization and Fake News Detection,"  In this work, we study abstractive text summarization by exploring different
models such as LSTM-encoder-decoder with attention, pointer-generator networks,
coverage mechanisms, and transformers. Upon extensive and careful
hyperparameter tuning we compare the proposed architectures against each other
for the abstractive text summarization task. Finally, as an extension of our
work, we apply our text summarization model as a feature extractor for a fake
news detection task where the news articles prior to classification will be
summarized and the results are compared against the classification using only
the original news text.
  keywords: LSTM, encoder-deconder, abstractive text summarization,
pointer-generator, coverage mechanism, transformers, fake news detection
",5.0
571,12,114461,2003.13894,COVID-19 and social media,Ramya Tekumalla and Juan M. Banda,Social Media Mining Toolkit (SMMT),"  There has been a dramatic increase in the popularity of utilizing social
media data for research purposes within the biomedical community. In PubMed
alone, there have been nearly 2,500 publication entries since 2014 that deal
with analyzing social media data from Twitter and Reddit. However, the vast
majority of those works do not share their code or data for replicating their
studies. With minimal exceptions, the few that do, place the burden on the
researcher to figure out how to fetch the data, how to best format their data,
and how to create automatic and manual annotations on the acquired data. In
order to address this pressing issue, we introduce the Social Media Mining
Toolkit (SMMT), a suite of tools aimed to encapsulate the cumbersome details of
acquiring, preprocessing, annotating and standardizing social media data. The
purpose of our toolkit is for researchers to focus on answering research
questions, and not the technical aspects of using social media data. By using a
standard toolkit, researchers will be able to acquire, use, and release data in
a consistent way that is transparent for everybody using the toolkit, hence,
simplifying research reproducibility and accessibility in the social media
domain.
",1.0
572,16,117026,2004.13271,activation function in neutral networks,Zhaohe Liao,Trainable Activation Function in Image Classification,"  In the current research of neural networks, the activation function is
manually specified by human and not able to change themselves during training.
This paper focus on how to make the activation function trainable for deep
neural networks. We use series and linear combination of different activation
functions make activation functions continuously variable. Also, we test the
performance of CNNs with Fourier series simulated activation(Fourier-CNN) and
CNNs with linear combined activation function (LC-CNN) on Cifar-10 dataset. The
result shows our trainable activation function reveals better performance than
the most used ReLU activation function. Finally, we improves the performance of
Fourier-CNN with Autoencoder, and test the performance of PSO algorithm in
optimizing the parameters of networks
",4.0
573,4,186601,2202.09662,pre-trained language model,"Farshid Faal, Ketra Schmitt and Jia Yuan Yu","Reward Modeling for Mitigating Toxicity in Transformer-based Language
  Models","  Transformer-based language models are able to generate fluent text and be
efficiently adapted across various natural language generation tasks. However,
language models that are pretrained on large unlabeled web text corpora have
been shown to suffer from degenerating toxic content and social bias behaviors,
consequently hindering their safe deployment. Various detoxification methods
were proposed to mitigate the language model's toxicity; however, these methods
struggled to detoxify language models when conditioned on prompts that contain
specific social identities related to gender, race, or religion. In this study,
we propose Reinforce-Detoxify; A reinforcement learning-based method for
mitigating toxicity in language models. We address the challenge of safety in
language models and propose a new reward model that is able to detect toxic
content and mitigate unintended bias towards social identities in toxicity
prediction. The experiments demonstrate that the Reinforce-Detoxify method for
language model detoxification outperforms existing detoxification approaches in
automatic evaluation metrics, indicating the ability of our approach in
language model detoxification and less prone to unintended bias toward social
identities in generated content.
",5.0
574,14,42318,1605.02948,text summarization model,"Milad Moradi, Nasser Ghadiri","Different approaches for identifying important concepts in probabilistic
  biomedical text summarization","  Automatic text summarization tools help users in biomedical domain to acquire
their intended information from various textual resources more efficiently.
Some of the biomedical text summarization systems put the basis of their
sentence selection approach on the frequency of concepts extracted from the
input text. However, it seems that exploring other measures rather than the
frequency for identifying the valuable content of the input document, and
considering the correlations existing between concepts may be more useful for
this type of summarization. In this paper, we describe a Bayesian summarizer
for biomedical text documents. The Bayesian summarizer initially maps the input
text to the Unified Medical Language System (UMLS) concepts, then it selects
the important ones to be used as classification features. We introduce
different feature selection approaches to identify the most important concepts
of the text and to select the most informative content according to the
distribution of these concepts. We show that with the use of an appropriate
feature selection approach, the Bayesian biomedical summarizer can improve the
performance of summarization. We perform extensive evaluations on a corpus of
scientific papers in biomedical domain. The results show that the Bayesian
summarizer outperforms the biomedical summarizers that rely on the frequency of
concepts, the domain-independent and baseline methods based on the
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover,
the results suggest that using the meaningfulness measure and considering the
correlations of concepts in the feature selection step lead to a significant
increase in the performance of summarization.
",5.0
575,12,56344,1708.01967,COVID-19 and social media,"Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu",Fake News Detection on Social Media: A Data Mining Perspective,"  Social media for news consumption is a double-edged sword. On the one hand,
its low cost, easy access, and rapid dissemination of information lead people
to seek out and consume news from social media. On the other hand, it enables
the wide spread of ""fake news"", i.e., low quality news with intentionally false
information. The extensive spread of fake news has the potential for extremely
negative impacts on individuals and society. Therefore, fake news detection on
social media has recently become an emerging research that is attracting
tremendous attention. Fake news detection on social media presents unique
characteristics and challenges that make existing detection algorithms from
traditional news media ineffective or not applicable. First, fake news is
intentionally written to mislead readers to believe false information, which
makes it difficult and nontrivial to detect based on news content; therefore,
we need to include auxiliary information, such as user social engagements on
social media, to help make a determination. Second, exploiting this auxiliary
information is challenging in and of itself as users' social engagements with
fake news produce data that is big, incomplete, unstructured, and noisy.
Because the issue of fake news detection on social media is both challenging
and relevant, we conducted this survey to further facilitate research on the
problem. In this survey, we present a comprehensive review of detecting fake
news on social media, including fake news characterizations on psychology and
social theories, existing algorithms from a data mining perspective, evaluation
metrics and representative datasets. We also discuss related research areas,
open problems, and future research directions for fake news detection on social
media.
",2.0
576,5,130122,2009.01279,matrix completion,"C. Strohmeier, D. Needell",Clustering of Nonnegative Data and an Application to Matrix Completion,"  In this paper, we propose a simple algorithm to cluster nonnegative data
lying in disjoint subspaces. We analyze its performance in relation to a
certain measure of correlation between said subspaces. We use our clustering
algorithm to develop a matrix completion algorithm which can outperform
standard matrix completion algorithms on data matrices satisfying certain
natural conditions.
",5.0
577,8,155568,2105.03178,node embedding for graph,"Gongxu Luo, Jianxin Li, Jianlin Su, Hao Peng, Carl Yang, Lichao Sun,
  Philip S. Yu, Lifang He","Graph Entropy Guided Node Embedding Dimension Selection for Graph Neural
  Networks","  Graph representation learning has achieved great success in many areas,
including e-commerce, chemistry, biology, etc. However, the fundamental problem
of choosing the appropriate dimension of node embedding for a given graph still
remains unsolved. The commonly used strategies for Node Embedding Dimension
Selection (NEDS) based on grid search or empirical knowledge suffer from heavy
computation and poor model performance. In this paper, we revisit NEDS from the
perspective of minimum entropy principle. Subsequently, we propose a novel
Minimum Graph Entropy (MinGE) algorithm for NEDS with graph data. To be
specific, MinGE considers both feature entropy and structure entropy on graphs,
which are carefully designed according to the characteristics of the rich
information in them. The feature entropy, which assumes the embeddings of
adjacent nodes to be more similar, connects node features and link topology on
graphs. The structure entropy takes the normalized degree as basic unit to
further measure the higher-order structure of graphs. Based on them, we design
MinGE to directly calculate the ideal node embedding dimension for any graph.
Finally, comprehensive experiments with popular Graph Neural Networks (GNNs) on
benchmark datasets demonstrate the effectiveness and generalizability of our
proposed MinGE.
",3.0
578,7,54749,1706.06341,gradient boosting,"Kaidong Wang, Yao Wang, Qian Zhao, Deyu Meng and Zongben Xu","SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced
  Learning","  It is known that Boosting can be interpreted as a gradient descent technique
to minimize an underlying loss function. Specifically, the underlying loss
being minimized by the traditional AdaBoost is the exponential loss, which is
proved to be very sensitive to random noise/outliers. Therefore, several
Boosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to
improve the robustness of AdaBoost by replacing the exponential loss with some
designed robust loss functions. In this work, we present a new way to robustify
AdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning
(SPL) into Boosting framework. Specifically, we design a new robust Boosting
algorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented
by slightly modifying off-the-shelf Boosting packages. Extensive experiments
and a theoretical characterization are also carried out to illustrate the
merits of the proposed SPLBoost.
",3.0
579,19,147048,2102.07654,artificial intelligence for low carbon,"Yassine Himeur, Abdullah Alsalemi, Ayman Al-Kababji, Faycal Bensaali,
  Abbes Amira, Christos Sardianos, George Dimitrakopoulos, Iraklis Varlamis","A survey of recommender systems for energy efficiency in buildings:
  Principles, challenges and prospects","  Recommender systems have significantly developed in recent years in parallel
with the witnessed advancements in both internet of things (IoT) and artificial
intelligence (AI) technologies. Accordingly, as a consequence of IoT and AI,
multiple forms of data are incorporated in these systems, e.g. social,
implicit, local and personal information, which can help in improving
recommender systems' performance and widen their applicability to traverse
different disciplines. On the other side, energy efficiency in the building
sector is becoming a hot research topic, in which recommender systems play a
major role by promoting energy saving behavior and reducing carbon emissions.
However, the deployment of the recommendation frameworks in buildings still
needs more investigations to identify the current challenges and issues, where
their solutions are the keys to enable the pervasiveness of research findings,
and therefore, ensure a large-scale adoption of this technology. Accordingly,
this paper presents, to the best of the authors' knowledge, the first timely
and comprehensive reference for energy-efficiency recommendation systems
through (i) surveying existing recommender systems for energy saving in
buildings; (ii) discussing their evolution; (iii) providing an original
taxonomy of these systems based on specified criteria, including the nature of
the recommender engine, its objective, computing platforms, evaluation metrics
and incentive measures; and (iv) conducting an in-depth, critical analysis to
identify their limitations and unsolved issues. The derived challenges and
areas of future implementation could effectively guide the energy research
community to improve the energy-efficiency in buildings and reduce the cost of
developed recommender systems-based solutions.
",2.0
580,9,218431,cs/0610020,language model for long documents,William F. Gilreath,XString: XML as a String,"  Extensible markup language (XML) is a technology that has been much hyped, so
that XML has become an industry buzzword. Behind the hype is a powerful
technology for data representation in a platform independent manner. As a text
document, however, XML suffers from being too bloated, and requires an XML
parser to access and manipulate it. XString is an encoding method for XML, in
essence, a markup language's markup language. XString gives the benefit of
compressing XML, and allows for easy manipulation and processing of XML source
as a very long string.
",4.0
581,3,34862,1507.00646,database management system,Oliver Schulte and Zhensong Qian,SQL for SRL: Structure Learning Inside a Database System,"  The position we advocate in this paper is that relational algebra can provide
a unified language for both representing and computing with
statistical-relational objects, much as linear algebra does for traditional
single-table machine learning. Relational algebra is implemented in the
Structured Query Language (SQL), which is the basis of relational database
management systems. To support our position, we have developed the FACTORBASE
system, which uses SQL as a high-level scripting language for
statistical-relational learning of a graphical model structure. The design
philosophy of FACTORBASE is to manage statistical models as first-class
citizens inside a database. Our implementation shows how our SQL constructs in
FACTORBASE facilitate fast, modular, and reliable program development.
Empirical evidence from six benchmark databases indicates that leveraging
database system capabilities achieves scalable model structure learning.
",4.0
582,15,33311,1504.06868,relevance feedback for imformation retrieval,Gordon V. Cormack and Maura R. Grossman,"Autonomy and Reliability of Continuous Active Learning for
  Technology-Assisted Review","  We enhance the autonomy of the continuous active learning method shown by
Cormack and Grossman (SIGIR 2014) to be effective for technology-assisted
review, in which documents from a collection are retrieved and reviewed, using
relevance feedback, until substantially all of the relevant documents have been
reviewed. Autonomy is enhanced through the elimination of topic-specific and
dataset-specific tuning parameters, so that the sole input required by the user
is, at the outset, a short query, topic description, or single relevant
document; and, throughout the review, ongoing relevance assessments of the
retrieved documents. We show that our enhancements consistently yield superior
results to Cormack and Grossman's version of continuous active learning, and
other methods, not only on average, but on the vast majority of topics from
four separate sets of tasks: the legal datasets examined by Cormack and
Grossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and
the construction of the TREC 2002 filtering test collection.
",2.0
583,12,36243,1509.02238,COVID-19 and social media,"Fangfang Li, Yanchang Zhao, Klaus Felsche, Guandong Xu, and Longbing
  Cao",Coupling Analysis Between Twitter and Call Centre,"  Social media has been contributing many research areas such as data mining,
recommender systems, time series analysis, etc. However, there are not many
successful applications regarding social media in government agencies. In fact,
lots of governments have social media accounts such as twitter and facebook.
More and more customers are likely to communicate with governments on social
media, causing massive external social media data for governments. This
external data would be beneficial for analysing behaviours and real needs of
the customers. Besides this, most governments also have a call centre to help
customers solve their problems. It is not difficult to imagine that the
enquiries on external social media and internal call centre may have some
coupling relationships. The couplings could be helpful for studying customers'
intent and allocating government's limited resources for better service. In
this paper, we mainly focus on analysing the coupling relations between
internal call centre and external public media using time series analysis
methods for Australia Department of Immigration and Border Protec-tion. The
discovered couplings demonstrate that call centre and public media indeed have
correlations, which are significant for understanding customers' behaviours.
",2.0
584,8,204198,2207.10149,node embedding for graph,"Ciwan Ceylan, Kambiz Ghoorchian and Danica Kragic","Digraphwave: Scalable Extraction of Structural Node Embeddings via
  Diffusion on Directed Graphs","  Structural node embeddings, vectors capturing local connectivity information
for each node in a graph, have many applications in data mining and machine
learning, e.g., network alignment and node classification, clustering and
anomaly detection. For the analysis of directed graphs, e.g., transactions
graphs, communication networks and social networks, the capability to capture
directional information in the structural node embeddings is highly desirable,
as is scalability of the embedding extraction method. Most existing methods are
nevertheless only designed for undirected graph. Therefore, we present
Digraphwave -- a scalable algorithm for extracting structural node embeddings
on directed graphs. The Digraphwave embeddings consist of compressed diffusion
pattern signatures, which are twice enhanced to increase their discriminate
capacity. By proving a lower bound on the heat contained in the local vicinity
of a diffusion initialization node, theoretically justified diffusion timescale
values are established, and Digraphwave is left with only two easy-to-interpret
hyperparameters: the embedding dimension and a neighbourhood resolution
specifier. In our experiments, the two embedding enhancements, named
transposition and aggregation, are shown to lead to a significant increase in
macro F1 score for classifying automorphic identities, with Digraphwave
outperforming all other structural embedding baselines. Moreover, Digraphwave
either outperforms or matches the performance of all baselines on real graph
datasets, displaying a particularly large performance gain in a network
alignment task, while also being scalable to graphs with millions of nodes and
edges, running up to 30x faster than a previous diffusion pattern based method
and with a fraction of the memory consumption.
",5.0
585,18,84166,1903.04649,infomation retrieval time complexity,Richa Tripathi and Amit Reza,"A subset selection based approach to finding important structure of
  complex networks","  Most of the real world networks such as the internet network, collaboration
networks, brain networks, citation networks, powerline and airline networks are
very large and to study their structure, and dynamics one often requires
working with large connectivity (adjacency) matrices. However, it is almost
always true that a few or sometimes most of the nodes and their connections are
not very crucial for network functioning or that the network is robust to a
failure of certain nodes and their connections to the rest of the network. In
the present work, we aim to extract the size reduced representation of complex
networks such that new representation has the most relevant network nodes and
connections and retains its spectral properties. To achieve this, we use the
Subset Selection (SS) procedure. The SS method, in general, is used to retrieve
maximum information from a matrix in terms of its most informative columns. The
retrieved matrix, typically known as subset has columns of an original matrix
that have the least linear dependency. We present the application of SS
procedure to many adjacency matrices of real-world networks and model network
types to extract their subset. The subset owing to its small size can play a
crucial role in analyzing spectral properties of large complex networks where
space and time complexity of analyzing full adjacency matrices are too
expensive. The adjacency matrix constructed from the obtained subset has a
smaller size and represents the most important network structure. We observed
that the subset network which is almost half the size of the original network
has better information flow efficiency than the original network.
",1.0
586,2,192353,2204.05389,random forests,"Maciej Piernik, Dariusz Brzezinski, Pawel Zawadzki",Random Similarity Forests,"  The wealth of data being gathered about humans and their surroundings drives
new machine learning applications in various fields. Consequently, more and
more often, classifiers are trained using not only numerical data but also
complex data objects. For example, multi-omics analyses attempt to combine
numerical descriptions with distributions, time series data, discrete
sequences, and graphs. Such integration of data from different domains requires
either omitting some of the data, creating separate models for different
formats, or simplifying some of the data to adhere to a shared scale and
format, all of which can hinder predictive performance. In this paper, we
propose a classification method capable of handling datasets with features of
arbitrary data types while retaining each feature's characteristic. The
proposed algorithm, called Random Similarity Forest, uses multiple
domain-specific distance measures to combine the predictive performance of
Random Forests with the flexibility of Similarity Forests. We show that Random
Similarity Forests are on par with Random Forests on numerical data and
outperform them on datasets from complex or mixed data domains. Our results
highlight the applicability of Random Similarity Forests to noisy, multi-source
datasets that are becoming ubiquitous in high-impact life science projects.
",5.0
587,15,31797,1502.05168,relevance feedback for imformation retrieval,"Rekha Vaidyanathan, Sujoy Das, Namita Srivastava","Query Expansion Strategy based on Pseudo Relevance Feedback and Term
  Weight Scheme for Monolingual Retrieval","  Query Expansion using Pseudo Relevance Feedback is a useful and a popular
technique for reformulating the query. In our proposed query expansion method,
we assume that relevant information can be found within a document near the
central idea. The document is normally divided into sections, paragraphs and
lines. The proposed method tries to extract keywords that are closer to the
central theme of the document. The expansion terms are obtained by
equi-frequency partition of the documents obtained from pseudo relevance
feedback and by using tf-idf scores. The idf factor is calculated for number of
partitions in documents. The group of words for query expansion is selected
using the following approaches: the highest score, average score and a group of
words that has maximum number of keywords. As each query behaved differently
for different methods, the effect of these methods in selecting the words for
query expansion is investigated. From this initial study, we extend the
experiment to develop a rule-based statistical model that automatically selects
the best group of words incorporating the tf-idf scoring and the 3 approaches
explained here, in the future. The experiments were performed on FIRE 2011
Adhoc Hindi and English test collections on 50 queries each, using Terrier as
retrieval engine.
",4.0
588,6,12325,1203.5084,query expansion for imformation retrieval,"Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood",A Data Driven Approach to Query Expansion in Question Answering,"  Automated answering of natural language questions is an interesting and
useful problem to solve. Question answering (QA) systems often perform
information retrieval at an initial stage. Information retrieval (IR)
performance, provided by engines such as Lucene, places a bound on overall
system performance. For example, no answer bearing documents are retrieved at
low ranks for almost 40% of questions.
  In this paper, answer texts from previous QA evaluations held as part of the
Text REtrieval Conferences (TREC) are paired with queries and analysed in an
attempt to identify performance-enhancing words. These words are then used to
evaluate the performance of a query expansion method.
  Data driven extension words were found to help in over 70% of difficult
questions. These words can be used to improve and evaluate query expansion
methods. Simple blind relevance feedback (RF) was correctly predicted as
unlikely to help overall performance, and an possible explanation is provided
for its low value in IR for QA.
",5.0
589,8,192640,2204.06963,node embedding for graph,"Yun Shen and Yufei Han and Zhikun Zhang and Min Chen and Ting Yu and
  Michael Backes and Yang Zhang and Gianluca Stringhini",Finding MNEMON: Reviving Memories of Node Embeddings,"  Previous security research efforts orbiting around graphs have been
exclusively focusing on either (de-)anonymizing the graphs or understanding the
security and privacy issues of graph neural networks. Little attention has been
paid to understand the privacy risks of integrating the output from graph
embedding models (e.g., node embeddings) with complex downstream machine
learning pipelines. In this paper, we fill this gap and propose a novel
model-agnostic graph recovery attack that exploits the implicit graph
structural information preserved in the embeddings of graph nodes. We show that
an adversary can recover edges with decent accuracy by only gaining access to
the node embedding matrix of the original graph without interactions with the
node embedding models. We demonstrate the effectiveness and applicability of
our graph recovery attack through extensive experiments.
",3.0
590,11,217935,cs/0601045,PageRank for web search,Oren Kurland and Lillian Lee,"PageRank without hyperlinks: Structural re-ranking using links induced
  by language models","  Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web
search, we propose a structural re-ranking approach to ad hoc information
retrieval: we reorder the documents in an initially retrieved set by exploiting
asymmetric relationships between them. Specifically, we consider generation
links, which indicate that the language model induced from one document assigns
high probability to the text of another; in doing so, we take care to prevent
bias against long documents. We study a number of re-ranking criteria based on
measures of centrality in the graphs formed by generation links, and show that
integrating centrality into standard language-model-based retrieval is quite
effective at improving precision at top ranks.
",4.0
591,5,203318,2207.05802,matrix completion,"Yuepeng Yang, Cong Ma",Optimal tuning-free convex relaxation for noisy matrix completion,"  This paper is concerned with noisy matrix completion--the problem of
recovering a low-rank matrix from partial and noisy entries. Under uniform
sampling and incoherence assumptions, we prove that a tuning-free square-root
matrix completion estimator (square-root MC) achieves optimal statistical
performance for solving the noisy matrix completion problem. Similar to the
square-root Lasso estimator in high-dimensional linear regression, square-root
MC does not rely on the knowledge of the size of the noise. While solving
square-root MC is a convex program, our statistical analysis of square-root MC
hinges on its intimate connections to a nonconvex rank-constrained estimator.
",4.0
592,2,38157,1511.08327,random forests,"Robin Genuer (ISPED, SISTM), Jean-Michel Poggi (UPD5, LM-Orsay),
  Christine Tuleau-Malot (JAD), Nathalie Villa-Vialaneix (MIAT INRA)",Random Forests for Big Data,"  Big Data is one of the major challenges of statistical science and has
numerous consequences from algorithmic and theoretical viewpoints. Big Data
always involve massive data but they also often include online data and data
heterogeneity. Recently some statistical methods have been adapted to process
Big Data, like linear regression models, clustering methods and bootstrapping
schemes. Based on decision trees combined with aggregation and bootstrap ideas,
random forests were introduced by Breiman in 2001. They are a powerful
nonparametric statistical method allowing to consider in a single and versatile
framework regression problems, as well as two-class and multi-class
classification problems. Focusing on classification problems, this paper
proposes a selective review of available proposals that deal with scaling
random forests to Big Data problems. These proposals rely on parallel
environments or on online adaptations of random forests. We also describe how
related quantities -- such as out-of-bag error and variable importance -- are
addressed in these methods. Then, we formulate various remarks for random
forests in the Big Data context. Finally, we experiment five variants on two
massive datasets (15 and 120 millions of observations), a simulated one as well
as real world data. One variant relies on subsampling while three others are
related to parallel implementations of random forests and involve either
various adaptations of bootstrap to Big Data or to ""divide-and-conquer""
approaches. The fifth variant relates on online learning of random forests.
These numerical experiments lead to highlight the relative performance of the
different variants, as well as some of their limitations.
",5.0
593,13,45159,1608.05982,social network analysis with natrual language processing,"Andrzej Jarynowski, Stephanie Boland","Social Networks Analysis in Discovering the Narrative Structure of
  Literary Fiction","  In our paper we would like to make a cross-disciplinary leap and use the
tools of network theory to understand and explore narrative structure in
literary fiction, an approach that is still underestimated. However, the
systems in fiction are sensitive to readers subjectivity and attention must to
be paid to different methods of extracting networks. The project aims at
investigating into different ways social interactions are read in texts by
comparing networks produced by automated algorithms-natural language processing
with those created by surveying more subjective human responses. Conversation
networks from fiction have been already extracted by scientists, but the more
general framework surrounding these interactions was missing. We propose
several NLP methods for detecting interactions and test them against a range of
human perceptions. In doing so, we have pointed to some limitations of using
network analysis to test literary theory, e.g. interaction, which corresponds
to the plot, does not form climax.
",4.0
594,5,107153,2001.01152,matrix completion,"Xu Zhang, Wei Cui, Yulong Liu","Matrix Completion with Prior Subspace Information via Maximizing
  Correlation","  This paper studies the problem of completing a low-rank matrix from a few of
its random entries with the aid of prior information. We suggest a strategy to
incorporate prior information into the standard matrix completion procedure by
maximizing the correlation between the original signal and the prior
information. We also establish performance guarantees for the proposed method,
which show that with suitable prior information, the proposed procedure can
reduce the sample complexity of the standard matrix completion by a logarithmic
factor. To illustrate the theory, we further analyze an important practical
application where the prior subspace information is available. Both synthetic
and real-world experiments are provided to verify the validity of the theory.
",5.0
595,19,216167,2211.01759,artificial intelligence for low carbon,"Gregor Cerar, Bla\v{z} Bertalani\v{c}, Carolina Fortuna",Resource-aware Deep Learning for Wireless Fingerprinting Localization,"  Location based services, already popular with end users, are now inevitably
becoming part of new wireless infrastructures and emerging business processes.
The increasingly popular Deep Learning (DL) artificial intelligence methods
perform very well in wireless fingerprinting localization based on extensive
indoor radio measurement data. However, with the increasing complexity these
methods become computationally very intensive and energy hungry, both for their
training and subsequent operation. Considering only mobile users, estimated to
exceed 7.4 billion by the end of 2025, and assuming that the networks serving
these users will need to perform only one localization per user per hour on
average, the machine learning models used for the calculation would need to
perform $65 \times 10^{12}$ predictions per year. Add to this equation tens of
billions of other connected devices and applications that rely heavily on more
frequent location updates, and it becomes apparent that localization will
contribute significantly to carbon emissions unless more energy-efficient
models are developed and used. In this Chapter, we discuss the latest results
and trends in wireless localization and look at paths towards achieving more
sustainable AI. We then elaborate on a methodology for computing DL model
complexity, energy consumption and carbon footprint and show on a concrete
example how to develop a more resource-aware model for fingerprinting. We
finally compare relevant works in terms of complexity and training CO$_2$
footprint.
",2.0
596,3,55984,1707.08259,database management system,"Amir Mohammad Saba, Elham Shahab, Hadi Abdolrahimpour, Mahsa Hakimi,
  Akbar Moazzam","A Comparative Analysis of XML Documents, XML Enabled Databases and
  Native XML Databases","  With the increasing popularity of XML data and a great need for a database
management system able to store, retrieve and manipulate XML-based data in an
efficient manner, database research communities and software industries have
tried to respond to this requirement. XML-enabled database and native XML
database are two approaches that have been proposed to address this challenge.
These two approaches are a legacy database systems which are extended to store,
retrieve and manipulate XML-based data. The major objective of this paper is to
explore and compare between the two approaches and reach to some criteria to
have a suitable guideline to select the best approach in each circumstance. In
general, native XML database systems have more ability in comparison with
XML-enabled database system for managing XML-based data
",4.0
597,4,120936,2006.04152,pre-trained language model,"Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu
  and Furu Wei",BERT Loses Patience: Fast and Robust Inference with Early Exit,"  In this paper, we propose Patience-based Early Exit, a straightforward yet
effective inference method that can be used as a plug-and-play technique to
simultaneously improve the efficiency and robustness of a pretrained language
model (PLM). To achieve this, our approach couples an internal-classifier with
each layer of a PLM and dynamically stops inference when the intermediate
predictions of the internal classifiers remain unchanged for a pre-defined
number of steps. Our approach improves inference efficiency as it allows the
model to make a prediction with fewer layers. Meanwhile, experimental results
with an ALBERT model show that our method can improve the accuracy and
robustness of the model by preventing it from overthinking and exploiting
multiple classifiers for prediction, yielding a better accuracy-speed trade-off
compared to existing early exit methods.
",3.0
598,3,83609,1903.00731,database management system,"Dimitrios Liarokapis, Elizabeth ONeil, Patrick ONeil","HISTEX (HISTory EXerciser) : A tool for testing the implementation of
  Isolation Levels of Relational Database Management Systems","  We present a multi-process application called HISTEX (HISTory EXerciser),
which executes input histories in a generic transactional notation on
commercial DBMS platforms. HISTEX could be used to discover potential errors in
the implementation of Isolation Levels by Relational Database Management
Systems or cases where a system behaves over restrictively. It can also be used
for performance measurements related to database workloads executing on real
database systems instead of simulated environments. HISTEX has been implemented
in C by utilizing Embedded SQL. However, many of its ideas could be
reincarnated in new implementations that could rely on other database
connectivity paradigms such as JDBC, JPA etc. We expect that by presenting some
of the ideas behind its development we could re-invigorate some fresh interest
and involvement in the research community regarding such tools.
",3.0
599,12,97218,1909.03362,COVID-19 and social media,"Yudi Chen, Qi Wang, Wenying Ji","Assessing Disaster Impacts on Highways Using Social Media: Case Study of
  Hurricane Harvey","  During and after disasters, highways provide vital routes for emergency
services, relief efforts, and evacuation activities. Thus, a timely and
reliable assessment of disaster impacts on highways is critical for
decision-makers to quickly and effectively perform relief and recovery efforts.
Recently, social media has increasingly been used in disaster management for
obtaining a rapid, public-centric assessment of disaster impacts due to its
near real-time, social and informational characteristics. Although promising,
the employment of social media for assessing disaster impacts on highways is
still limited due to the inability of extracting accurate highway-related data
from social media. To overcome this limitation, a systematic approach is
proposed to identify highway-related data from social media for assessing
disaster impacts on highways, and a case study of Hurricane Harvey in Houston,
TX is employed for the demonstration. The approach is constructed through three
steps: (1) building data sources for social media and highways of interest in
Houston, respectively; (2) adapting the social media data to each highway
through a developed mapping algorithm; (3) assessing disaster impacts through
analyzing social media activities in terms of their intensity, geographic, and
topic distributions. Results show that the proposed approach is capable of
capturing the temporal patterns of disaster impacts on highways. Official news
and reports are employed to validate the assessed impacts.
",2.0
600,13,137477,2011.04446,social network analysis with natrual language processing,"Tanvirul Alam, Akib Khan and Firoj Alam",Bangla Text Classification using Transformers,"  Text classification has been one of the earliest problems in NLP. Over time
the scope of application areas has broadened and the difficulty of dealing with
new areas (e.g., noisy social media content) has increased. The problem-solving
strategy switched from classical machine learning to deep learning algorithms.
One of the recent deep neural network architecture is the Transformer. Models
designed with this type of network and its variants recently showed their
success in many downstream natural language processing tasks, especially for
resource-rich languages, e.g., English. However, these models have not been
explored fully for Bangla text classification tasks. In this work, we fine-tune
multilingual transformer models for Bangla text classification tasks in
different domains, including sentiment analysis, emotion detection, news
categorization, and authorship attribution. We obtain the state of the art
results on six benchmark datasets, improving upon the previous results by 5-29%
accuracy across different tasks.
",2.0
601,18,141488,2012.08907,infomation retrieval time complexity,Abdulmalik Johar,Information retrieval system for silte language using BM25 weighting,"  The main aim of an information retrieval system is to extract appropriate
information from an enormous collection of data based on users need. The basic
concept of the information retrieval system is that when a user sends out a
query, the system would try to generate a list of related documents ranked in
order, according to their degree of relevance. Digital unstructured Silte text
documents increase from time to time. The growth of digital text information
makes the utilization and access of the right information difficult. Thus,
developing an information retrieval system for Silte language allows searching
and retrieving relevant documents that satisfy information need of users. In
this research, we design probabilistic information retrieval system for Silte
language. The system has both indexing and searching part was created. In these
modules, different text operations such as tokenization, stemming, stop word
removal and synonym is included.
",1.0
602,1,5370,1005.0961,advanced search engine,"M. Umamaheswari, S. Sivasubramanian","Performance Oriented Query Processing In GEO Based Location Search
  Engines","  Geographic location search engines allow users to constrain and order search
results in an intuitive manner by focusing a query on a particular geographic
region. Geographic search technology, also called location search, has recently
received significant interest from major search engine companies. Academic
research in this area has focused primarily on techniques for extracting
geographic knowledge from the web. In this paper, we study the problem of
efficient query processing in scalable geographic search engines. Query
processing is a major bottleneck in standard web search engines, and the main
reason for the thousands of machines used by the major engines. Geographic
search engine query processing is different in that it requires a combination
of text and spatial data processing techniques. We propose several algorithms
for efficient query processing in geographic search engines, integrate them
into an existing web search query processor, and evaluate them on large sets of
real data and query traces.
",5.0
603,2,35315,1507.06105,random forests,Jianyuan Sun and Guoqiang Zhong and Junyu Dong and Yajuan Cai,Banzhaf Random Forests,"  Random forests are a type of ensemble method which makes predictions by
combining the results of several independent trees. However, the theory of
random forests has long been outpaced by their application. In this paper, we
propose a novel random forests algorithm based on cooperative game theory.
Banzhaf power index is employed to evaluate the power of each feature by
traversing possible feature coalitions. Unlike the previously used information
gain rate of information theory, which simply chooses the most informative
feature, the Banzhaf power index can be considered as a metric of the
importance of each feature on the dependency among a group of features. More
importantly, we have proved the consistency of the proposed algorithm, named
Banzhaf random forests (BRF). This theoretical analysis takes a step towards
narrowing the gap between the theory and practice of random forests for
classification problems. Experiments on several UCI benchmark data sets show
that BRF is competitive with state-of-the-art classifiers and dramatically
outperforms previous consistent random forests. Particularly, it is much more
efficient than previous consistent random forests.
",5.0
604,6,131170,2009.07258,query expansion for imformation retrieval,"Zhi Zheng, Kai Hui, Ben He, Xianpei Han, Le Sun, Andrew Yates",BERT-QE: Contextualized Query Expansion for Document Re-ranking,"  Query expansion aims to mitigate the mismatch between the language used in a
query and in a document. However, query expansion methods can suffer from
introducing non-relevant information when expanding the query. To bridge this
gap, inspired by recent advances in applying contextualized models like BERT to
the document retrieval task, this paper proposes a novel query expansion model
that leverages the strength of the BERT model to select relevant document
chunks for expansion. In evaluation on the standard TREC Robust04 and GOV2 test
collections, the proposed BERT-QE model significantly outperforms BERT-Large
models.
",5.0
605,9,12485,1204.1162,language model for long documents,"Abd El Salam Al Hajjar, Anis Ismail, Mohammad Hajjar, Mazen El-Sayed","Performance of the Google Desktop, Arabic Google Desktop and Peer to
  Peer Application in Arabic Language","  The Arabic language is a complex language; it is different from Western
languages especially at the morphological and spelling variations. Indeed, the
performance of information retrieval systems in the Arabic language is still a
problem. For this reason, we are interested in studying the performance of the
most famous search engine, which is a Google Desktop, while searching in Arabic
language documents. Then, we propose an update to the Google Desktop to take
into consideration in search the Arabic words that have the same root. After
that, we evaluate the performance of the Google Desktop in this context. Also,
we are interested in evaluation the performance of peer-to-peer application in
two ways. The first one uses a simple indexation that indexes Arabic documents
without taking in consideration the root of words. The second way takes in
consideration the roots in the indexation of Arabic documents. This evaluation
is done by using a corpus of ten thousand documents and one hundred different
queries.
",1.0
606,0,52812,1704.08464,learning to rank with partitioned preference,"Zhiwei Lin, Yi Li, and Xiaolian Guo",Consensus measure of rankings,"  A ranking is an ordered sequence of items, in which an item with higher
ranking score is more preferred than the items with lower ranking scores. In
many information systems, rankings are widely used to represent the preferences
over a set of items or candidates. The consensus measure of rankings is the
problem of how to evaluate the degree to which the rankings agree. The
consensus measure can be used to evaluate rankings in many information systems,
as quite often there is not ground truth available for evaluation.
  This paper introduces a novel approach for consensus measure of rankings by
using graph representation, in which the vertices or nodes are the items and
the edges are the relationship of items in the rankings. Such representation
leads to various algorithms for consensus measure in terms of different aspects
of rankings, including the number of common patterns, the number of common
patterns with fixed length and the length of the longest common patterns. The
proposed measure can be adopted for various types of rankings, such as full
rankings, partial rankings and rankings with ties. This paper demonstrates how
the proposed approaches can be used to evaluate the quality of rank aggregation
and the quality of top-$k$ rankings from Google and Bing search engines.
",3.0
607,12,57338,1709.02426,COVID-19 and social media,"Tahora H. Nazer, Guoliang Xue, Yusheng Ji, Huan Liu",Intelligent Disaster Response via Social Media Analysis - A Survey,"  The success of a disaster relief and response process is largely dependent on
timely and accurate information regarding the status of the disaster, the
surrounding environment, and the affected people. This information is primarily
provided by first responders on-site and can be enhanced by the firsthand
reports posted in real-time on social media. Many tools and methods have been
developed to automate disaster relief by extracting, analyzing, and visualizing
actionable information from social media. However, these methods are not well
integrated in the relief and response processes and the relation between the
two requires exposition for further advancement. In this survey, we review the
new frontier of intelligent disaster relief and response using social media,
show stages of disasters which are reflected on social media, establish a
connection between proposed methods based on social media and relief efforts by
first responders, and outline pressing challenges and future research
directions.
",3.0
608,2,65202,1803.08,random forests,"Indrayudh Ghosal, Giles Hooker","Boosting Random Forests to Reduce Bias; One-Step Boosted Forest and its
  Variance Estimate","  In this paper we propose using the principle of boosting to reduce the bias
of a random forest prediction in the regression setting. From the original
random forest fit we extract the residuals and then fit another random forest
to these residuals. We call the sum of these two random forests a
\textit{one-step boosted forest}. We show with simulated and real data that the
one-step boosted forest has a reduced bias compared to the original random
forest. The paper also provides a variance estimate of the one-step boosted
forest by an extension of the infinitesimal Jackknife estimator. Using this
variance estimate we can construct prediction intervals for the boosted forest
and we show that they have good coverage probabilities. Combining the bias
reduction and the variance estimate we show that the one-step boosted forest
has a significant reduction in predictive mean squared error and thus an
improvement in predictive performance. When applied on datasets from the UCI
database, one-step boosted forest performs better than random forest and
gradient boosting machine algorithms. Theoretically we can also extend such a
boosting process to more than one step and the same principles outlined in this
paper can be used to find variance estimates for such predictors. Such boosting
will reduce bias even further but it risks over-fitting and also increases the
computational burden.
",5.0
609,19,156408,2105.07194,artificial intelligence for low carbon,"Cheng Qiu, Yuzi Han, Logesh Shanmugam, Fengyang Jiang, Zhidong Guan,
  Shanyi Du, Jinglei Yang","An even-load-distribution design for composite bolted joints using a
  novel circuit model and artificial neural networks","  Due to the brittle feature of carbon fiber reinforced plastic laminates,
mechanical multi-joint within these composite components show uneven load
distribution for each bolt, which weaken the strength advantage of composite
laminates. In order to reduce this defect and achieve the goal of even load
distribution in mechanical joints, we propose a machine learning-based
framework as an optimization method. Since that the friction effect has been
proven to be a significant factor in determining bolt load distribution, our
framework aims at providing optimal parameters including bolt-hole clearances
and tightening torques for a minimum unevenness of bolt load. A novel circuit
model is established to generate data samples for the training of artificial
networks at a relatively low computational cost. A database for all the
possible inputs in the design space is built through the machine learning
model. The optimal dataset of clearances and torques provided by the database
is validated by both the finite element method, circuit model, and an
experimental measurement based on the linear superposition principle, which
shows the effectiveness of this general framework for the optimization problem.
Then, our machine learning model is further compared and worked in
collaboration with commonly used optimization algorithms, which shows the
potential of greatly increasing computational efficiency for the inverse design
problem.
",1.0
610,12,158623,2106.0225,COVID-19 and social media,"Yihong Zhang, Masumi Shirakawa and Takahiro Hara",A General Method for Event Detection on Social Media,"  Event detection on social media has attracted a number of researches, given
the recent availability of large volumes of social media discussions. Previous
works on social media event detection either assume a specific type of event,
or assume certain behavior of observed variables. In this paper, we propose a
general method for event detection on social media that makes few assumptions.
The main assumption we make is that when an event occurs, affected semantic
aspects will behave differently from its usual behavior. We generalize the
representation of time units based on word embeddings of social media text, and
propose an algorithm to detect events in time series in a general sense. In the
experimental evaluation, we use a novel setting to test if our method and
baseline methods can exhaustively catch all real-world news in the test period.
The evaluation results show that when the event is quite unusual with regard to
the base social media discussion, it can be captured more effectively with our
method. Our method can be easily implemented and can be treated as a starting
point for more specific applications.
",2.0
611,14,119136,2005.09252,text summarization model,"Anubhav Jangra, Sriparna Saha, Adam Jatowt, Mohammad Hasanuzzaman",Multi-Modal Summary Generation using Multi-Objective Optimization,"  Significant development of communication technology over the past few years
has motivated research in multi-modal summarization techniques. A majority of
the previous works on multi-modal summarization focus on text and images. In
this paper, we propose a novel extractive multi-objective optimization based
model to produce a multi-modal summary containing text, images, and videos.
Important objectives such as intra-modality salience, cross-modal redundancy
and cross-modal similarity are optimized simultaneously in a multi-objective
optimization framework to produce effective multi-modal output. The proposed
model has been evaluated separately for different modalities, and has been
found to perform better than state-of-the-art approaches.
",3.0
612,14,47999,1611.09238,text summarization model,"Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei",Improving Multi-Document Summarization via Text Classification,"  Developed so far, multi-document summarization has reached its bottleneck due
to the lack of sufficient training data and diverse categories of documents.
Text classification just makes up for these deficiencies. In this paper, we
propose a novel summarization system called TCSum, which leverages plentiful
text classification data to improve the performance of multi-document
summarization. TCSum projects documents onto distributed representations which
act as a bridge between text classification and summarization. It also utilizes
the classification results to produce summaries of different styles. Extensive
experiments on DUC generic multi-document summarization datasets show that,
TCSum can achieve the state-of-the-art performance without using any
hand-crafted features and has the capability to catch the variations of summary
styles with respect to different text categories.
",3.0
613,10,47373,1611.02493,web archive,"Ed Summers, Ricardo Punzalan","Bots, Seeds and People: Web Archives as Infrastructure","  The field of web archiving provides a unique mix of human and automated
agents collaborating to achieve the preservation of the web. Centuries old
theories of archival appraisal are being transplanted into the sociotechnical
environment of the World Wide Web with varying degrees of success. The work of
the archivist and bots in contact with the material of the web present a
distinctive and understudied CSCW shaped problem. To investigate this space we
conducted semi-structured interviews with archivists and technologists who were
directly involved in the selection of content from the web for archives. These
semi-structured interviews identified thematic areas that inform the appraisal
process in web archives, some of which are encoded in heuristics and
algorithms. Making the infrastructure of web archives legible to the archivist,
the automated agents and the future researcher is presented as a challenge to
the CSCW and archival community.
",3.0
614,19,116400,2004.09959,artificial intelligence for low carbon,"Kerstin H\""otte, Anton Pichler, Fran\c{c}ois Lafond",The rise of science in low-carbon energy technologies,"  Successfully combating climate change will require substantial technological
improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient
allocation of R\&D budgets requires a better understanding of how LCETs rely on
scientific knowledge. Using data covering almost all US patents and scientific
articles that are cited by them over the past two centuries, we describe the
evolution of knowledge bases of ten key LCETs and show how technological
interdependencies have changed over time. The composition of low-carbon energy
innovations shifted over time, from Hydro and Wind energy in the 19th and early
20th century, to Nuclear fission after World War II, and more recently to Solar
PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels
(including energy from waste) have 35-65\% of their citations directed toward
scientific papers, while this ratio is less than 10\% for Wind, Solar thermal,
Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing
science and the share of citations that are to scientific papers has been
increasing for all technology types. The analysis of the scientific knowledge
base of each LCET reveals three fairly separate clusters, with nuclear energy
technologies, Biofuels and Waste, and all the other LCETs. Our detailed
description of knowledge requirements for each LCET helps to design of targeted
innovation policies.
",2.0
615,13,16653,1212.3228,social network analysis with natrual language processing,"Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari,
  Jedidiah Crandall, George Luger","Language Without Words: A Pointillist Model for Natural Language
  Processing","  This paper explores two separate questions: Can we perform natural language
processing tasks without a lexicon?; and, Should we? Existing natural language
processing techniques are either based on words as units or use units such as
grams only for basic classification tasks. How close can a machine come to
reasoning about the meanings of words and phrases in a corpus without using any
lexicon, based only on grams?
  Our own motivation for posing this question is based on our efforts to find
popular trends in words and phrases from online Chinese social media. This form
of written Chinese uses so many neologisms, creative character placements, and
combinations of writing systems that it has been dubbed the ""Martian Language.""
Readers must often use visual queues, audible queues from reading out loud, and
their knowledge and understanding of current events to understand a post. For
analysis of popular trends, the specific problem is that it is difficult to
build a lexicon when the invention of new ways to refer to a word or concept is
easy and common. For natural language processing in general, we argue in this
paper that new uses of language in social media will challenge machines'
abilities to operate with words as the basic unit of understanding, not only in
Chinese but potentially in other languages.
",5.0
616,8,112874,2003.04508,node embedding for graph,"Rui Zhang, Yunxing Zhang, Xuelong Li",Unsupervised Graph Embedding via Adaptive Graph Learning,"  Graph autoencoders (GAEs) are powerful tools in representation learning for
graph embedding. However, the performance of GAEs is very dependent on the
quality of the graph structure, i.e., of the adjacency matrix. In other words,
GAEs would perform poorly when the adjacency matrix is incomplete or be
disturbed. In this paper, two novel unsupervised graph embedding methods,
unsupervised graph embedding via adaptive graph learning (BAGE) and
unsupervised graph embedding via variational adaptive graph learning (VBAGE)
are proposed. The proposed methods expand the application range of GAEs on
graph embedding, i.e, on the general datasets without graph structure.
Meanwhile, the adaptive learning mechanism can initialize the adjacency matrix
without be affected by the parameter. Besides that, the latent representations
are embedded in the laplacian graph structure to preserve the topology
structure of the graph in the vector space. Moreover, the adjacency matrix can
be self-learned for better embedding performance when the original graph
structure is incomplete. With adaptive learning, the proposed method is much
more robust to the graph structure. Experimental studies on several datasets
validate our design and demonstrate that our methods outperform baselines by a
wide margin in node clustering, node classification, and graph visualization
tasks.
",5.0
617,14,3313,906.469,text summarization model,"Ladda Suanmali, Naomie Salim and Mohammed Salem Binwahlan",Fuzzy Logic Based Method for Improving Text Summarization,"  Text summarization can be classified into two approaches: extraction and
abstraction. This paper focuses on extraction approach. The goal of text
summarization based on extraction approach is sentence selection. One of the
methods to obtain the suitable sentences is to assign some numerical measure of
a sentence for the summary called sentence weighting and then select the best
ones. The first step in summarization by extraction is the identification of
important features. In our experiment, we used 125 test documents in DUC2002
data set. Each document is prepared by preprocessing process: sentence
segmentation, tokenization, removing stop word, and word stemming. Then, we use
8 important features and calculate their score for each sentence. We propose
text summarization based on fuzzy logic to improve the quality of the summary
created by the general statistic method. We compare our results with the
baseline summarizer and Microsoft Word 2007 summarizers. The results show that
the best average precision, recall, and f-measure for the summaries were
obtained by fuzzy method.
",5.0
618,4,52517,1704.05393,pre-trained language model,"Michela Fazzolari, Marinella Petrocchi, Alessandro Tommasi, Cesare
  Zavattari","Mining Worse and Better Opinions. Unsupervised and Agnostic Aggregation
  of Online Reviews","  In this paper, we propose a novel approach for aggregating online reviews,
according to the opinions they express. Our methodology is unsupervised - due
to the fact that it does not rely on pre-labeled reviews - and it is agnostic -
since it does not make any assumption about the domain or the language of the
review content. We measure the adherence of a review content to the domain
terminology extracted from a review set. First, we demonstrate the
informativeness of the adherence metric with respect to the score associated
with a review. Then, we exploit the metric values to group reviews, according
to the opinions they express. Our experimental campaign has been carried out on
two large datasets collected from Booking and Amazon, respectively.
",2.0
619,8,157629,2105.13251,node embedding for graph,"T. Mitchell Roddenberry, Yu Zhu, Santiago Segarra",An Impossibility Theorem for Node Embedding,"  With the increasing popularity of graph-based methods for dimensionality
reduction and representation learning, node embedding functions have become
important objects of study in the literature. In this paper, we take an
axiomatic approach to understanding node embedding methods, first stating three
properties for embedding dissimilarity networks, then proving that all three
cannot be satisfied simultaneously by any node embedding method. Similar to
existing results on the impossibility of clustering under certain axiomatic
assumptions, this points to fundamental difficulties inherent to node embedding
tasks. Once these difficulties are identified, we then relax these axioms to
allow for certain node embedding methods to be admissible in our framework.
",5.0
620,2,99305,1910.00943,random forests,Jos\'e A. Ferreira,"Models under which random forests perform badly; consequences for
  applications","  We give examples of data-generating models under which Breiman's random
forest may be extremely slow to converge to the optimal predictor or even fail
to be consistent. The evidence provided for these properties is based on mostly
intuitive arguments, similar to those used earlier with simpler examples, and
on numerical experiments. Although one can always choose models under which
random forests perform very badly, we show that simple methods based on
statistics of `variable use' and `variable importance' can often be used to
construct a much better predictor based on a `many-armed' random forest
obtained by forcing initial splits on variables which the default version of
the algorithm tends to ignore.
",3.0
621,14,118257,2005.04684,text summarization model,"Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao and Rui Yan","From Standard Summarization to New Tasks and Beyond: Summarization with
  Manifold Information","  Text summarization is the research area aiming at creating a short and
condensed version of the original document, which conveys the main idea of the
document in a few words. This research topic has started to attract the
attention of a large community of researchers, and it is nowadays counted as
one of the most promising research areas. In general, text summarization
algorithms aim at using a plain text document as input and then output a
summary. However, in real-world applications, most of the data is not in a
plain text format. Instead, there is much manifold information to be
summarized, such as the summary for a web page based on a query in the search
engine, extreme long document (e.g., academic paper), dialog history and so on.
In this paper, we focus on the survey of these new summarization tasks and
approaches in the real-world application.
",4.0
622,10,127407,2008.00137,web archive,"Shawn M. Jones, Martin Klein, Michele C. Weigle, Michael L. Nelson",MementoEmbed and Raintale for Web Archive Storytelling,"  For traditional library collections, archivists can select a representative
sample from a collection and display it in a featured physical or digital
library space. Web archive collections may consist of thousands of archived
pages, or mementos. How should an archivist display this sample to drive
visitors to their collection? Search engines and social media platforms often
represent web pages as cards consisting of text snippets, titles, and images.
Web storytelling is a popular method for grouping these cards in order to
summarize a topic. Unfortunately, social media platforms are not archive-aware
and fail to consistently create a good experience for mementos. They also allow
no UI alterations for their cards. Thus, we created MementoEmbed to generate
cards for individual mementos and Raintale for creating entire stories that
archivists can export to a variety of formats.
",4.0
623,12,202931,2207.03997,COVID-19 and social media,Mohammadsepehr Karimiziarani,"A Tutorial on Event Detection using Social Media Data Analysis:
  Applications, Challenges, and Open Problems","  In recent years, social media has become one of the most popular platforms
for communication. These platforms allow users to report real-world incidents
that might swiftly and widely circulate throughout the whole social network. A
social event is a real-world incident that is documented on social media.
Social gatherings could contain vital documentation of crisis scenarios.
Monitoring and analyzing this rich content can produce information that is
extraordinarily valuable and help people and organizations learn how to take
action. In this paper, a survey on the potential benefits and applications of
event detection with social media data analysis will be presented. Moreover,
the critical challenges and the fundamental tradeoffs in event detection will
be methodically investigated by monitoring social media stream. Then,
fundamental open questions and possible research directions will be introduced.
",1.0
624,4,57422,1709.03136,pre-trained language model,Vahid Moosavi,"Computational Machines in a Coexistence with Concrete Universals and
  Data Streams","  We discuss that how the majority of traditional modeling approaches are
following the idealism point of view in scientific modeling, which follow the
set theoretical notions of models based on abstract universals. We show that
while successful in many classical modeling domains, there are fundamental
limits to the application of set theoretical models in dealing with complex
systems with many potential aspects or properties depending on the
perspectives. As an alternative to abstract universals, we propose a conceptual
modeling framework based on concrete universals that can be interpreted as a
category theoretical approach to modeling. We call this modeling framework
pre-specific modeling. We further, discuss how a certain group of mathematical
and computational methods, along with ever-growing data streams are able to
operationalize the concept of pre-specific modeling.
",1.0
625,11,7499,1102.0695,PageRank for web search,"Debajyoti Mukhopadhyay, Aritra Banik, Sreemoyee Mukherjee, Jhilik
  Bhattacharya, Young-Chon Kim",A Domain Specific Ontology Based Semantic Web Search Engine,"  Since its emergence in the 1990s the World Wide Web (WWW) has rapidly evolved
into a huge mine of global information and it is growing in size everyday. The
presence of huge amount of resources on the Web thus poses a serious problem of
accurate search. This is mainly because today's Web is a human-readable Web
where information cannot be easily processed by machine. Highly sophisticated,
efficient keyword based search engines that have evolved today have not been
able to bridge this gap. So comes up the concept of the Semantic Web which is
envisioned by Tim Berners-Lee as the Web of machine interpretable information
to make a machine processable form for expressing information. Based on the
semantic Web technologies we present in this paper the design methodology and
development of a semantic Web search engine which provides exact search results
for a domain specific search. This search engine is developed for an
agricultural Website which hosts agricultural information about the state of
West Bengal.
",1.0
626,0,47077,1610.1,learning to rank with partitioned preference,"Xueqing Liu, Chengxiang Zhai, Wei Han, Onur Gungor",Numerical Facet Range Partition: Evaluation Metric and Methods,"  Faceted navigation is a very useful component in today's search engines. It
is especially useful when user has an exploratory information need or prefer
certain attribute values than others. Existing work has tried to optimize
faceted systems in many aspects, but little work has been done on optimizing
numerical facet ranges (e.g., price ranges of product). In this paper, we
introduce for the first time the research problem on numerical facet range
partition and formally frame it as an optimization problem. To enable
quantitative evaluation of a partition algorithm, we propose an evaluation
metric to be applied to search engine logs. We further propose two range
partition algorithms that computationally optimize the defined metric.
Experimental results on a two-month search log from a major e-Commerce engine
show that our proposed method can significantly outperform baseline.
",0.0
627,0,167851,2109.00062,learning to rank with partitioned preference,"Negar Arabzadeh and Alexandra Vtyurina and Xinyi Yan and Charles L. A.
  Clarke",Shallow pooling for sparse labels,"  Recent years have seen enormous gains in core IR tasks, including document
and passage ranking. Datasets and leaderboards, and in particular the MS MARCO
datasets, illustrate the dramatic improvements achieved by modern neural
rankers. When compared with traditional test collections, the MS MARCO datasets
employ substantially more queries with substantially fewer known relevant items
per query. Given the sparsity of these relevance labels, the MS MARCO
leaderboards track improvements with mean reciprocal rank (MRR). In essence, a
relevant item is treated as the ""right answer"", with rankers scored on their
ability to place this item high in the ranking. In working with these sparse
labels, we have observed that the top items returned by a ranker often appear
superior to judged relevant items. To test this observation, we employed
crowdsourced workers to make preference judgments between the top item returned
by a modern neural ranking stack and a judged relevant item. The results
support our observation. If we imagine a perfect ranker under MRR, with a score
of 1 on all queries, our preference judgments indicate that a searcher would
prefer the top result from a modern neural ranking stack more frequently than
the top result from the imaginary perfect ranker, making our neural ranker
""better than perfect"". To understand the implications for the leaderboard, we
pooled the top document from available runs near the top of the passage ranking
leaderboard for over 500 queries. We employed crowdsourced workers to make
preference judgments over these pools and re-evaluated the runs. Our results
support our concerns that current MS MARCO datasets may no longer be able to
recognize genuine improvements in rankers. In future, if rankers are measured
against a single ""right answer"", this answer should be the best answer or most
preferred answer, and maintained with ongoing judgments.
",0.0
628,15,186391,2202.08751,relevance feedback for imformation retrieval,"Syed Raza Bashir, Vojislav Misic",Improving Rating and Relevance with Point-of-Interest Recommender System,"  The recommendation of points of interest (POIs) is essential in
location-based social networks. It makes it easier for users and locations to
share information. Recently, researchers tend to recommend POIs by treating
them as large-scale retrieval systems that require a large amount of training
data representing query-item relevance. However, gathering user feedback in
retrieval systems is an expensive task. Existing POI recommender systems make
recommendations based on user and item (location) interactions solely. However,
there are numerous sources of feedback to consider. For example, when the user
visits a POI, what is the POI is about and such. Integrating all these
different types of feedback is essential when developing a POI recommender. In
this paper, we propose using user and item information and auxiliary
information to improve the recommendation modelling in a retrieval system. We
develop a deep neural network architecture to model query-item relevance in the
presence of both collaborative and content information. We also improve the
quality of the learned representations of queries and items by including the
contextual information from the user feedback data. The application of these
learned representations to a large-scale dataset resulted in significant
improvements.
",3.0
629,12,182010,2201.0333,COVID-19 and social media,"Emmanuel Etuh, Francis S. Bakpo and Eneh Agozie H",Social Media Networks Attacks and their Preventive Mechanisms: A Review,"  We live in a virtual world where actual lifestyles are replicated. The
growing reliance on the use of social media networks worldwide has resulted in
great concern for information security. One of the factors popularizing the
social media platforms is how they connect people worldwide to interact, share
content, and engage in mutual interactions of common interest that cut across
geographical boundaries. Behind all these incredible gains are digital crime
equivalence that threatens the physical socialization. Criminal-minded elements
and hackers are exploiting Social Media Platforms (SMP) for many nefarious
activities to harm others. As detection tools are developed to control these
crimes, hackers' tactics and techniques are constantly evolving. Hackers are
constantly developing new attacking tools and hacking strategies to gain
malicious access to systems and attack social media network thereby making it
difficult for security administrators and organizations to develop and
implement the proper policies and procedures necessary to prevent the hackers'
attacks. The increase in cyber-attacks on the social media platforms calls for
urgent and more intelligent security measures to enhance the effectiveness of
social media platforms. This paper explores the mode and tactics of hackers'
mode of attacks on social media and ways of preventing their activities against
users to ensure secure social cyberspace and enhance virtual socialization.
Social media platforms are briefly categorized, the various types of attacks
are also highlighted with current state-of-the-art preventive mechanisms to
overcome the attacks as proposed in research works, finally, social media
intrusion detection mechanism is suggested as a second line of defense to
combat cybercrime on social media networks
",1.0
630,12,87407,1905.00635,COVID-19 and social media,"Martina Patone, Li-Chun Zhang",On two existing approaches to statistical analysis of social media data,"  Using social media data for statistical analysis of general population faces
commonly two basic obstacles: firstly, social media data are collected for
different objects than the population units of interest; secondly, the relevant
measures are typically not available directly but need to be extracted by
algorithms or machine learning techniques. In this paper we examine and
summarise two existing approaches to statistical analysis based on social media
data, which can be discerned in the literature. In the first approach, analysis
is applied to the social media data that are organised around the objects
directly observed in the data; in the second one, a different analysis is
applied to a constructed pseudo survey dataset, aimed to transform the observed
social media data to a set of units from the target population. We elaborate
systematically the relevant data quality frameworks, exemplify their
applications, and highlight some typical challenges associated with social
media data.
",1.0
631,11,20397,1305.5959,PageRank for web search,Ahmed AlSum and Michael L. Nelson,"ArcLink: Optimization Techniques to Build and Retrieve the Temporal Web
  Graph","  Archiving the web is socially and culturally critical, but presents problems
of scale. The Internet Archive's Wayback Machine can replay captured web pages
as they existed at a certain point in time, but it has limited ability to
provide extensive content and structural metadata about the web graph. While
the live web has developed a rich ecosystem of APIs to facilitate web
applications (e.g., APIs from Google and Twitter), the web archiving community
has not yet broadly implemented this level of access.
  We present ArcLink, a proof-of-concept system that complements open source
Wayback Machine installations by optimizing the construction, storage, and
access to the temporal web graph. We divide the web graph construction into
four stages (filtering, extraction, storage, and access) and explore
optimization for each stage. ArcLink extends the current Web archive interfaces
to return content and structural metadata for each URI. We show how this API
can be applied to such applications as retrieving inlinks, outlinks,
anchortext, and PageRank.
",2.0
632,19,147038,2102.07627,artificial intelligence for low carbon,"Xinchi Qiu, Titouan Parcollet, Javier Fernandez-Marques, Pedro Porto
  Buarque de Gusmao, Yan Gao, Daniel J. Beutel, Taner Topal, Akhil Mathur,
  Nicholas D. Lane",A first look into the carbon footprint of federated learning,"  Despite impressive results, deep learning-based technologies also raise
severe privacy and environmental concerns induced by the training procedure
often conducted in data centers. In response, alternatives to centralized
training such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL
is starting to be deployed at a global scale by companies that must adhere to
new legal demands and policies originating from governments and social groups
advocating for privacy protection. \textit{However, the potential environmental
impact related to FL remains unclear and unexplored. This paper offers the
first-ever systematic study of the carbon footprint of FL.} First, we propose a
rigorous model to quantify the carbon footprint, hence facilitating the
investigation of the relationship between FL design and carbon emissions. Then,
we compare the carbon footprint of FL to traditional centralized learning. Our
findings show that, depending on the configuration, FL can emit up to two order
of magnitude more carbon than centralized machine learning. However, in certain
settings, it can be comparable to centralized learning due to the reduced
energy consumption of embedded devices. We performed extensive experiments
across different types of datasets, settings and various deep learning models
with FL. Finally, we highlight and connect the reported results to the future
challenges and trends in FL to reduce its environmental impact, including
algorithms efficiency, hardware capabilities, and stronger industry
transparency.
",2.0
633,4,103408,1911.07056,pre-trained language model,Pattarawat Chormai and Ponrawee Prasertsom and Attapol Rutherford,AttaCut: A Fast and Accurate Neural Thai Word Segmenter,"  Word segmentation is a fundamental pre-processing step for Thai Natural
Language Processing. The current off-the-shelf solutions are not benchmarked
consistently, so it is difficult to compare their trade-offs. We conducted a
speed and accuracy comparison of the popular systems on three different domains
and found that the state-of-the-art deep learning system is slow and moreover
does not use sub-word structures to guide the model. Here, we propose a fast
and accurate neural Thai Word Segmenter that uses dilated CNN filters to
capture the environment of each character and uses syllable embeddings as
features. Our system runs at least 5.6x faster and outperforms the previous
state-of-the-art system on some domains. In addition, we develop the first
ML-based Thai orthographical syllable segmenter, which yields syllable
embeddings to be used as features by the word segmenter.
",5.0
634,15,2975,904.4041,relevance feedback for imformation retrieval,Jie Luo and Mario A. Nascimento,Content-Based Sub-Image Retrieval with Relevance Feedback,"  The typical content-based image retrieval problem is to find images within a
database that are similar to a given query image. This paper presents a
solution to a different problem, namely that of content based sub-image
retrieval, i.e., finding images from a database that contains another image.
Note that this is different from finding a region in a (segmented) image that
is similar to another image region given as a query. We present a technique for
CBsIR that explores relevance feedback, i.e., the user's input on intermediary
results, in order to improve retrieval efficiency. Upon modeling images as a
set of overlapping and recursive tiles, we use a tile re-weighting scheme that
assigns penalties to each tile of the database images and updates the tile
penalties for all relevant images retrieved at each iteration using both the
relevant and irrelevant images identified by the user. Each tile is modeled by
means of its color content using a compact but very efficient method which can,
indirectly, capture some notion of texture as well, despite the fact that only
color information is maintained. Performance evaluation on a largely
heterogeneous dataset of over 10,000 images shows that the system can achieve a
stable average recall value of 70% within the top 20 retrieved (and presented)
images after only 5 iterations, with each such iteration taking about 2 seconds
on an off-the-shelf desktop computer.
",3.0
635,19,216954,cs/0311031,artificial intelligence for low carbon,J. Gerard Wolff,"Towards an Intelligent Database System Founded on the SP Theory of
  Computing and Cognition","  The SP theory of computing and cognition, described in previous publications,
is an attractive model for intelligent databases because it provides a simple
but versatile format for different kinds of knowledge, it has capabilities in
artificial intelligence, and it can also function like established database
models when that is required.
  This paper describes how the SP model can emulate other models used in
database applications and compares the SP model with those other models. The
artificial intelligence capabilities of the SP model are reviewed and its
relationship with other artificial intelligence systems is described. Also
considered are ways in which current prototypes may be translated into an
'industrial strength' working system.
",1.0
636,3,216587,cs/0202014,database management system,"Jim Gray, Alex S. Szalay, Ani R. Thakar, Peter Z. Kunszt, Christopher
  Stoughton, Don Slutz, Jan vandenBerg",Data Mining the SDSS SkyServer Database,"  An earlier paper (Szalay et. al. ""Designing and Mining MultiTerabyte
Astronomy Archives: The Sloan Digital Sky Survey,"" ACM SIGMOD 2000) described
the Sloan Digital Sky Survey's (SDSS) data management needs by defining twenty
database queries and twelve data visualization tasks that a good data
management system should support. We built a database and interfaces to support
both the query load and also a website for ad-hoc access. This paper reports on
the database design, describes the data loading pipeline, and reports on the
query implementation and performance. The queries typically translated to a
single SQL statement. Most queries run in less than 20 seconds, allowing
scientists to interactively explore the database. This paper is an in-depth
tour of those queries. Readers should first have studied the companion overview
paper Szalay et. al. ""The SDSS SkyServer, Public Access to the Sloan Digital
Sky Server Data"" ACM SIGMOND 2002.
",5.0
637,14,99750,1910.03177,text summarization model,"Rajeev Bhatt Ambati, Saptarashmi Bandyopadhyay and Prasenjit Mitra","Read, Highlight and Summarize: A Hierarchical Neural Semantic
  Encoder-based Approach","  Traditional sequence-to-sequence (seq2seq) models and other variations of the
attention-mechanism such as hierarchical attention have been applied to the
text summarization problem. Though there is a hierarchy in the way humans use
language by forming paragraphs from sentences and sentences from words,
hierarchical models have usually not worked that much better than their
traditional seq2seq counterparts. This effect is mainly because either the
hierarchical attention mechanisms are too sparse using hard attention or noisy
using soft attention. In this paper, we propose a method based on extracting
the highlights of a document; a key concept that is conveyed in a few
sentences. In a typical text summarization dataset consisting of documents that
are 800 tokens in length (average), capturing long-term dependencies is very
important, e.g., the last sentence can be grouped with the first sentence of a
document to form a summary. LSTMs (Long Short-Term Memory) proved useful for
machine translation. However, they often fail to capture long-term dependencies
while modeling long sequences. To address these issues, we have adapted Neural
Semantic Encoders (NSE) to text summarization, a class of memory-augmented
neural networks by improving its functionalities and proposed a novel
hierarchical NSE that outperforms similar previous models significantly. The
quality of summarization was improved by augmenting linguistic factors, namely
lemma, and Part-of-Speech (PoS) tags, to each word in the dataset for improved
vocabulary coverage and generalization. The hierarchical NSE model on factored
dataset outperformed the state-of-the-art by nearly 4 ROUGE points. We further
designed and used the first GPU-based self-critical Reinforcement Learning
model.
",5.0
638,12,140832,2012.05852,COVID-19 and social media,"Valerio Lorini, Carlos Castillo, Domenico Nappo, Francesco Dottori,
  Peter Salamon","Social Media Alerts can Improve, but not Replace Hydrological Models for
  Forecasting Floods","  Social media can be used for disaster risk reduction as a complement to
traditional information sources, and the literature has suggested numerous ways
to achieve this. In the case of floods, for instance, data collection from
social media can be triggered by a severe weather forecast and/or a flood
prediction. By way of contrast, in this paper we explore the possibility of
having an entirely independent flood monitoring system which is based
completely on social media, and which is completely self-activated. This
independence and self-activation would bring increased robustness, as the
system would not depend on other mechanisms for forecasting. We observe that
social media can indeed help in the early detection of some flood events that
would otherwise not be detected until later, albeit at the cost of many false
positives. Overall, our experiments suggest that social media signals should
only be used to complement existing monitoring systems, and we provide various
explanations to support this argument.
",1.0
639,18,5974,1007.5137,infomation retrieval time complexity,"Rajeswari Sridhar, A. Amudha, S. Karthiga and Geetha T V (Anna
  University-Chennai, India)","Comparison Of Modified Dual Ternary Indexing And Multi-Key Hashing
  Algorithms For Music Information Retrieval","  In this work we have compared two indexing algorithms that have been used to
index and retrieve Carnatic music songs. We have compared a modified algorithm
of the Dual ternary indexing algorithm for music indexing and retrieval with
the multi-key hashing indexing algorithm proposed by us. The modification in
the dual ternary algorithm was essential to handle variable length query phrase
and to accommodate features specific to Carnatic music. The dual ternary
indexing algorithm is adapted for Carnatic music by segmenting using the
segmentation technique for Carnatic music. The dual ternary algorithm is
compared with the multi-key hashing algorithm designed by us for indexing and
retrieval in which features like MFCC, spectral flux, melody string and
spectral centroid are used as features for indexing data into a hash table. The
way in which collision resolution was handled by this hash table is different
than the normal hash table approaches. It was observed that multi-key hashing
based retrieval had a lesser time complexity than dual-ternary based indexing
The algorithms were also compared for their precision and recall in which
multi-key hashing had a better recall than modified dual ternary indexing for
the sample data considered.
",4.0
640,9,103234,1911.06154,language model for long documents,"Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzman, Philipp Koehn",CCAligned: A Massive Collection of Cross-Lingual Web-Document Pairs,"  Cross-lingual document alignment aims to identify pairs of documents in two
distinct languages that are of comparable content or translations of each
other. In this paper, we exploit the signals embedded in URLs to label web
documents at scale with an average precision of 94.5% across different language
pairs. We mine sixty-eight snapshots of the Common Crawl corpus and identify
web document pairs that are translations of each other. We release a new web
dataset consisting of over 392 million URL pairs from Common Crawl covering
documents in 8144 language pairs of which 137 pairs include English. In
addition to curating this massive dataset, we introduce baseline methods that
leverage cross-lingual representations to identify aligned documents based on
their textual content. Finally, we demonstrate the value of this parallel
documents dataset through a downstream task of mining parallel sentences and
measuring the quality of machine translations from models trained on this mined
data. Our objective in releasing this dataset is to foster new research in
cross-lingual NLP across a variety of low, medium, and high-resource languages.
",1.0
641,19,213823,2210.0985,artificial intelligence for low carbon,"Zhengwen Zhang, Jinjin Gu, Junhua Zhao, Jianwei Huang, Haifeng Wu","Near Real-time CO$_2$ Emissions Based on Carbon Satellite and Artificial
  Intelligence","  To limit global warming to pre-industrial levels, global governments,
industry and academia are taking aggressive efforts to reduce carbon emissions.
The evaluation of anthropogenic carbon dioxide (CO$_2$) emissions, however,
depends on the self-reporting information that is not always reliable. Society
need to develop an objective, independent, and generalized system to meter
CO$_2$ emissions. Satellite CO$_2$ observation from space that reports
column-average regional CO$_2$ dry-air mole fractions has gradually indicated
its potential to build such a system. Nevertheless, estimating anthropogenic
CO$_2$ emissions from CO$_2$ observing satellite is bottlenecked by the
influence of the highly complicated physical characteristics of atmospheric
activities. Here we provide the first method that combines the advanced
artificial intelligence (AI) techniques and the carbon satellite monitor to
quantify anthropogenic CO$_2$ emissions. We propose an integral AI based
pipeline that contains both a data retrieval algorithm and a two-step
data-driven solution. First, the data retrieval algorithm can generate
effective datasets from multi-modal data including carbon satellite, the
information of carbon sources, and several environmental factors. Second, the
two-step data-driven solution that applies the powerful representation of deep
learning techniques to learn to quantify anthropogenic CO$_2$ emissions from
satellite CO$_2$ observation with other factors. Our work unmasks the potential
of quantifying CO$_2$ emissions based on the combination of deep learning
algorithms and the carbon satellite monitor.
",5.0
642,19,187342,2202.12678,artificial intelligence for low carbon,"Milad Moradi, Matthias Samwald","Deep Learning, Natural Language Processing, and Explainable Artificial
  Intelligence in the Biomedical Domain","  In this article, we first give an introduction to artificial intelligence and
its applications in biology and medicine in Section 1. Deep learning methods
are then described in Section 2. We narrow down the focus of the study on
textual data in Section 3, where natural language processing and its
applications in the biomedical domain are described. In Section 4, we give an
introduction to explainable artificial intelligence and discuss the importance
of explainability of artificial intelligence systems, especially in the
biomedical domain.
",2.0
643,4,203794,2207.08087,pre-trained language model,"Yinghui Li, Shulin Huang, Xinwei Zhang, Qingyu Zhou, Yangning Li,
  Ruiyang Liu, Yunbo Cao, Hai-Tao Zheng, Ying Shen",Automatic Context Pattern Generation for Entity Set Expansion,"  Entity Set Expansion (ESE) is a valuable task that aims to find entities of
the target semantic class described by given seed entities. Various NLP and IR
downstream applications have benefited from ESE due to its ability to discover
knowledge. Although existing bootstrapping methods have achieved great
progress, most of them still rely on manually pre-defined context patterns. A
non-negligible shortcoming of the pre-defined context patterns is that they
cannot be flexibly generalized to all kinds of semantic classes, and we call
this phenomenon as ""semantic sensitivity"". To address this problem, we devise a
context pattern generation module that utilizes autoregressive language models
(e.g., GPT-2) to automatically generate high-quality context patterns for
entities. In addition, we propose the GAPA, a novel ESE framework that
leverages the aforementioned GenerAted PAtterns to expand target entities.
Extensive experiments and detailed analyses on three widely used datasets
demonstrate the effectiveness of our method. All the codes of our experiments
will be available for reproducibility.
",2.0
644,11,13815,1206.5584,PageRank for web search,"Sukanta Sinha, Rana Duttagupta, Debajyoti Mukhopadhyay","Web-page Prediction for Domain Specific Web-search using Boolean Bit
  Mask","  Search Engine is a Web-page retrieval tool. Nowadays Web searchers utilize
their time using an efficient search engine. To improve the performance of the
search engine, we are introducing a unique mechanism which will give Web
searchers more prominent search results. In this paper, we are going to discuss
a domain specific Web search prototype which will generate the predicted
Web-page list for user given search string using Boolean bit mask.
",1.0
645,11,12074,1203.1457,PageRank for web search,Olivier Fercoq,PageRank optimization applied to spam detection,"  We give a new link spam detection and PageRank demotion algorithm called
MaxRank. Like TrustRank and AntiTrustRank, it starts with a seed of hand-picked
trusted and spam pages. We define the MaxRank of a page as the frequency of
visit of this page by a random surfer minimizing an average cost per time unit.
On a given page, the random surfer selects a set of hyperlinks and clicks with
uniform probability on any of these hyperlinks. The cost function penalizes
spam pages and hyperlink removals. The goal is to determine a hyperlink
deletion policy that minimizes this score. The MaxRank is interpreted as a
modified PageRank vector, used to sort web pages instead of the usual PageRank
vector. The bias vector of this ergodic control problem, which is unique up to
an additive constant, is a measure of the ""spamicity"" of each page, used to
detect spam pages. We give a scalable algorithm for MaxRank computation that
allowed us to perform experimental results on the WEBSPAM-UK2007 dataset. We
show that our algorithm outperforms both TrustRank and AntiTrustRank for spam
and nonspam page detection.
",4.0
646,8,201540,2206.13176,node embedding for graph,"Yifan Hou, Hongzhi Chen, Changji Li, James Cheng, Ming-Chang Yang",A Representation Learning Framework for Property Graphs,"  Representation learning on graphs, also called graph embedding, has
demonstrated its significant impact on a series of machine learning
applications such as classification, prediction and recommendation. However,
existing work has largely ignored the rich information contained in the
properties (or attributes) of both nodes and edges of graphs in modern
applications, e.g., those represented by property graphs. To date, most
existing graph embedding methods either focus on plain graphs with only the
graph topology, or consider properties on nodes only. We propose PGE, a graph
representation learning framework that incorporates both node and edge
properties into the graph embedding procedure. PGE uses node clustering to
assign biases to differentiate neighbors of a node and leverages multiple
data-driven matrices to aggregate the property information of neighbors sampled
based on a biased strategy. PGE adopts the popular inductive model for
neighborhood aggregation. We provide detailed analyses on the efficacy of our
method and validate the performance of PGE by showing how PGE achieves better
embedding results than the state-of-the-art graph embedding methods on
benchmark applications such as node classification and link prediction over
real-world datasets.
",4.0
647,0,139367,2011.13772,learning to rank with partitioned preference,"Hung-Hsu Chou, Carsten Gieshoff, Johannes Maly, Holger Rauhut","Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit
  Bias towards Low Rank","  In deep learning, it is common to use more network parameters than training
points. In such scenarioof over-parameterization, there are usually multiple
networks that achieve zero training error so that thetraining algorithm induces
an implicit bias on the computed solution. In practice, (stochastic)
gradientdescent tends to prefer solutions which generalize well, which provides
a possible explanation of thesuccess of deep learning. In this paper we analyze
the dynamics of gradient descent in the simplifiedsetting of linear networks
and of an estimation problem. Although we are not in an
overparameterizedscenario, our analysis nevertheless provides insights into the
phenomenon of implicit bias. In fact, wederive a rigorous analysis of the
dynamics of vanilla gradient descent, and characterize the dynamicalconvergence
of the spectrum. We are able to accurately locate time intervals where the
effective rankof the iterates is close to the effective rank of a low-rank
projection of the ground-truth matrix. Inpractice, those intervals can be used
as criteria for early stopping if a certain regularity is desired. Wealso
provide empirical evidence for implicit bias in more general scenarios, such as
matrix sensing andrandom initialization. This suggests that deep learning
prefers trajectories whose complexity (measuredin terms of effective rank) is
monotonically increasing, which we believe is a fundamental concept for
thetheoretical understanding of deep learning.
",0.0
648,1,12551,1204.2079,advanced search engine,"Nacim Yanes, Sihem Ben Sassi, and Henda Hajjami Ben Ghezala","A Theoretical and Empirical Evaluation of Software Component Search
  Engines, Semantic Search Engines and Google Search Engine in the Context of
  COTS-Based Development","  COTS-based development is a component reuse approach promising to reduce
costs and risks, and ensure higher quality. The growing availability of COTS
components on the Web has concretized the possibility of achieving these
objectives. In this multitude, a recurrent problem is the identification of the
COTS components that best satisfy the user requirements. Finding an adequate
COTS component implies searching among heterogeneous descriptions of the
components within a broad search space. Thus, the use of search engines is
required to make more efficient the COTS components identification. In this
paper, we investigate, theoretically and empirically, the COTS component search
performance of eight software component search engines, nine semantic search
engines and a conventional search engine (Google). Our empirical evaluation is
conducted with respect to precision and normalized recall. We defined ten
queries for the assessed search engines. These queries were carefully selected
to evaluate the capability of each search engine for handling COTS component
identification.
",4.0
649,2,4755,1002.224,random forests,Joe Suzuki,"A Generalization of the Chow-Liu Algorithm and its Application to
  Statistical Learning","  We extend the Chow-Liu algorithm for general random variables while the
previous versions only considered finite cases. In particular, this paper
applies the generalization to Suzuki's learning algorithm that generates from
data forests rather than trees based on the minimum description length by
balancing the fitness of the data to the forest and the simplicity of the
forest. As a result, we successfully obtain an algorithm when both of the
Gaussian and finite random variables are present.
",5.0
650,6,42763,1605.07891,query expansion for imformation retrieval,"Fernando Diaz, Bhaskar Mitra, Nick Craswell",Query Expansion with Locally-Trained Word Embeddings,"  Continuous space word embeddings have received a great deal of attention in
the natural language processing and machine learning communities for their
ability to model term similarity and other relationships. We study the use of
term relatedness in the context of query expansion for ad hoc information
retrieval. We demonstrate that word embeddings such as word2vec and GloVe, when
trained globally, underperform corpus and query specific embeddings for
retrieval tasks. These results suggest that other tasks benefiting from global
embeddings may also benefit from local embeddings.
",4.0
651,11,50593,1702.06934,PageRank for web search,Olegs Verhodubs,Realization of Ontology Web Search Engine,"  This paper describes the realization of the Ontology Web Search Engine. The
Ontology Web Search Engine is realizable as independent project and as a part
of other projects. The main purpose of this paper is to present the Ontology
Web Search Engine realization details as the part of the Semantic Web Expert
System and to present the results of the Ontology Web Search Engine
functioning. It is expected that the Semantic Web Expert System will be able to
process ontologies from the Web, generate rules from these ontologies and
develop its knowledge base.
",1.0
652,6,150529,2103.10474,query expansion for imformation retrieval,"Onifade Olufade, Arise Abiola, Ogboo Chisom","Dynamic Model for Query-Document Expansion towards Improving Retrieval
  Relevance","  Getting relevant information from search engines has been the heart of
research works in information retrieval. Query expansion is a retrieval
technique that has been studied and proved to yield positive results in
relevance. Users are required to express their queries as a shortlist of words,
sentences, or questions. With this short format, a huge amount of information
is lost in the process of translating the information need from the actual
query size since the user cannot convey all his thoughts in a few words. This
mostly leads to poor query representation which contributes to undesired
retrieval effectiveness. This loss of information has made the study of query
expansion technique a strong area of study. This research work focuses on two
methods of retrieval for both tweet-length queries and sentence-length queries.
Two algorithms have been proposed and the implementation is expected to produce
a better relevance retrieval model than most state-the-art relevance models.
",5.0
653,16,95324,1908.03682,activation function in neutral networks,"Yang Liu, Jianpeng Zhang, Chao Gao, Jinghua Qu, Lixin Ji","Natural-Logarithm-Rectified Activation Function in Convolutional Neural
  Networks","  Activation functions play a key role in providing remarkable performance in
deep neural networks, and the rectified linear unit (ReLU) is one of the most
widely used activation functions. Various new activation functions and
improvements on ReLU have been proposed, but each carry performance drawbacks.
In this paper, we propose an improved activation function, which we name the
natural-logarithm-rectified linear unit (NLReLU). This activation function uses
the parametric natural logarithmic transform to improve ReLU and is simply
defined as. NLReLU not only retains the sparse activation characteristic of
ReLU, but it also alleviates the ""dying ReLU"" and vanishing gradient problems
to some extent. It also reduces the bias shift effect and heteroscedasticity of
neuron data distributions among network layers in order to accelerate the
learning process. The proposed method was verified across ten convolutional
neural networks with different depths for two essential datasets. Experiments
illustrate that convolutional neural networks with NLReLU exhibit higher
accuracy than those with ReLU, and that NLReLU is comparable to other
well-known activation functions. NLReLU provides 0.16% and 2.04% higher
classification accuracy on average compared to ReLU when used in shallow
convolutional neural networks with the MNIST and CIFAR-10 datasets,
respectively. The average accuracy of deep convolutional neural networks with
NLReLU is 1.35% higher on average with the CIFAR-10 dataset.
",5.0
654,7,94630,1907.12608,gradient boosting,Erhan Bilal,"Deep Gradient Boosting -- Layer-wise Input Normalization of Neural
  Networks","  Stochastic gradient descent (SGD) has been the dominant optimization method
for training deep neural networks due to its many desirable properties. One of
the more remarkable and least understood quality of SGD is that it generalizes
relatively well on unseen data even when the neural network has millions of
parameters. We hypothesize that in certain cases it is desirable to relax its
intrinsic generalization properties and introduce an extension of SGD called
deep gradient boosting (DGB). The key idea of DGB is that back-propagated
gradients inferred using the chain rule can be viewed as pseudo-residual
targets of a gradient boosting problem. Thus at each layer of a neural network
the weight update is calculated by solving the corresponding boosting problem
using a linear base learner. The resulting weight update formula can also be
viewed as a normalization procedure of the data that arrives at each layer
during the forward pass. When implemented as a separate input normalization
layer (INN) the new architecture shows improved performance on image
recognition tasks when compared to the same architecture without normalization
layers. As opposed to batch normalization (BN), INN has no learnable parameters
however it matches its performance on CIFAR10 and ImageNet classification
tasks.
",3.0
655,15,197210,2205.1268,relevance feedback for imformation retrieval,"Mujeen Sung, Jungsoo Park, Jaewoo Kang, Danqi Chen, Jinhyuk Lee",Refining Query Representations for Dense Retrieval at Test Time,"  Dense retrieval uses a contrastive learning framework to learn dense
representations of queries and contexts. Trained encoders are directly used for
each test query, but they often fail to accurately represent out-of-domain
queries. In this paper, we introduce a framework that refines instance-level
query representations at test time, with only the signals coming from the
intermediate retrieval results. We optimize the query representation based on
the retrieval result similar to pseudo relevance feedback (PRF) in information
retrieval. Specifically, we adopt a cross-encoder labeler to provide pseudo
labels over the retrieval result and iteratively refine the query
representation with a gradient descent method, treating each test query as a
single data point to train on. Our theoretical analysis reveals that our
framework can be viewed as a generalization of the classical Rocchio's
algorithm for PRF, which leads us to propose interesting variants of our
method. We show that our test-time query refinement strategy improves the
performance of phrase retrieval (+8.1% Acc@1) and passage retrieval (+3.7%
Acc@20) for open-domain QA with large improvements on out-of-domain queries.
",2.0
656,11,35234,1507.05214,PageRank for web search,Antonia Korba,"On the Application of Link Analysis Algorithms for Ranking Bipartite
  Graphs","  Recently bipartite graphs have been widely used to represent the relationship
two sets of items for information retrieval applications. The Web offers a wide
range of data which can be represented by bipartite graphs, such us movies and
reviewers in recomender systems, queries and URLs in search engines, users and
posts in social networks. The size and the dynamic nature of such graphs
generate the need for more efficient ranking methods.
  In this thesis, at first we present the fundamental mathematical backround
that we use subsequently and we describe the basic principles of the
Perron-Frobebius theory for non negative matrices as well as the the basic
principles of the Markov chain theory. Then, we propose a novel algorithm named
BipartiteRank, which is suitable to rank scenarios, that can be represented as
a bipartite graph. This algorithm is based on the random surfer model and
inherits the basic mathematical characteristics of PageRank. What makes it
different, is the fact that it introduces an alternative type of teleportation,
based on the block structure of the bipartite graph in order to achieve more
efficient ranking. Finally, we support this opinion with mathematical arguments
and then we confirm it experimentally through a series of tests on real data.
",2.0
657,0,179813,2112.07428,learning to rank with partitioned preference,"Wonbin Kweon, SeongKu Kang, Hwanjo Yu",Obtaining Calibrated Probabilities with Personalized Ranking Models,"  For personalized ranking models, the well-calibrated probability of an item
being preferred by a user has great practical value. While existing work shows
promising results in image classification, probability calibration has not been
much explored for personalized ranking. In this paper, we aim to estimate the
calibrated probability of how likely a user will prefer an item. We investigate
various parametric distributions and propose two parametric calibration
methods, namely Gaussian calibration and Gamma calibration. Each proposed
method can be seen as a post-processing function that maps the ranking scores
of pre-trained models to well-calibrated preference probabilities, without
affecting the recommendation performance. We also design the unbiased empirical
risk minimization framework that guides the calibration methods to learning of
true preference probability from the biased user-item interaction dataset.
Extensive evaluations with various personalized ranking models on real-world
datasets show that both the proposed calibration methods and the unbiased
empirical risk minimization significantly improve the calibration performance.
",1.0
658,8,126381,2007.10445,node embedding for graph,"Vassilis N. Ioannidis, Da Zheng, George Karypis","PanRep: Graph neural networks for extracting universal node embeddings
  in heterogeneous graphs","  Learning unsupervised node embeddings facilitates several downstream tasks
such as node classification and link prediction. A node embedding is universal
if it is designed to be used by and benefit various downstream tasks. This work
introduces PanRep, a graph neural network (GNN) model, for unsupervised
learning of universal node representations for heterogenous graphs. PanRep
consists of a GNN encoder that obtains node embeddings and four decoders, each
capturing different topological and node feature properties. Abiding to these
properties the novel unsupervised framework learns universal embeddings
applicable to different downstream tasks. PanRep can be furthered fine-tuned to
account for possible limited labels. In this operational setting PanRep is
considered as a pretrained model for extracting node embeddings of heterogenous
graph data. PanRep outperforms all unsupervised and certain supervised methods
in node classification and link prediction, especially when the labeled data
for the supervised methods is small. PanRep-FT (with fine-tuning) outperforms
all other supervised approaches, which corroborates the merits of pretraining
models. Finally, we apply PanRep-FT for discovering novel drugs for Covid-19.
We showcase the advantage of universal embeddings in drug repurposing and
identify several drugs used in clinical trials as possible drug candidates.
",5.0
659,0,2242,812.1811,learning to rank with partitioned preference,"J.-C. Delvenne, S.N. Yaliraki, and M. Barahona",Stability of graph communities across time scales,"  The complexity of biological, social and engineering networks makes it
desirable to find natural partitions into communities that can act as
simplified descriptions and provide insight into the structure and function of
the overall system. Although community detection methods abound, there is a
lack of consensus on how to quantify and rank the quality of partitions. We
show here that the quality of a partition can be measured in terms of its
stability, defined in terms of the clustered autocovariance of a Markov process
taking place on the graph. Because the stability has an intrinsic dependence on
time scales of the graph, it allows us to compare and rank partitions at each
time and also to establish the time spans over which partitions are optimal.
Hence the Markov time acts effectively as an intrinsic resolution parameter
that establishes a hierarchy of increasingly coarser clusterings. Within our
framework we can then provide a unifying view of several standard partitioning
measures: modularity and normalized cut size can be interpreted as one-step
time measures, whereas Fiedler's spectral clustering emerges at long times. We
apply our method to characterize the relevance and persistence of partitions
over time for constructive and real networks, including hierarchical graphs and
social networks. We also obtain reduced descriptions for atomic level protein
structures over different time scales.
",0.0
660,4,212731,2210.05709,pre-trained language model,William Held and Diyi Yang,"Shapley Head Pruning: Identifying and Removing Interference in
  Multilingual Transformers","  Multilingual transformer-based models demonstrate remarkable zero and
few-shot transfer across languages by learning and reusing language-agnostic
features. However, as a fixed-size model acquires more languages, its
performance across all languages degrades, a phenomenon termed interference.
Often attributed to limited model capacity, interference is commonly addressed
by adding additional parameters despite evidence that transformer-based models
are overparameterized. In this work, we show that it is possible to reduce
interference by instead identifying and pruning language-specific parameters.
First, we use Shapley Values, a credit allocation metric from coalitional game
theory, to identify attention heads that introduce interference. Then, we show
that removing identified attention heads from a fixed model improves
performance for a target language on both sentence classification and
structural prediction, seeing gains as large as 24.7\%. Finally, we provide
insights on language-agnostic and language-specific attention heads using
attention visualization.
",3.0
661,1,33124,1504.04909,advanced search engine,"Jean-Baptiste Mouret, Jeff Clune",Illuminating search spaces by mapping elites,"  Many fields use search algorithms, which automatically explore a search space
to find high-performing solutions: chemists search through the space of
molecules to discover new drugs; engineers search for stronger, cheaper, safer
designs, scientists search for models that best explain data, etc. The goal of
search algorithms has traditionally been to return the single
highest-performing solution in a search space. Here we describe a new,
fundamentally different type of algorithm that is more useful because it
provides a holistic view of how high-performing solutions are distributed
throughout a search space. It creates a map of high-performing solutions at
each point in a space defined by dimensions of variation that a user gets to
choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites)
algorithm illuminates search spaces, allowing researchers to understand how
interesting attributes of solutions combine to affect performance, either
positively or, equally of interest, negatively. For example, a drug company may
wish to understand how performance changes as the size of molecules and their
cost-to-produce vary. MAP-Elites produces a large diversity of high-performing,
yet qualitatively different solutions, which can be more helpful than a single,
high-performing solution. Interestingly, because MAP-Elites explores more of
the search space, it also tends to find a better overall solution than
state-of-the-art search algorithms. We demonstrate the benefits of this new
algorithm in three different problem domains ranging from producing modular
neural networks to designing simulated and real soft robots. Because MAP-
Elites (1) illuminates the relationship between performance and dimensions of
interest in solutions, (2) returns a set of high-performing, yet diverse
solutions, and (3) improves finding a single, best solution, it will advance
science and engineering.
",4.0
662,7,194121,2204.13792,gradient boosting,"Recep Yusuf Bekci, Yacine Mahdid, Jinling Xing, Nikita Letov, Ying
  Zhang, Zahid Pasha",Probabilistic Models for Manufacturing Lead Times,"  In this study, we utilize Gaussian processes, probabilistic neural network,
natural gradient boosting, and quantile regression augmented gradient boosting
to model lead times of laser manufacturing processes. We introduce
probabilistic modelling in the domain and compare the models in terms of
different abilities. While providing a comparison between the models in
real-life data, our work has many use cases and substantial business value. Our
results indicate that all of the models beat the company estimation benchmark
that uses domain experience and have good calibration with the empirical
frequencies.
",3.0
663,8,101182,1910.10379,node embedding for graph,"Huang Zhenhua, Wang Zhenyu, Zhang Rui, Zhao Yangyang, Xie Xiaohui,
  Sharad Mehrotra","Network2Vec Learning Node Representation Based on Space Mapping in
  Networks","  Complex networks represented as node adjacency matrices constrains the
application of machine learning and parallel algorithms. To address this
limitation, network embedding (i.e., graph representation) has been intensively
studied to learn a fixed-length vector for each node in an embedding space,
where the node properties in the original graph are preserved. Existing methods
mainly focus on learning embedding vectors to preserve nodes proximity, i.e.,
nodes next to each other in the graph space should also be closed in the
embedding space, but do not enforce algebraic statistical properties to be
shared between the embedding space and graph space. In this work, we propose a
lightweight model, entitled Network2Vec, to learn network embedding on the base
of semantic distance mapping between the graph space and embedding space. The
model builds a bridge between the two spaces leveraging the property of group
homomorphism. Experiments on different learning tasks, including node
classification, link prediction, and community visualization, demonstrate the
effectiveness and efficiency of the new embedding method, which improves the
state-of-the-art model by 19% in node classification and 7% in link prediction
tasks at most. In addition, our method is significantly faster, consuming only
a fraction of the time used by some famous methods.
",5.0
664,1,209995,2209.09726,advanced search engine,Christian Riegger and Ilia Petrov,Storage Management with Multi-Version Partitioned B-Trees,"  Database Management Systems and K/V-Stores operate on updatable datasets --
massively exceeding the size of available main memory. Tree-based K/V storage
management structures became particularly popular in storage engines. B+ Trees
allow constant search performance, however write-heavy workloads yield in
inefficient write patterns to secondary storage devices and poor performance
characteristics. LSM-Trees overcome this issue by horizontal partitioning
fractions of data - small enough to fully reside in main memory, but require
frequent maintenance to sustain search performance.
  Firstly, we propose Multi-Version Partitioned BTrees (MV-PBT) as sole storage
and index management structure in key-sorted storage engines like K/V-Stores.
Secondly, we compare MV-PBT against LSM-Trees. The logical horizontal
partitioning in MV-PBT allows leveraging recent advances in modern B$^+$-Tree
techniques in a small transparent and memory resident portion of the structure.
Structural properties sustain steady read performance, yielding efficient write
patterns and reducing write amplification.
  We integrated MV-PBT in the WiredTiger KV storage engine. MV-PBT offers an up
to 2x increased steady throughput in comparison to LSM-Trees and several orders
of magnitude in comparison to B+ Trees in a YCSB workload.
",2.0
665,14,90976,1906.04165,text summarization model,Derek Miller,Leveraging BERT for Extractive Text Summarization on Lectures,"  In the last two decades, automatic extractive text summarization on lectures
has demonstrated to be a useful tool for collecting key phrases and sentences
that best represent the content. However, many current approaches utilize dated
approaches, producing sub-par outputs or requiring several hours of manual
tuning to produce meaningful results. Recently, new machine learning
architectures have provided mechanisms for extractive summarization through the
clustering of output embeddings from deep learning models. This paper reports
on the project called Lecture Summarization Service, a python based RESTful
service that utilizes the BERT model for text embeddings and KMeans clustering
to identify sentences closes to the centroid for summary selection. The purpose
of the service was to provide students a utility that could summarize lecture
content, based on their desired number of sentences. On top of the summary
work, the service also includes lecture and summary management, storing content
on the cloud which can be used for collaboration. While the results of
utilizing BERT for extractive summarization were promising, there were still
areas where the model struggled, providing feature research opportunities for
further improvement.
",5.0
666,2,104193,1911.10829,random forests,Christoph Reinders and Bodo Rosenhahn,Neural Random Forest Imitation,"  We present Neural Random Forest Imitation - a novel approach for transforming
random forests into neural networks. Existing methods produce very inefficient
architectures and do not scale. In this paper, we introduce a new method for
generating data from a random forest and learning a neural network that
imitates it. Without any additional training data, this transformation creates
very efficient neural networks that learn the decision boundaries of a random
forest. The generated model is fully differentiable and can be combined with
the feature extraction in a single pipeline enabling further end-to-end
processing. Experiments on several real-world benchmark datasets demonstrate
outstanding performance in terms of scalability, accuracy, and learning with
very few training examples. Compared to state-of-the-art mappings, we
significantly reduce the network size while achieving the same or even improved
accuracy due to better generalization.
",3.0
667,0,127932,2008.0305,learning to rank with partitioned preference,"Esra Erdem, Muge Fidan, David Manlove, Patrick Prosser","A General Framework for Stable Roommates Problems using Answer Set
  Programming","  The Stable Roommates problem (SR) is characterized by the preferences of
agents over other agents as roommates: each agent ranks all others in strict
order of preference. A solution to SR is then a partition of the agents into
pairs so that each pair shares a room, and there is no pair of agents that
would block this matching (i.e., who prefers the other to their roommate in the
matching). There are interesting variations of SR that are motivated by
applications (e.g., the preference lists may be incomplete (SRI) and involve
ties (SRTI)), and that try to find a more fair solution (e.g., Egalitarian SR).
Unlike the Stable Marriage problem, every SR instance is not guaranteed to have
a solution. For that reason, there are also variations of SR that try to find a
good-enough solution (e.g., Almost SR). Most of these variations are NP-hard.
We introduce a formal framework, called SRTI-ASP, utilizing the logic
programming paradigm Answer Set Programming, that is provable and general
enough to solve many of such variations of SR. Our empirical analysis shows
that SRTI-ASP is also promising for applications. This paper is under
consideration for acceptance in TPLP.
",0.0
668,0,89820,1905.12966,learning to rank with partitioned preference,"Zhengui Xue, Zhiwei Lin, Hui Wang and Sally McClean",Quantifying consensus of rankings based on q-support patterns,"  Rankings, representing preferences over a set of candidates, are widely used
in many information systems, e.g., group decision making and information
retrieval. It is of great importance to evaluate the consensus of the obtained
rankings from multiple agents. An overall measure of the consensus degree
provides an insight into the ranking data. Moreover, it could provide a
quantitative indicator for consensus comparison between groups and further
improvement of a ranking system. Existing studies are insufficient in assessing
the overall consensus of a ranking set. They did not provide an evaluation of
the consensus degree of preference patterns in most rankings. In this paper, a
novel consensus quantifying approach, without the need for any correlation or
distance functions as in existing studies of consensus, is proposed based on a
concept of q-support patterns of rankings. The q-support patterns represent the
commonality embedded in a set of rankings. A method for detecting outliers in a
set of rankings is naturally derived from the proposed consensus quantifying
approach. Experimental studies are conducted to demonstrate the effectiveness
of the proposed approach.
",1.0
669,9,13041,1205.2657,language model for long documents,"Jordan Boyd-Graber, David Blei",Multilingual Topic Models for Unaligned Text,"  We develop the multilingual topic model for unaligned text (MuTo), a
probabilistic model of text that is designed to analyze corpora composed of
documents in two languages. From these documents, MuTo uses stochastic EM to
simultaneously discover both a matching between the languages and multilingual
latent topics. We demonstrate that MuTo is able to find shared topics on
real-world multilingual corpora, successfully pairing related documents across
languages. MuTo provides a new framework for creating multilingual topic models
without needing carefully curated parallel corpora and allows applications
built using the topic model formalism to be applied to a much wider class of
corpora.
",1.0
670,17,88544,1905.07387,robustness of neutral networks,"Ching-Yun Ko, Zhaoyang Lyu, Tsui-Wei Weng, Luca Daniel, Ngai Wong,
  Dahua Lin",POPQORN: Quantifying Robustness of Recurrent Neural Networks,"  The vulnerability to adversarial attacks has been a critical issue for deep
neural networks. Addressing this issue requires a reliable way to evaluate the
robustness of a network. Recently, several methods have been developed to
compute $\textit{robustness quantification}$ for neural networks, namely,
certified lower bounds of the minimum adversarial perturbation. Such methods,
however, were devised for feed-forward networks, e.g. multi-layer perceptron or
convolutional networks. It remains an open problem to quantify robustness for
recurrent networks, especially LSTM and GRU. For such networks, there exist
additional challenges in computing the robustness quantification, such as
handling the inputs at multiple steps and the interaction between gates and
states. In this work, we propose $\textit{POPQORN}$
($\textbf{P}$ropagated-$\textbf{o}$ut$\textbf{p}$ut $\textbf{Q}$uantified
R$\textbf{o}$bustness for $\textbf{RN}$Ns), a general algorithm to quantify
robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its
effectiveness on different network architectures and show that the robustness
quantification on individual steps can lead to new insights.
",4.0
671,11,21021,1307.1179,PageRank for web search,"Andrew Trotman, Jinglan Zhang",Future Web Growth and its Consequences for Web Search Architectures,"  Introduction: Before embarking on the design of any computer system it is
first necessary to assess the magnitude of the problem. In the case of a web
search engine this assessment amounts to determining the current size of the
web, the growth rate of the web, and the quantity of computing resource
necessary to search it, and projecting the historical growth of this into the
future. Method: The over 20 year history of the web makes it possible to make
short-term projections on future growth. The longer history of hard disk drives
(and smart phone memory card) makes it possible to make short-term hardware
projections. Analysis: Historical data on Internet uptake and hardware growth
is extrapolated. Results: It is predicted that within a decade the storage
capacity of a single hard drive will exceed the size of the index of the web at
that time. Within another decade it will be possible to store the entire
searchable text on the same hard drive. Within another decade the entire
searchable web (including images) will also fit. Conclusion: This result raises
questions about the future architecture of search engines. Several new models
are proposed. In one model the user's computer is an active part of the
distributed search architecture. They search a pre-loaded snapshot (back-file)
of the web on their local device which frees up the online data centre for
searching just the difference between the snapshot and the current time.
Advantageously this also makes it possible to search when the user is
disconnected from the Internet. In another model all changes to all files are
broadcast to all users (forming a star-like network) and no data centre is
needed.
",1.0
672,5,94914,1908.00963,matrix completion,Shantanu Prasad Burnwal and Mathukumalli Vidyasagar,"Deterministic Completion of Rectangular Matrices Using Asymmetric
  Ramanujan Graphs: Exact and Stable Recovery","  In this paper we study the matrix completion problem: Suppose $X \in {\mathbb
R}^{n_r \times n_c}$ is unknown except for a known upper bound $r$ on its rank.
By measuring a small number $m \ll n_r n_c$ of elements of $X$, is it possible
to recover $X$ exactly with noise-free measurements, or to construct a good
approximation of $X$ with noisy measurements? Existing solutions to these
problems involve sampling the elements uniformly and at random, and can
guarantee exact recovery of the unknown matrix only with high probability. In
this paper, we present a \textit{deterministic} sampling method for matrix
completion. We achieve this by choosing the sampling set as the edge set of an
asymmetric Ramanujan bigraph, and constrained nuclear norm minimization is the
recovery method. Specifically, we derive sufficient conditions under which the
unknown matrix is completed exactly with noise-free measurements, and is
approximately completed with noisy measurements, which we call ""stable""
completion.
  The conditions derived here are only sufficient and more restrictive than
random sampling. To study how close they are to being necessary, we conducted
numerical simulations on randomly generated low rank matrices, using the LPS
families of Ramanujan graphs. These simulations demonstrate two facts: (i) In
order to achieve exact completion, it appears sufficient to choose the degree
$d$ of the Ramanujan graph to be $\geq 3r$. (ii) There is a ""phase transition,""
whereby the likelihood of success suddenly drops from 100\% to 0\% if the rank
is increased by just one or two beyond a critical value. The phase transition
phenomenon is well-known and well-studied in vector recovery using
$\ell_1$-norm minimization. However, it is less studied in matrix completion
and nuclear norm minimization, and not much understood.
",5.0
673,11,17044,1301.1626,PageRank for web search,Vivek Kandiah and Dima L. Shepelyansky,Google matrix analysis of DNA sequences,"  For DNA sequences of various species we construct the Google matrix G of
Markov transitions between nearby words composed of several letters. The
statistical distribution of matrix elements of this matrix is shown to be
described by a power law with the exponent being close to those of outgoing
links in such scale-free networks as the World Wide Web (WWW). At the same time
the sum of ingoing matrix elements is characterized by the exponent being
significantly larger than those typical for WWW networks. This results in a
slow algebraic decay of the PageRank probability determined by the distribution
of ingoing elements. The spectrum of G is characterized by a large gap leading
to a rapid relaxation process on the DNA sequence networks. We introduce the
PageRank proximity correlator between different species which determines their
statistical similarity from the view point of Markov chains. The properties of
other eigenstates of the Google matrix are also discussed. Our results
establish scale-free features of DNA sequence networks showing their
similarities and distinctions with the WWW and linguistic networks.
",2.0
674,4,20197,1305.2755,pre-trained language model,Issam Sahmoudi and Abdelmonaime Lachkar,Clustering Web Search Results For Effective Arabic Language Browsing,"  The process of browsing Search Results is one of the major problems with
traditional Web search engines for English, European, and any other languages
generally, and for Arabic Language particularly. This process is absolutely
time consuming and the browsing style seems to be unattractive. Organizing Web
search results into clusters facilitates users quick browsing through search
results. Traditional clustering techniques (data-centric clustering algorithms)
are inadequate since they don't generate clusters with highly readable names or
cluster labels. To solve this problem, Description-centric algorithms such as
Suffix Tree Clustering (STC) algorithm have been introduced and used
successfully and extensively with different adapted versions for English,
European, and Chinese Languages. However, till the day of writing this paper,
in our knowledge, STC algorithm has been never applied for Arabic Web Snippets
Search Results Clustering.In this paper, we propose first, to study how STC can
be applied for Arabic Language? We then illustrate by example that is
impossible to apply STC after Arabic Snippets pre-processing (stem or root
extraction) because the Merging process yields many redundant clusters.
Secondly, to overcome this problem, we propose to integrate STC in a new scheme
taking into a count the Arabic language properties in order to get the web more
and more adapted to Arabic users. The proposed approach automatically clusters
the web search results into high quality, and high significant clusters labels.
The obtained clusters not only are coherent, but also can convey the contents
to the users concisely and accurately. Therefore the Arabic users can decide at
a glance whether the contents of a cluster are of interest.....
",3.0
675,19,143581,2101.04414,artificial intelligence for low carbon,"Emmanuel Raj, Magnus Westerlund, Leonardo Espinosa-Leal",Reliable Fleet Analytics for Edge IoT Solutions,"  In recent years we have witnessed a boom in Internet of Things (IoT) device
deployments, which has resulted in big data and demand for low-latency
communication. This shift in the demand for infrastructure is also enabling
real-time decision making using artificial intelligence for IoT applications.
Artificial Intelligence of Things (AIoT) is the combination of Artificial
Intelligence (AI) technologies and the IoT infrastructure to provide robust and
efficient operations and decision making. Edge computing is emerging to enable
AIoT applications. Edge computing enables generating insights and making
decisions at or near the data source, reducing the amount of data sent to the
cloud or a central repository. In this paper, we propose a framework for
facilitating machine learning at the edge for AIoT applications, to enable
continuous delivery, deployment, and monitoring of machine learning models at
the edge (Edge MLOps). The contribution is an architecture that includes
services, tools, and methods for delivering fleet analytics at scale. We
present a preliminary validation of the framework by performing experiments
with IoT devices on a university campus's rooms. For the machine learning
experiments, we forecast multivariate time series for predicting air quality in
the respective rooms by using the models deployed in respective edge devices.
By these experiments, we validate the proposed fleet analytics framework for
efficiency and robustness.
",2.0
676,7,178798,2112.02589,gradient boosting,Hanyuan Hang,"Local Adaptivity of Gradient Boosting in Histogram Transform Ensemble
  Learning","  In this paper, we propose a gradient boosting algorithm called
\textit{adaptive boosting histogram transform} (\textit{ABHT}) for regression
to illustrate the local adaptivity of gradient boosting algorithms in histogram
transform ensemble learning. From the theoretical perspective, when the target
function lies in a locally H\""older continuous space, we show that our ABHT can
filter out the regions with different orders of smoothness. Consequently, we
are able to prove that the upper bound of the convergence rates of ABHT is
strictly smaller than the lower bound of \textit{parallel ensemble histogram
transform} (\textit{PEHT}). In the experiments, both synthetic and real-world
data experiments empirically validate the theoretical results, which
demonstrates the advantageous performance and local adaptivity of our ABHT.
",5.0
677,6,8302,1104.3213,query expansion for imformation retrieval,"Ziyang Liu (Arizona State University), Sivaramakrishnan Natarajan
  (Arizona State University), Yi Chen (ASU)",Query Expansion Based on Clustered Results,"  Query expansion is a functionality of search engines that suggests a set of
related queries for a user-issued keyword query. Typical corpus-driven keyword
query expansion approaches return popular words in the results as expanded
queries. Using these approaches, the expanded queries may correspond to a
subset of possible query semantics, and thus miss relevant results. To handle
ambiguous queries and exploratory queries, whose result relevance is difficult
to judge, we propose a new framework for keyword query expansion: we start with
clustering the results according to user specified granularity, and then
generate expanded queries, such that one expanded query is generated for each
cluster whose result set should ideally be the corresponding cluster. We
formalize this problem and show its APX-hardness. Then we propose two efficient
algorithms named iterative single-keyword refinement and partial elimination
based convergence, respectively, which effectively generate a set of expanded
queries from clustered results that provide a classification of the original
query results. We believe our study of generating an optimal query based on the
ground truth of the query results not only has applications in query expansion,
but has significance for studying keyword search quality in general.
",5.0
678,9,202374,2207.01262,language model for long documents,"Leonid Boytsov, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang,
  Eric Nyberg","Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding","  We carry out a comprehensive evaluation of 13 recent models for ranking of
long documents using two popular collections (MS MARCO documents and Robust04).
Our model zoo includes two specialized Transformer models (such as Longformer)
that can process long documents without the need to split them. Along the way,
we document several difficulties regarding training and comparing such models.
Somewhat surprisingly, we find the simple FirstP baseline (truncating documents
to satisfy the input-sequence constraint of a typical Transformer model) to be
quite effective. We analyze the distribution of relevant passages (inside
documents) to explain this phenomenon. We further argue that, despite their
widespread use, Robust04 and MS MARCO documents are not particularly useful for
benchmarking of long-document models.
",5.0
679,14,64110,1802.09426,text summarization model,"Mayank Chaudhari, Aakash Nelson Mattukoyya",Tone Biased MMR Text Summarization,"  Text summarization is an interesting area for researchers to develop new
techniques to provide human like summaries for vast amounts of information.
Summarization techniques tend to focus on providing accurate representation of
content, and often the tone of the content is ignored. Tone of the content sets
a baseline for how a reader perceives the content. As such being able to
generate summary with tone that is appropriate for the reader is important. In
our work we implement Maximal Marginal Relevance [MMR] based multi-document
text summarization and propose a naive model to change tone of the
summarization by setting a bias to specific set of words and restricting other
words in the summarization output. This bias towards a specified set of words
produces a summary whose tone is same as tone of specified words.
",5.0
680,8,103939,1911.09454,node embedding for graph,"Bitan Hou, Yujing Wang, Ming Zeng, Shan Jiang, Ole J. Mengshoel,
  Yunhai Tong, Jing Bai","Customized Graph Embedding: Tailoring Embedding Vectors to different
  Applications","  Graph is a natural representation of data for a variety of real-word
applications, such as knowledge graph mining, social network analysis and
biological network comparison. For these applications, graph embedding is
crucial as it provides vector representations of the graph. One limitation of
existing graph embedding methods is that their embedding optimization
procedures are disconnected from the target application. In this paper, we
propose a novel approach, namely Customized Graph Embedding (CGE) to tackle
this problem. The CGE algorithm learns customized vector representations of
graph nodes by differentiating the importance of distinct graph paths
automatically for a specific application. Extensive experiments were carried
out on a diverse set of node classification datasets, which demonstrate strong
performances of CGE and provide deep insights into the model.
",5.0
681,5,55756,1707.06055,matrix completion,"Guilherme Ramos, Joao Saude, Carlos Caleiro, Soummya Kar",Recommendation via matrix completion using Kolmogorov complexity,"  A usual way to model a recommendation system is as a matrix completion
problem. There are several matrix completion methods, typically using
optimization approaches or collaborative filtering. Most approaches assume that
the matrix is either low rank, or that there are a small number of latent
variables that encode the full problem. Here, we propose a novel matrix
completion algorithm for recommendation systems, without any assumptions on the
rank and that is model free, i.e., the entries are not assumed to be a function
of some latent variables. Instead, we use a technique akin to information
theory. Our method performs hybrid neighborhood-based collaborative filtering
using Kolmogorov complexity. It decouples the matrix completion into a vector
completion problem for each user. The recommendation for one user is thus
independent of the recommendation for other users. This makes the algorithm
scalable because the computations are highly parallelizable. Our results are
competitive with state-of-the-art approaches on both synthetic and real-world
dataset benchmarks.
",4.0
682,12,53738,1705.08598,COVID-19 and social media,"Omid Aghili, Mark Sanderson","Journalists' information needs, seeking behavior, and its determinants
  on social media","  We describe the results of a qualitative study on journalists' information
seeking behavior on social media. Based on interviews with eleven journalists
along with a study of a set of university level journalism modules, we
determined the categories of information need types that lead journalists to
social media. We also determined the ways that social media is exploited as a
tool to satisfy information needs and to define influential factors, which
impacted on journalists' information seeking behavior. We find that not only is
social media used as an information source, but it can also be a supplier of
stories found serendipitously. We find seven information need types that expand
the types found in previous work. We also find five categories of influential
factors that affect the way journalists seek information.
",1.0
683,12,32319,1503.03752,COVID-19 and social media,Emilio Ferrara,Manipulation and abuse on social media,"  The computer science research community has became increasingly interested in
the study of social media due to their pervasiveness in the everyday life of
millions of individuals. Methodological questions and technical challenges
abound as more and more data from social platforms become available for
analysis. This data deluge not only yields the unprecedented opportunity to
unravel questions about online individuals' behavior at scale, but also allows
to explore the potential perils that the massive adoption of social media
brings to our society. These communication channels provide plenty of
incentives (both economical and social) and opportunities for abuse. As social
media activity became increasingly intertwined with the events in the offline
world, individuals and organizations have found ways to exploit these platforms
to spread misinformation, to attack and smear others, or to deceive and
manipulate. During crises, social media have been effectively used for
emergency response, but fear-mongering actions have also triggered mass
hysteria and panic. Criminal gangs and terrorist organizations like ISIS adopt
social media for propaganda and recruitment. Synthetic activity and social bots
have been used to coordinate orchestrated astroturf campaigns, to manipulate
political elections and the stock market. The lack of effective content
verification systems on many of these platforms, including Twitter and
Facebook, rises concerns when younger users become exposed to cyber-bulling,
harassment, or hate speech, inducing risks like depression and suicide. This
article illustrates some of the recent advances facing these issues and
discusses what it remains to be done, including the challenges to address in
the future to make social media a more useful and accessible, safer and
healthier environment for all users.
",1.0
684,18,156740,2105.08713,infomation retrieval time complexity,Karim Banawan and Ahmed Arafa and Sennur Ulukus,Timely Private Information Retrieval,"  We introduce the problem of \emph{timely} private information retrieval (PIR)
from $N$ non-colluding and replicated servers. In this problem, a user desires
to retrieve a message out of $M$ messages from the servers, whose contents are
continuously updating. The retrieval process should be executed in a timely
manner such that no information is leaked about the identity of the message. To
assess the timeliness, we use the \emph{age of information} (AoI) metric.
Interestingly, the timely PIR problem reduces to an AoI minimization subject to
PIR constraints under \emph{asymmetric traffic}. We explicitly characterize the
optimal tradeoff between the PIR rate and the AoI metric (peak AoI or average
AoI) for the case of $N=2$, $M=3$. Further, we provide some structural insights
on the general problem with arbitrary $N$, $M$.
",1.0
685,10,69582,1806.06878,web archive,"Shawn M. Jones, Alexander Nwala, Michele C. Weigle, Michael L. Nelson",The Many Shapes of Archive-It,"  Web archives, a key area of digital preservation, meet the needs of
journalists, social scientists, historians, and government organizations. The
use cases for these groups often require that they guide the archiving process
themselves, selecting their own original resources, or seeds, and creating
their own web archive collections. We focus on the collections within
Archive-It, a subscription service started by the Internet Archive in 2005 for
the purpose of allowing organizations to create their own collections of
archived web pages, or mementos. Understanding these collections could be done
via their user-supplied metadata or via text analysis, but the metadata is
applied inconsistently between collections and some Archive-It collections
consist of hundreds of thousands of seeds, making it costly in terms of time to
download each memento. Our work proposes using structural metadata as an
additional way to understand these collections. We explore structural features
currently existing in these collections that can unveil curation and crawling
behaviors. We adapt the concept of the collection growth curve for
understanding Archive-It collection curation and crawling behavior. We also
introduce several seed features and come to an understanding of the diversity
of resources that make up a collection. Finally, we use the descriptions of
each collection to identify four semantic categories of Archive-It collections.
Using the identified structural features, we reviewed the results of runs with
20 classifiers and are able to predict the semantic category of a collection
using a Random Forest classifier with a weighted average F1 score of 0.720,
thus bridging the structural to the descriptive. Our method is useful because
it saves the researcher time and bandwidth. Identifying collections by their
semantic category allows further downstream processing to be tailored to these
categories.
",4.0
686,12,25691,1403.6838,COVID-19 and social media,"Manuel Gomez Rodriguez, Krishna Gummadi and Bernhard Schoelkopf","Quantifying Information Overload in Social Media and its Impact on
  Social Contagions","  Information overload has become an ubiquitous problem in modern society.
Social media users and microbloggers receive an endless flow of information,
often at a rate far higher than their cognitive abilities to process the
information. In this paper, we conduct a large scale quantitative study of
information overload and evaluate its impact on information dissemination in
the Twitter social media site. We model social media users as information
processing systems that queue incoming information according to some policies,
process information from the queue at some unknown rates and decide to forward
some of the incoming information to other users. We show how timestamped data
about tweets received and forwarded by users can be used to uncover key
properties of their queueing policies and estimate their information processing
rates and limits. Such an understanding of users' information processing
behaviors allows us to infer whether and to what extent users suffer from
information overload.
  Our analysis provides empirical evidence of information processing limits for
social media users and the prevalence of information overloading. The most
active and popular social media users are often the ones that are overloaded.
Moreover, we find that the rate at which users receive information impacts
their processing behavior, including how they prioritize information from
different sources, how much information they process, and how quickly they
process information. Finally, the susceptibility of a social media user to
social contagions depends crucially on the rate at which she receives
information. An exposure to a piece of information, be it an idea, a convention
or a product, is much less effective for users that receive information at
higher rates, meaning they need more exposures to adopt a particular contagion.
",1.0
687,2,69775,1806.08079,random forests,"Manqing Dong, Lina Yao, Xianzhi Wang, Boualem Benatallah and Shuai
  Zhang","GrCAN: Gradient Boost Convolutional Autoencoder with Neural Decision
  Forest","  Random forest and deep neural network are two schools of effective
classification methods in machine learning. While the random forest is robust
irrespective of the data domain, the deep neural network has advantages in
handling high dimensional data. In view that a differentiable neural decision
forest can be added to the neural network to fully exploit the benefits of both
models, in our work, we further combine convolutional autoencoder with neural
decision forest, where autoencoder has its advantages in finding the hidden
representations of the input data. We develop a gradient boost module and embed
it into the proposed convolutional autoencoder with neural decision forest to
improve the performance. The idea of gradient boost is to learn and use the
residual in the prediction. In addition, we design a structure to learn the
parameters of the neural decision forest and gradient boost module at
contiguous steps. The extensive experiments on several public datasets
demonstrate that our proposed model achieves good efficiency and prediction
performance compared with a series of baseline methods.
",3.0
688,6,149615,2103.05256,query expansion for imformation retrieval,"Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, James Allan",CEQE: Contextualized Embeddings for Query Expansion,"  In this work we leverage recent advances in context-sensitive language models
to improve the task of query expansion. Contextualized word representation
models, such as ELMo and BERT, are rapidly replacing static embedding models.
We propose a new model, Contextualized Embeddings for Query Expansion (CEQE),
that utilizes query-focused contextualized embedding vectors. We study the
behavior of contextual representations generated for query expansion in ad-hoc
document retrieval. We conduct our experiments on probabilistic retrieval
models as well as in combination with neural ranking models. We evaluate CEQE
on two standard TREC collections: Robust and Deep Learning. We find that CEQE
outperforms static embedding-based expansion methods on multiple collections
(by up to 18% on Robust and 31% on Deep Learning on average precision) and also
improves over proven probabilistic pseudo-relevance feedback (PRF) models. We
further find that multiple passes of expansion and reranking result in
continued gains in effectiveness with CEQE-based approaches outperforming other
approaches. The final model incorporating neural and CEQE-based expansion score
achieves gains of up to 5% in P@20 and 2% in AP on Robust over the
state-of-the-art transformer-based re-ranking model, Birch.
",3.0
689,11,106257,1912.09519,PageRank for web search,"Nikitha Rao, Chetan Bansal, Thomas Zimmermann, Ahmed Hassan Awadallah,
  Nachiappan Nagappan",Analyzing Web Search Behavior for Software Engineering Tasks,"  Web search plays an integral role in software engineering (SE) to help with
various tasks such as finding documentation, debugging, installation, etc. In
this work, we present the first large-scale analysis of web search behavior for
SE tasks using the search query logs from Bing, a commercial web search engine.
First, we use distant supervision techniques to build a machine learning
classifier to extract the SE search queries with an F1 score of 93%. We then
perform an analysis on one million search sessions to understand how software
engineering related queries and sessions differ from other queries and
sessions. Subsequently, we propose a taxonomy of intents to identify the
various contexts in which web search is used in software engineering. Lastly,
we analyze millions of SE queries to understand the distribution, search
metrics and trends across these SE search intents. Our analysis shows that SE
related queries form a significant portion of the overall web search traffic.
Additionally, we found that there are six major intent categories for which web
search is used in software engineering. The techniques and insights can not
only help improve existing tools but can also inspire the development of new
tools that aid in finding information for SE related tasks.
",1.0
690,13,125901,2007.08024,social network analysis with natrual language processing,"Giovanni Da San Martino, Stefano Cresci, Alberto Barron-Cedeno,
  Seunghak Yu, Roberto Di Pietro, Preslav Nakov",A Survey on Computational Propaganda Detection,"  Propaganda campaigns aim at influencing people's mindset with the purpose of
advancing a specific agenda. They exploit the anonymity of the Internet, the
micro-profiling ability of social networks, and the ease of automatically
creating and managing coordinated networks of accounts, to reach millions of
social network users with persuasive messages, specifically targeted to topics
each individual user is sensitive to, and ultimately influencing the outcome on
a targeted issue. In this survey, we review the state of the art on
computational propaganda detection from the perspective of Natural Language
Processing and Network Analysis, arguing about the need for combined efforts
between these communities. We further discuss current challenges and future
research directions.
",3.0
691,18,76950,1811.0339,infomation retrieval time complexity,"Frank Soboczenski, Michael D. Himes, Molly D. O'Beirne, Simone Zorzan,
  Atilim Gunes Baydin, Adam D. Cobb, Yarin Gal, Daniel Angerhausen, Massimo
  Mascaro, Giada N. Arney, Shawn D. Domagal-Goldman",Bayesian Deep Learning for Exoplanet Atmospheric Retrieval,"  Over the past decade, the study of extrasolar planets has evolved rapidly
from plain detection and identification to comprehensive categorization and
characterization of exoplanet systems and their atmospheres. Atmospheric
retrieval, the inverse modeling technique used to determine an exoplanetary
atmosphere's temperature structure and composition from an observed spectrum,
is both time-consuming and compute-intensive, requiring complex algorithms that
compare thousands to millions of atmospheric models to the observational data
to find the most probable values and associated uncertainties for each model
parameter. For rocky, terrestrial planets, the retrieved atmospheric
composition can give insight into the surface fluxes of gaseous species
necessary to maintain the stability of that atmosphere, which may in turn
provide insight into the geological and/or biological processes active on the
planet. These atmospheres contain many molecules, some of them biosignatures,
spectral fingerprints indicative of biological activity, which will become
observable with the next generation of telescopes. Runtimes of traditional
retrieval models scale with the number of model parameters, so as more
molecular species are considered, runtimes can become prohibitively long.
Recent advances in machine learning (ML) and computer vision offer new ways to
reduce the time to perform a retrieval by orders of magnitude, given a
sufficient data set to train with. Here we present an ML-based retrieval
framework called Intelligent exoplaNet Atmospheric RetrievAl (INARA) that
consists of a Bayesian deep learning model for retrieval and a data set of
3,000,000 synthetic rocky exoplanetary spectra generated using the NASA
Planetary Spectrum Generator. Our work represents the first ML retrieval model
for rocky, terrestrial exoplanets and the first synthetic data set of
terrestrial spectra generated at this scale.
",1.0
692,3,92540,1907.0005,database management system,"Bernd Amann, Youry Khmelevsky and Gaetan Hains",State-of-the-Art on Query & Transaction Processing Acceleration,"  The vast amount of processing power and memory bandwidth provided by modern
Graphics Processing Units (GPUs) make them a platform for data-intensive
applications. The database community identified GPUs as effective co-processors
for data processing. In the past years, there were many approaches to make use
of GPUs at different levels of a database system. In this Internal Technical
Report, based on the [1] and some other research papers, we identify possible
research areas at LIP6 for GPU-accelerated database management systems. We
describe some key properties, typical challenges of GPU-aware database
architectures, and identify major open challenges.
",4.0
693,8,175112,2111.00604,node embedding for graph,"Lu Lin, Ethan Blaser and Hongning Wang",Graph Embedding with Hierarchical Attentive Membership,"  The exploitation of graph structures is the key to effectively learning
representations of nodes that preserve useful information in graphs. A
remarkable property of graph is that a latent hierarchical grouping of nodes
exists in a global perspective, where each node manifests its membership to a
specific group based on the context composed by its neighboring nodes. Most
prior works ignore such latent groups and nodes' membership to different
groups, not to mention the hierarchy, when modeling the neighborhood structure.
Thus, they fall short of delivering a comprehensive understanding of the nodes
under different contexts in a graph. In this paper, we propose a novel
hierarchical attentive membership model for graph embedding, where the latent
memberships for each node are dynamically discovered based on its neighboring
context. Both group-level and individual-level attentions are performed when
aggregating neighboring states to generate node embeddings. We introduce
structural constraints to explicitly regularize the inferred memberships of
each node, such that a well-defined hierarchical grouping structure is
captured. The proposed model outperformed a set of state-of-the-art graph
embedding solutions on node classification and link prediction tasks in a
variety of graphs including citation networks and social networks. Qualitative
evaluations visualize the learned node embeddings along with the inferred
memberships, which proved the concept of membership hierarchy and enables
explainable embedding learning in graphs.
",5.0
694,2,132619,2009.14572,random forests,Delilah Donick and Sandro Claudio Lera,"Uncovering Feature Interdependencies in High-Noise Environments with
  Stepwise Lookahead Decision Forests","  Conventionally, random forests are built from ""greedy"" decision trees which
each consider only one split at a time during their construction. The
sub-optimality of greedy implementation has been well-known, yet mainstream
adoption of more sophisticated tree building algorithms has been lacking. We
examine under what circumstances an implementation of less greedy decision
trees actually yields outperformance. To this end, a ""stepwise lookahead""
variation of the random forest algorithm is presented for its ability to better
uncover binary feature interdependencies. In contrast to the greedy approach,
the decision trees included in this random forest algorithm, each
simultaneously consider three split nodes in tiers of depth two. It is
demonstrated on synthetic data and financial price time series that the
lookahead version significantly outperforms the greedy one when (a) certain
non-linear relationships between feature-pairs are present and (b) if the
signal-to-noise ratio is particularly low. A long-short trading strategy for
copper futures is then backtested by training both greedy and stepwise
lookahead random forests to predict the signs of daily price returns. The
resulting superior performance of the lookahead algorithm is at least partially
explained by the presence of ""XOR-like"" relationships between long-term and
short-term technical indicators. More generally, across all examined datasets,
when no such relationships between features are present, performance across
random forests is similar. Given its enhanced ability to understand the
feature-interdependencies present in complex systems, this lookahead variation
is a useful extension to the toolkit of data scientists, in particular for
financial machine learning, where conditions (a) and (b) are typically met.
",3.0
695,7,172329,2110.05007,gradient boosting,"Xiaojun Jia, Yong Zhang, Baoyuan Wu, Jue Wang and Xiaochun Cao","Boosting Fast Adversarial Training with Learnable Adversarial
  Initialization","  Adversarial training (AT) has been demonstrated to be effective in improving
model robustness by leveraging adversarial examples for training. However, most
AT methods are in face of expensive time and computational cost for calculating
gradients at multiple steps in generating adversarial examples. To boost
training efficiency, fast gradient sign method (FGSM) is adopted in fast AT
methods by calculating gradient only once. Unfortunately, the robustness is far
from satisfactory. One reason may arise from the initialization fashion.
Existing fast AT generally uses a random sample-agnostic initialization, which
facilitates the efficiency yet hinders a further robustness improvement. Up to
now, the initialization in fast AT is still not extensively explored. In this
paper, we boost fast AT with a sample-dependent adversarial initialization,
i.e., an output from a generative network conditioned on a benign image and its
gradient information from the target network. As the generative network and the
target network are optimized jointly in the training phase, the former can
adaptively generate an effective initialization with respect to the latter,
which motivates gradually improved robustness. Experimental evaluations on four
benchmark databases demonstrate the superiority of our proposed method over
state-of-the-art fast AT methods, as well as comparable robustness to advanced
multi-step AT methods. The code is released at
https://github.com//jiaxiaojunQAQ//FGSM-SDI.
",1.0
696,8,158966,2106.03393,node embedding for graph,"Jiaren Xiao, Quanyu Dai, Xiaochen Xie, James Lam, Ka-Wai Kwok","Adversarially Regularized Graph Attention Networks for Inductive
  Learning on Partially Labeled Graphs","  Graph embedding is a general approach to tackling graph-analytic problems by
encoding nodes into low-dimensional representations. Most existing embedding
methods are transductive since the information of all nodes is required in
training, including those to be predicted. In this paper, we propose a novel
inductive embedding method for semi-supervised learning on graphs. This method
generates node representations by learning a parametric function to aggregate
information from the neighborhood using an attention mechanism, and hence
naturally generalizes to previously unseen nodes. Furthermore, adversarial
training serves as an external regularization enforcing the learned
representations to match a prior distribution for improving robustness and
generalization ability. Experiments on real-world clean or noisy graphs are
used to demonstrate the effectiveness of this approach.
",5.0
697,15,167308,2108.11044,relevance feedback for imformation retrieval,"Hang Li and Ahmed Mourad and Shengyao Zhuang and Bevan Koopman and
  Guido Zuccon","Pseudo Relevance Feedback with Deep Language Models and Dense
  Retrievers: Successes and Pitfalls","  Pseudo Relevance Feedback (PRF) is known to improve the effectiveness of
bag-of-words retrievers. At the same time, deep language models have been shown
to outperform traditional bag-of-words rerankers. However, it is unclear how to
integrate PRF directly with emergent deep language models. In this article, we
address this gap by investigating methods for integrating PRF signals into
rerankers and dense retrievers based on deep language models. We consider
text-based and vector-based PRF approaches, and investigate different ways of
combining and scoring relevance signals. An extensive empirical evaluation was
conducted across four different datasets and two task settings (retrieval and
ranking). Text-based PRF results show that the use of PRF had a mixed effect on
deep rerankers across different datasets. We found that the best effectiveness
was achieved when (i) directly concatenating each PRF passage with the query,
searching with the new set of queries, and then aggregating the scores; (ii)
using Borda to aggregate scores from PRF runs. Vector-based PRF results show
that the use of PRF enhanced the effectiveness of deep rerankers and dense
retrievers over several evaluation metrics. We found that higher effectiveness
was achieved when (i) the query retains either the majority or the same weight
within the PRF mechanism, and (ii) a shallower PRF signal (i.e., a smaller
number of top-ranked passages) was employed, rather than a deeper signal. Our
vector-based PRF method is computationally efficient; thus this represents a
general PRF method others can use with deep rerankers and dense retrievers.
",4.0
698,16,110693,2002.07224,activation function in neutral networks,"Garrett Bingham, William Macke, and Risto Miikkulainen",Evolutionary Optimization of Deep Learning Activation Functions,"  The choice of activation function can have a large effect on the performance
of a neural network. While there have been some attempts to hand-engineer novel
activation functions, the Rectified Linear Unit (ReLU) remains the most
commonly-used in practice. This paper shows that evolutionary algorithms can
discover novel activation functions that outperform ReLU. A tree-based search
space of candidate activation functions is defined and explored with mutation,
crossover, and exhaustive search. Experiments on training wide residual
networks on the CIFAR-10 and CIFAR-100 image datasets show that this approach
is effective. Replacing ReLU with evolved activation functions results in
statistically significant increases in network accuracy. Optimal performance is
achieved when evolution is allowed to customize activation functions to a
particular task; however, these novel activation functions are shown to
generalize, achieving high performance across tasks. Evolutionary optimization
of activation functions is therefore a promising new dimension of metalearning
in neural networks.
",4.0
699,4,195041,2205.03719,pre-trained language model,Laura Sisson,Odor Descriptor Understanding through Prompting,"  Embeddings from contemporary natural language processing (NLP) models are
commonly used as numerical representations for words or sentences. However,
odor descriptor words, like ""leather"" or ""fruity"", vary significantly between
their commonplace usage and their olfactory usage, as a result traditional
methods for generating these embeddings do not suffice. In this paper, we
present two methods to generate embeddings for odor words that are more closely
aligned with their olfactory meanings when compared to off-the-shelf
embeddings. These generated embeddings outperform the previous state-of-the-art
and contemporary fine-tuning/prompting methods on a pre-existing zero-shot
odor-specific NLP benchmark.
",4.0
700,9,165356,2108.00775,language model for long documents,"Paul Grundmann, Sebastian Arnold, Alexander L\""oser",Self-supervised Answer Retrieval on Clinical Notes,"  Retrieving answer passages from long documents is a complex task requiring
semantic understanding of both discourse and document context. We approach this
challenge specifically in a clinical scenario, where doctors retrieve cohorts
of patients based on diagnoses and other latent medical aspects. We introduce
CAPR, a rule-based self-supervision objective for training Transformer language
models for domain-specific passage matching. In addition, we contribute a novel
retrieval dataset based on clinical notes to simulate this scenario on a large
corpus of clinical notes. We apply our objective in four Transformer-based
architectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From
our extensive evaluation on MIMIC-III and three other healthcare datasets, we
report that CAPR outperforms strong baselines in the retrieval of
domain-specific passages and effectively generalizes across rule-based and
human-labeled passages. This makes the model powerful especially in zero-shot
scenarios where only limited training data is available.
",4.0
701,6,165484,2108.01441,query expansion for imformation retrieval,"Quanye Jia, Rui Liu and Jianying Lin","Using Query Expansion in Manifold Ranking for Query-Oriented
  Multi-Document Summarization","  Manifold ranking has been successfully applied in query-oriented
multi-document summarization. It not only makes use of the relationships among
the sentences, but also the relationships between the given query and the
sentences. However, the information of original query is often insufficient. So
we present a query expansion method, which is combined in the manifold ranking
to resolve this problem. Our method not only utilizes the information of the
query term itself and the knowledge base WordNet to expand it by synonyms, but
also uses the information of the document set itself to expand the query in
various ways (mean expansion, variance expansion and TextRank expansion).
Compared with the previous query expansion methods, our method combines
multiple query expansion methods to better represent query information, and at
the same time, it makes a useful attempt on manifold ranking. In addition, we
use the degree of word overlap and the proximity between words to calculate the
similarity between sentences. We performed experiments on the datasets of DUC
2006 and DUC2007, and the evaluation results show that the proposed query
expansion method can significantly improve the system performance and make our
system comparable to the state-of-the-art systems.
",5.0
702,4,86413,1904.08061,pre-trained language model,"Jia Li, Xiao Sun, Xing Wei, Changliang Li, Jianhua Tao","Reinforcement Learning Based Emotional Editing Constraint Conversation
  Generation","  In recent years, the generation of conversation content based on deep neural
networks has attracted many researchers. However, traditional neural language
models tend to generate general replies, lacking logical and emotional factors.
This paper proposes a conversation content generation model that combines
reinforcement learning with emotional editing constraints to generate more
meaningful and customizable emotional replies. The model divides the replies
into three clauses based on pre-generated keywords and uses the emotional
editor to further optimize the final reply. The model combines multi-task
learning with multiple indicator rewards to comprehensively optimize the
quality of replies. Experiments shows that our model can not only improve the
fluency of the replies, but also significantly enhance the logical relevance
and emotional relevance of the replies.
",5.0
703,5,30035,1411.6081,matrix completion,Cho-Jui Hsieh and Nagarajan Natarajan and Inderjit S. Dhillon,PU Learning for Matrix Completion,"  In this paper, we consider the matrix completion problem when the
observations are one-bit measurements of some underlying matrix M, and in
particular the observed samples consist only of ones and no zeros. This problem
is motivated by modern applications such as recommender systems and social
networks where only ""likes"" or ""friendships"" are observed. The problem of
learning from only positive and unlabeled examples, called PU
(positive-unlabeled) learning, has been studied in the context of binary
classification. We consider the PU matrix completion problem, where an
underlying real-valued matrix M is first quantized to generate one-bit
observations and then a subset of positive entries is revealed. Under the
assumption that M has bounded nuclear norm, we provide recovery guarantees for
two different observation models: 1) M parameterizes a distribution that
generates a binary matrix, 2) M is thresholded to obtain a binary matrix. For
the first case, we propose a ""shifted matrix completion"" method that recovers M
using only a subset of indices corresponding to ones, while for the second
case, we propose a ""biased matrix completion"" method that recovers the
(thresholded) binary matrix. Both methods yield strong error bounds --- if M is
n by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes
the fraction of ones observed. This implies a sample complexity of O(n\log n)
ones to achieve a small error, when M is dense and n is large. We extend our
methods and guarantees to the inductive matrix completion problem, where rows
and columns of M have associated features. We provide efficient and scalable
optimization procedures for both the methods and demonstrate the effectiveness
of the proposed methods for link prediction (on real-world networks consisting
of over 2 million nodes and 90 million links) and semi-supervised clustering
tasks.
",4.0
704,3,148511,2103.0017,database management system,"Rodrigo Laigner and Yongluan Zhou and Marcos Antonio Vaz Salles and
  Yijian Liu and Marcos Kalinowski","Data Management in Microservices: State of the Practice, Challenges, and
  Research Directions","  Microservices have become a popular architectural style for data-driven
applications, given their ability to functionally decompose an application into
small and autonomous services to achieve scalability, strong isolation, and
specialization of database systems to the workloads and data formats of each
service. Despite the accelerating industrial adoption of this architectural
style, an investigation of the state of the practice and challenges
practitioners face regarding data management in microservices is lacking. To
bridge this gap, we conducted a systematic literature review of representative
articles reporting the adoption of microservices, we analyzed a set of popular
open-source microservice applications, and we conducted an online survey to
cross-validate the findings of the previous steps with the perceptions and
experiences of over 120 experienced practitioners and researchers.
  Through this process, we were able to categorize the state of practice of
data management in microservices and observe several foundational challenges
that cannot be solved by software engineering practices alone, but rather
require system-level support to alleviate the burden imposed on practitioners.
We discuss the shortcomings of state-of-the-art database systems regarding
microservices and we conclude by devising a set of features for
microservice-oriented database systems.
",5.0
705,12,205219,2208.00479,COVID-19 and social media,"Immaculate Wanza, Irad Kamuti, David Gichohi, Kinyua Gikunda","The impact of Twitter on political influence on the choice of a running
  mate: Social Network Analysis and Semantic Analysis -- A Review","  In this new era of social media, social networks are becoming increasingly
important sources of user-generated content on the internet. These kinds of
information resources, which include a lot of people's feelings, opinions,
feedback, and reviews, are very useful for big businesses, markets, politics,
journalism, and many other fields. Politics is one of the most talked-about and
popular topics on social media networks right now. Many politicians use
micro-blogging services like Twitter because they have a large number of
followers and supporters on those networks. Politicians, political parties,
political organizations, and foundations use social media networks to
communicate with citizens ahead of time. Today, social media is used by
hundreds of thousands of political groups and politicians. On these social
media networks, every politician and political party has millions of followers,
and politicians find new and innovative ways to urge individuals to participate
in politics. Furthermore, social media assists politicians in various
decision-making processes by providing recommendations, such as developing
policies and strategies based on previous experiences, recommending and
selecting suitable candidates for a particular constituency, recommending a
suitable person for a particular position in the party, and launching a
political campaign based on citizen sentiments on various issues and
controversies, among other things. This research is a review on the use of
social network analysis (SNA) and semantic analysis (SA) on the Twitter
platform to study the supporters networks of political leaders because it can
help in decision-making when predicting their political futures.
",1.0
706,4,76712,1811.01833,pre-trained language model,"Guofu Li, Pengjia Zhu, and Zhiyi Chen","Accelerating System Log Processing by Semi-supervised Learning: A
  Technical Report","  There is an increasing need for more automated system-log analysis tools for
large scale online system in a timely manner. However, conventional way to
monitor and classify the log output based on keyword list does not scale well
for complex system in which codes contributed by a large group of developers,
with diverse ways of encoding the error messages, often with misleading pre-set
labels. In this paper, we propose that the design of a large scale online log
analysis should follow the ""Least Prior Knowledge Principle"", in which
unsupervised or semi-supervised solution with the minimal prior knowledge of
the log should be encoded directly. Thereby, we report our experience in
designing a two-stage machine learning based method, in which the system logs
are regarded as the output of a quasi-natural language, pre-filtered by a
perplexity score threshold, and then undergo a fine-grained classification
procedure. Tests on empirical data show that our method has obvious advantage
regarding to the processing speed and classification accuracy.
",5.0
707,16,118625,2005.06678,activation function in neutral networks,"Chi-Chun Zhou, Hai-Long Tu, Yue-Jie Hou, Zhen Ling, Yi Liu, and Jian
  Hua",Activation functions are not needed: the ratio net,"  A deep neural network for classification tasks is essentially consist of two
components: feature extractors and function approximators. They usually work as
an integrated whole, however, improvements on any components can promote the
performance of the whole algorithm. This paper focus on designing a new
function approximator. Conventionally, to build a function approximator, one
usually uses the method based on the nonlinear activation function or the
nonlinear kernel function and yields classical networks such as the
feed-forward neural network (MLP) and the radial basis function network (RBF).
In this paper, a new function approximator that is effective and efficient is
proposed. Instead of designing new activation functions or kernel functions,
the new proposed network uses the fractional form. For the sake of convenience,
we name the network the ratio net. We compare the effectiveness and efficiency
of the ratio net and that of the RBF and the MLP with various kinds of
activation functions in the classification task on the mnist database of
handwritten digits and the Internet Movie Database (IMDb) which is a binary
sentiment analysis dataset. It shows that, in most cases, the ratio net
converges faster and outperforms both the MLP and the RBF.
",2.0
708,5,32222,1503.02596,matrix completion,"Daniel L. Pimentel-Alarc\'on, Nigel Boston, Robert D. Nowak","A Characterization of Deterministic Sampling Patterns for Low-Rank
  Matrix Completion","  Low-rank matrix completion (LRMC) problems arise in a wide variety of
applications. Previous theory mainly provides conditions for completion under
missing-at-random samplings. This paper studies deterministic conditions for
completion. An incomplete $d \times N$ matrix is finitely rank-$r$ completable
if there are at most finitely many rank-$r$ matrices that agree with all its
observed entries. Finite completability is the tipping point in LRMC, as a few
additional samples of a finitely completable matrix guarantee its unique
completability. The main contribution of this paper is a deterministic sampling
condition for finite completability. We use this to also derive deterministic
sampling conditions for unique completability that can be efficiently verified.
We also show that under uniform random sampling schemes, these conditions are
satisfied with high probability if $O(\max\{r,\log d\})$ entries per column are
observed. These findings have several implications on LRMC regarding lower
bounds, sample and computational complexity, the role of coherence, adaptive
settings and the validation of any completion algorithm. We complement our
theoretical results with experiments that support our findings and motivate
future analysis of uncharted sampling regimes.
",5.0
709,3,21568,1308.144,database management system,"L\'aszl\'o Dobos and Alexander S. Szalay and Tam\'as Budav\'ari and
  Istv\'an Csabai and Nolan Li",Graywulf: A platform for federated scientific databases and services,"  Many fields of science rely on relational database management systems to
analyze, publish and share data. Since RDBMS are originally designed for, and
their development directions are primarily driven by, business use cases they
often lack features very important for scientific applications. Horizontal
scalability is probably the most important missing feature which makes it
challenging to adapt traditional relational database systems to the ever
growing data sizes. Due to the limited support of array data types and metadata
management, successful application of RDBMS in science usually requires the
development of custom extensions. While some of these extensions are specific
to the field of science, the majority of them could easily be generalized and
reused in other disciplines. With the Graywulf project we intend to target
several goals. We are building a generic platform that offers reusable
components for efficient storage, transformation, statistical analysis and
presentation of scientific data stored in Microsoft SQL Server. Graywulf also
addresses the distributed computational issues arising from current RDBMS
technologies. The current version supports load balancing of simple queries and
parallel execution of partitioned queries over a set of mirrored databases.
Uniform user access to the data is provided through a web based query interface
and a data surface for software clients. Queries are formulated in a slightly
modified syntax of SQL that offers a transparent view of the distributed data.
The software library consists of several components that can be reused to
develop complex scientific data warehouses: a system registry, administration
tools to manage entire database server clusters, a sophisticated workflow
execution framework, and a SQL parser library.
",5.0
710,4,101231,1910.1067,pre-trained language model,"Jun Liu, Jiedan Zhu, Vishal Kathuria, Fuchun Peng",Efficient Dynamic WFST Decoding for Personalized Language Models,"  We propose a two-layer cache mechanism to speed up dynamic WFST decoding with
personalized language models. The first layer is a public cache that stores
most of the static part of the graph. This is shared globally among all users.
A second layer is a private cache that caches the graph that represents the
personalized language model, which is only shared by the utterances from a
particular user. We also propose two simple yet effective pre-initialization
methods, one based on breadth-first search, and another based on a data-driven
exploration of decoder states using previous utterances. Experiments with a
calling speech recognition task using a personalized contact list demonstrate
that the proposed public cache reduces decoding time by factor of three
compared to decoding without pre-initialization. Using the private cache
provides additional efficiency gains, reducing the decoding time by a factor of
five.
",3.0
711,1,5004,1003.4418,advanced search engine,"Stefan Endrullis, Andreas Thor, Erhard Rahm",Evaluation of Query Generators for Entity Search Engines,"  Dynamic web applications such as mashups need efficient access to web data
that is only accessible via entity search engines (e.g. product or publication
search engines). However, most current mashup systems and applications only
support simple keyword searches for retrieving data from search engines. We
propose the use of more powerful search strategies building on so-called query
generators. For a given set of entities query generators are able to
automatically determine a set of search queries to retrieve these entities from
an entity search engine. We demonstrate the usefulness of query generators for
on-demand web data integration and evaluate the effectiveness and efficiency of
query generators for a challenging real-world integration scenario.
",5.0
712,15,3290,906.4026,relevance feedback for imformation retrieval,"B. Piwowarski, M. Lalmas","A Quantum-based Model for Interactive Information Retrieval (extended
  version)","  Even the best information retrieval model cannot always identify the most
useful answers to a user query. This is in particular the case with web search
systems, where it is known that users tend to minimise their effort to access
relevant information. It is, however, believed that the interaction between
users and a retrieval system, such as a web search engine, can be exploited to
provide better answers to users. Interactive Information Retrieval (IR)
systems, in which users access information through a series of interactions
with the search system, are concerned with building models for IR, where
interaction plays a central role. There are many possible interactions between
a user and a search system, ranging from query (re)formulation to relevance
feedback. However, capturing them within a single framework is difficult and
previously proposed approaches have mostly focused on relevance feedback. In
this paper, we propose a general framework for interactive IR that is able to
capture the full interaction process in a principled way. Our approach relies
upon a generalisation of the probability framework of quantum physics, whose
strong geometric component can be a key towards a successful interactive IR
model.
",2.0
713,4,193878,2204.12632,pre-trained language model,"Emmy Liu, Chen Cui, Kenneth Zheng, Graham Neubig",Testing the Ability of Language Models to Interpret Figurative Language,"  Figurative and metaphorical language are commonplace in discourse, and
figurative expressions play an important role in communication and cognition.
However, figurative language has been a relatively under-studied area in NLP,
and it remains an open question to what extent modern language models can
interpret nonliteral phrases. To address this question, we introduce Fig-QA, a
Winograd-style nonliteral language understanding task consisting of correctly
interpreting paired figurative phrases with divergent meanings. We evaluate the
performance of several state-of-the-art language models on this task, and find
that although language models achieve performance significantly over chance,
they still fall short of human performance, particularly in zero- or few-shot
settings. This suggests that further work is needed to improve the nonliteral
reasoning capabilities of language models.
",3.0
714,5,103454,1911.07255,matrix completion,"Amit Boyarski, Sanketh Vedula, Alex Bronstein",Spectral Geometric Matrix Completion,"  Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.
",4.0
715,5,4418,1001.1117,matrix completion,Bin Han and Xiaosheng Zhuang,Matrix Extension with Symmetry and Its Application to Filter Banks,"  In this paper, we completely solve the matrix extension problem with symmetry
and provide a step-by-step algorithm to construct such a desired matrix
$\mathsf{P}_e$ from a given matrix $\mathsf{P}$. Furthermore, using a cascade
structure, we obtain a complete representation of any $r\times s$ paraunitary
matrix $\mathsf{P}$ having compatible symmetry, which in turn leads to an
algorithm for deriving a desired matrix $\mathsf{P}_e$ from a given matrix
$\mathsf{P}$. Matrix extension plays an important role in many areas such as
electronic engineering, system sciences, applied mathematics, and pure
mathematics. As an application of our general results on matrix extension with
symmetry, we obtain a satisfactory algorithm for constructing symmetric
paraunitary filter banks and symmetric orthonormal multiwavelets by deriving
high-pass filters with symmetry from any given low-pass filters with symmetry.
Several examples are provided to illustrate the proposed algorithms and results
in this paper.
",1.0
716,14,163663,2107.06955,text summarization model,"Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu,
  Gargi Ghosh, Luke Zettlemoyer",HTLM: Hyper-Text Pre-Training and Prompting of Language Models,"  We introduce HTLM, a hyper-text language model trained on a large-scale web
crawl. Modeling hyper-text has a number of advantages: (1) it is easily
gathered at scale, (2) it provides rich document-level and end-task-adjacent
supervision (e.g. class and id attributes often encode document category
information), and (3) it allows for new structured prompting that follows the
established semantics of HTML (e.g. to do zero-shot summarization by infilling
title tags for a webpage that contains the input text). We show that
pretraining with a BART-style denoising loss directly on simplified HTML
provides highly effective transfer for a wide range of end tasks and
supervision levels. HTLM matches or exceeds the performance of comparably sized
text-only LMs for zero-shot prompting and fine-tuning for classification
benchmarks, while also setting new state-of-the-art performance levels for
zero-shot summarization. We also find that hyper-text prompts provide more
value to HTLM, in terms of data efficiency, than plain text prompts do for
existing LMs, and that HTLM is highly effective at auto-prompting itself, by
simply generating the most likely hyper-text formatting for any available
training data. We will release all code and models to support future HTLM
research.
",1.0
717,10,97438,1909.04404,web archive,"Martin Klein, Harihar Shankar, Lyudmila Balakireva, Herbert Van de
  Sompel","The Memento Tracer Framework: Balancing Quality and Scalability for Web
  Archiving","  Web archiving frameworks are commonly assessed by the quality of their
archival records and by their ability to operate at scale. The ubiquity of
dynamic web content poses a significant challenge for crawler-based solutions
such as the Internet Archive that are optimized for scale. Human driven
services such as the Webrecorder tool provide high-quality archival captures
but are not optimized to operate at scale. We introduce the Memento Tracer
framework that aims to balance archival quality and scalability. We outline its
concept and architecture and evaluate its archival quality and operation at
scale. Our findings indicate quality is on par or better compared against
established archiving frameworks and operation at scale comes with a manageable
overhead.
",4.0
718,0,136866,2011.01474,learning to rank with partitioned preference,"Jing Wang, Anna Choromanska",SGB: Stochastic Gradient Bound Method for Optimizing Partition Functions,"  This paper addresses the problem of optimizing partition functions in a
stochastic learning setting. We propose a stochastic variant of the bound
majorization algorithm that relies on upper-bounding the partition function
with a quadratic surrogate. The update of the proposed method, that we refer to
as Stochastic Partition Function Bound (SPFB), resembles scaled stochastic
gradient descent where the scaling factor relies on a second order term that is
however different from the Hessian. Similarly to quasi-Newton schemes, this
term is constructed using the stochastic approximation of the value of the
function and its gradient. We prove sub-linear convergence rate of the proposed
method and show the construction of its low-rank variant (LSPFB). Experiments
on logistic regression demonstrate that the proposed schemes significantly
outperform SGD. We also discuss how to use quadratic partition function bound
for efficient training of deep learning models and in non-convex optimization.
",0.0
719,2,203007,2207.04293,random forests,Lev V. Utkin and Andrei V. Konstantinov,Attention and Self-Attention in Random Forests,"  New models of random forests jointly using the attention and self-attention
mechanisms are proposed for solving the regression problem. The models can be
regarded as extensions of the attention-based random forest whose idea stems
from applying a combination of the Nadaraya-Watson kernel regression and the
Huber's contamination model to random forests. The self-attention aims to
capture dependencies of the tree predictions and to remove noise or anomalous
predictions in the random forest. The self-attention module is trained jointly
with the attention module for computing weights. It is shown that the training
process of attention weights is reduced to solving a single quadratic or linear
optimization problem. Three modifications of the general approach are proposed
and compared. A specific multi-head self-attention for the random forest is
also considered. Heads of the self-attention are obtained by changing its
tuning parameters including the kernel parameters and the contamination
parameter of models. Numerical experiments with various datasets illustrate the
proposed models and show that the supplement of the self-attention improves the
model performance for many datasets.
",3.0
720,9,96648,1908.1179,language model for long documents,"Dongyeop Kang, Hiroaki Hayashi, Alan W Black, Eduard Hovy","Linguistic Versus Latent Relations for Modeling Coherent Flow in
  Paragraphs","  Generating a long, coherent text such as a paragraph requires a high-level
control of different levels of relations between sentences (e.g., tense,
coreference). We call such a logical connection between sentences as a
(paragraph) flow. In order to produce a coherent flow of text, we explore two
forms of intersentential relations in a paragraph: one is a human-created
linguistical relation that forms a structure (e.g., discourse tree) and the
other is a relation from latent representation learned from the sentences
themselves. Our two proposed models incorporate each form of relations into
document-level language models: the former is a supervised model that jointly
learns a language model as well as discourse relation prediction, and the
latter is an unsupervised model that is hierarchically conditioned by a
recurrent neural network (RNN) over the latent information. Our proposed models
with both forms of relations outperform the baselines in partially conditioned
paragraph generation task. Our codes and data are publicly available.
",5.0
721,15,79091,1812.0491,relevance feedback for imformation retrieval,"Chang Li, Artem Grotov, Ilya Markov, and Maarten de Rijke",Online Learning to Rank with List-level Feedback for Image Filtering,"  Online learning to rank (OLTR) via implicit feedback has been extensively
studied for document retrieval in cases where the feedback is available at the
level of individual items. To learn from item-level feedback, the current
algorithms require certain assumptions about user behavior. In this paper, we
study a more general setup: OLTR with list-level feedback, where the feedback
is provided only at the level of an entire ranked list. We propose two methods
that allow online learning to rank in this setup. The first method, PGLearn,
uses a ranking model to generate policies and optimizes it online using policy
gradients. The second method, RegLearn, learns to combine individual document
relevance scores by directly predicting the observed list-level feedback
through regression. We evaluate the proposed methods on the image filtering
task, in which deep neural networks (DNNs) are used to rank images in response
to a set of standing queries. We show that PGLearn does not perform well in
OLTR with list-level feedback. RegLearn, instead, shows good performance in
both online and offline metrics.
",3.0
722,19,201014,2206.10953,artificial intelligence for low carbon,"Ruifeng Qian, Shijie Li, Mengjiao Bao, Huan Chen, Yu Che","Toward An Optimal Selection of Dialogue Strategies: A Target-Driven
  Approach for Intelligent Outbound Robots","  With the growth of the economy and society, enterprises, especially in the
FinTech industry, have increasing demands of outbound calls for customers such
as debt collection, marketing, anti-fraud calls, and so on. But a large amount
of repetitive and mechanical work occupies most of the time of human agents, so
the cost of equipment and labor for enterprises is increasing accordingly. At
the same time, with the development of artificial intelligence technology in
the past few decades, it has become quite common for companies to use new
technologies such as Big Data and artificial intelligence to empower outbound
call businesses. The intelligent outbound robot is a typical application of the
artificial intelligence technology in the field of outbound call businesses. It
is mainly used to communicate with customers in order to accomplish a certain
target. It has the characteristics of low cost, high reuse, and easy
compliance, which has attracted more attention from the industry.
  At present, there are two kinds of intelligent outbound robots in the
industry but both of them still leave large room for improvement. One kind of
them is based on a finite state machine relying on the configuration of jump
conditions and corresponding nodes based on manual experience. This kind of
intelligent outbound robot is also called a flow-based robot. For example, the
schematic diagram of the working model of a flow-based robot for debt
collection is shown in Fig.\ref{fig:label}. In each round, the robot will reply
to the user with the words corresponding to each node.
",1.0
723,19,211528,2210.00965,artificial intelligence for low carbon,C.-C. Jay Kuo and Azad M. Madni,"Green Learning: Introduction, Examples and Outlook","  Rapid advances in artificial intelligence (AI) in the last decade have
largely been built upon the wide applications of deep learning (DL). However,
the high carbon footprint yielded by larger and larger DL networks becomes a
concern for sustainability. Furthermore, DL decision mechanism is somewhat
obsecure and can only be verified by test data. Green learning (GL) has been
proposed as an alternative paradigm to address these concerns. GL is
characterized by low carbon footprints, small model sizes, low computational
complexity, and logical transparency. It offers energy-effective solutions in
cloud centers as well as mobile/edge devices. GL also provides a clear and
logical decision-making process to gain people's trust. Several statistical
tools have been developed to achieve this goal in recent years. They include
subspace approximation, unsupervised and supervised representation learning,
supervised discriminant feature selection, and feature space partitioning. We
have seen a few successful GL examples with performance comparable with
state-of-the-art DL solutions. This paper offers an introduction to GL, its
demonstrated applications, and future outlook.
",5.0
724,2,90428,1906.01741,random forests,"Louis Capitaine, J\'er\'emie Bigot, Rodolphe Thi\'ebaut and Robin
  Genuer","Fr\'echet random forests for metric space valued regression with non
  euclidean predictors","  Random forests are a statistical learning method widely used in many areas of
scientific research because of its ability to learn complex relationships
between input and output variables and also their capacity to handle
high-dimensional data. However, current random forest approaches are not
flexible enough to handle heterogeneous data such as curves, images and shapes.
In this paper, we introduce Fr\'echet trees and Fr\'echet random forests, which
allow to handle data for which input and output variables take values in
general metric spaces (which can be unordered). To this end, a new way of
splitting the nodes of trees is introduced and the prediction procedures of
trees and forests are generalized. Then, random forests out-of-bag error and
variable importance score are naturally adapted. A consistency theorem for
Fr\'echet regressogram predictor using data-driven partitions is given and
applied to Fr\'echet purely uniformly random trees. The method is studied
through several simulation scenarios on heterogeneous data combining
longitudinal, image and scalar data. Finally, two real datasets from HIV
vaccine trials are analyzed with the proposed method.
",5.0
725,14,215573,2210.16886,text summarization model,"Machel Reid, Vincent J. Hellendoorn, Graham Neubig",DiffusER: Discrete Diffusion via Edit-based Reconstruction,"  In text generation, models that generate text from scratch one token at a
time are currently the dominant paradigm. Despite being performant, these
models lack the ability to revise existing text, which limits their usability
in many practical scenarios. We look to address this, with DiffusER (Diffusion
via Edit-based Reconstruction), a new edit-based generative model for text
based on denoising diffusion models -- a class of models that use a Markov
chain of denoising steps to incrementally generate data. DiffusER is not only a
strong generative model in general, rivalling autoregressive models on several
tasks spanning machine translation, summarization, and style transfer; it can
also perform other varieties of generation that standard autoregressive models
are not well-suited for. For instance, we demonstrate that DiffusER makes it
possible for a user to condition generation on a prototype, or an incomplete
sequence, and continue revising based on previous edit steps.
",2.0
726,12,151413,2103.14804,COVID-19 and social media,"Xin Huang, Wenbin Zhang, Xuejiao Tang, Mingli Zhang, Jayachander
  Surbiryala, Vasileios Iosifidis, Zhen Liu and Ji Zhang",LSTM Based Sentiment Analysis for Cryptocurrency Prediction,"  Recent studies in big data analytics and natural language processing develop
automatic techniques in analyzing sentiment in the social media information. In
addition, the growing user base of social media and the high volume of posts
also provide valuable sentiment information to predict the price fluctuation of
the cryptocurrency. This research is directed to predicting the volatile price
movement of cryptocurrency by analyzing the sentiment in social media and
finding the correlation between them. While previous work has been developed to
analyze sentiment in English social media posts, we propose a method to
identify the sentiment of the Chinese social media posts from the most popular
Chinese social media platform Sina-Weibo. We develop the pipeline to capture
Weibo posts, describe the creation of the crypto-specific sentiment dictionary,
and propose a long short-term memory (LSTM) based recurrent neural network
along with the historical cryptocurrency price movement to predict the price
trend for future time frames. The conducted experiments demonstrate the
proposed approach outperforms the state of the art auto regressive based model
by 18.5% in precision and 15.4% in recall.
",0.0
727,12,97970,1909.07316,COVID-19 and social media,"Luke S. Snyder, Morteza Karimzadeh, Christina Stober, and David S.
  Ebert","Situational Awareness Enhanced through Social Media Analytics: A Survey
  of First Responders","  Social media data has been increasingly used to facilitate situational
awareness during events and emergencies such as natural disasters. While
researchers have investigated several methods to summarize, visualize or mine
the data for analysis, first responders have not been able to fully leverage
research advancements largely due to the gap between academic research and
deployed, functional systems. In this paper, we explore the opportunities and
barriers for the effective use of social media data from first responders'
perspective. We present the summary of several detailed interviews with first
responders on their use of social media for situational awareness. We further
assess the impact of SMART-a social media visual analytics system-on first
responder operations.
",1.0
728,18,41631,1604.04489,infomation retrieval time complexity,Robert Beinert,"One-dimensional phase retrieval with additional interference
  measurements","  The one-dimensional phase retrieval problem consists in the recovery of a
complex-valued signal from its Fourier intensity. Due to the well-known
ambiguousness of this problem, the determination of the original signal within
the extensive solution set is challenging and can only be done under suitable a
priori assumption or additional information about the unknown signal. Depending
on the application, one has sometimes access to further interference
measurements between the unknown signal and a reference signal. Beginning with
the reconstruction in the discrete-time setting, we show that each signal can
be uniquely recovered from its Fourier intensity and two further interference
measurements between the unknown signal and a modulation of the signal itself.
Afterwards, we consider the continuous-time problem, where we obtain an
equivalent result. Moreover, the unique recovery of a continuous-time signal
can also be ensured by using interference measurements with a known or an
unknown reference which is unrelated to the unknown signal.
",0.0
729,12,73726,1809.05834,COVID-19 and social media,"Praboda Rajapaksha, Reza Farahbakhsh, Noel Crespi, Bruno Defude",Inspecting Interactions: Online News Media Synergies in Social Media,"  The rising popularity of social media has radically changed the way news
content is propagated, including interactive attempts with new dimensions. To
date, traditional news media such as newspapers, television and radio have
already adapted their activities to the online news media by utilizing social
media, blogs, websites etc. This paper provides some insight into the social
media presence of worldwide popular news media outlets. Despite the fact that
these large news media propagate content via social media environments to a
large extent and very little is known about the news item producers, providers
and consumers in the news media community in social media.To better understand
these interactions, this work aims to analyze news items in two large social
media, Twitter and Facebook. Towards that end, we collected all published posts
on Twitter and Facebook from 48 news media to perform descriptive and
predictive analyses using the dataset of 152K tweets and 80K Facebook posts. We
explored a set of news media that originate content by themselves in social
media, those who distribute their news items to other news media and those who
consume news content from other news media and/or share replicas. We propose a
predictive model to increase news media popularity among readers based on the
number of posts, number of followers and number of interactions performed
within the news media community. The results manifested that, news media should
disperse their own content and they should publish first in social media in
order to become a popular news media and receive more attractions to their news
items from news readers.
",1.0
730,4,59537,1711.02013,pre-trained language model,"Yikang Shen, Zhouhan Lin, Chin-Wei Huang, Aaron Courville",Neural Language Modeling by Jointly Learning Syntax and Lexicon,"  We propose a neural language model capable of unsupervised syntactic
structure induction. The model leverages the structure information to form
better semantic representations and better language modeling. Standard
recurrent neural networks are limited by their structure and fail to
efficiently use syntactic information. On the other hand, tree-structured
recursive networks usually require additional structural supervision at the
cost of human expert annotation. In this paper, We propose a novel neural
language model, called the Parsing-Reading-Predict Networks (PRPN), that can
simultaneously induce the syntactic structure from unannotated sentences and
leverage the inferred structure to learn a better language model. In our model,
the gradient can be directly back-propagated from the language model loss into
the neural parsing network. Experiments show that the proposed model can
discover the underlying syntactic structure and achieve state-of-the-art
performance on word/character-level language model tasks.
",5.0
731,4,12783,1204.5648,pre-trained language model,"Ibrahim El Bitar, Fatima Zahra Belouadha and Ounsa Roudies",Taxonomy and synthesis of Web services querying languages,"  Most works on Web services has focused on discovery, composition and
selection processes of these kinds of services. Other few works were interested
in how to represent Web services search queries. However, these queries cannot
be processed by ensuring a high level of performance without being adequately
represented first. To this end, different query languages were designed. Even
so, in the absence of a standard, these languages are quite various. Their
diversity makes it difficult choosing the most suitable language. In fact, this
language should be able to cover all types of preferences or requirements of
clients such as their functional, nonfunctional,temporal or even specific
constraints as is the case of geographical or spatial constraints and meet
their needs and preferences helping to provide them the best answer. It must
also be mutually simple and imposes no restrictions or at least not too many
constraints in terms of prior knowledge to use and also provide a formal or
semi-formal queries presentation to support their automatic post-processing. A
comparative study is eventually established to allow to reveal the advantages
and limitations of various existing languages in this context. It is a
synthesis of this category of languages discussing their performance level and
their capability to respond to various needs related to the Web services
research and discovery case. The criterions identified at this stage may, in
our opinion, constitute then the main pre-requisite that a language should
satisfy to be called perfect or to be a future standard.
",1.0
732,4,138923,2011.114,pre-trained language model,Feng Qi,Language guided machine action,"  Here we build a hierarchical modular network called Language guided machine
action (LGMA), whose modules process information stream mimicking human
cortical network that allows to achieve multiple general tasks such as language
guided action, intention decomposition and mental simulation before action
execution etc. LGMA contains 3 main systems: (1) primary sensory system that
multimodal sensory information of vision, language and sensorimotor. (2)
association system involves and Broca modules to comprehend and synthesize
language, BA14/40 module to translate between sensorimotor and language,
midTemporal module to convert between language and vision, and superior
parietal lobe to integrate attended visual object and arm state into cognitive
map for future spatial actions. Pre-supplementary motor area (pre-SMA) can
converts high level intention into sequential atomic actions, while SMA can
integrate these atomic actions, current arm and attended object state into
sensorimotor vector to apply corresponding torques on arm via pre-motor and
primary motor of arm to achieve the intention. The high-level executive system
contains PFC that does explicit inference and guide voluntary action based on
language, while BG is the habitual action control center.
",1.0
733,10,49780,1701.08256,web archive,"Nattiya Kanhabua, Philipp Kemkes, Wolfgang Nejdl, Tu Ngoc Nguyen,
  Felipe Reis, Nam Khanh Tran",How to Search the Internet Archive Without Indexing It,"  Significant parts of cultural heritage are produced on the web during the
last decades. While easy accessibility to the current web is a good baseline,
optimal access to the past web faces several challenges. This includes dealing
with large-scale web archive collections and lacking of usage logs that contain
implicit human feedback most relevant for today's web search. In this paper, we
propose an entity-oriented search system to support retrieval and analytics on
the Internet Archive. We use Bing to retrieve a ranked list of results from the
current web. In addition, we link retrieved results to the WayBack Machine;
thus allowing keyword search on the Internet Archive without processing and
indexing its raw archived content. Our search system complements existing web
archive search tools through a user-friendly interface, which comes close to
the functionalities of modern web search engines (e.g., keyword search, query
auto-completion and related query suggestion), and provides a great benefit of
taking user feedback on the current web into account also for web archive
search. Through extensive experiments, we conduct quantitative and qualitative
analyses in order to provide insights that enable further research on and
practical applications of web archives.
",5.0
734,9,56246,1708.00781,language model for long documents,"Yangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin Choi, Noah A.
  Smith",Dynamic Entity Representations in Neural Language Models,"  Understanding a long document requires tracking how entities are introduced
and evolve over time. We present a new type of language model, EntityNLM, that
can explicitly model entities, dynamically update their representations, and
contextually generate their mentions. Our model is generative and flexible; it
can model an arbitrary number of entities in context while generating each
entity mention at an arbitrary length. In addition, it can be used for several
different tasks such as language modeling, coreference resolution, and entity
prediction. Experimental results with all these tasks demonstrate that our
model consistently outperforms strong baselines and prior work.
",5.0
735,4,186762,2202.10419,pre-trained language model,Kayo Yin and Graham Neubig,Interpreting Language Models with Contrastive Explanations,"  Model interpretability methods are often used to explain NLP model decisions
on tasks such as text classification, where the output space is relatively
small. However, when applied to language generation, where the output space
often consists of tens of thousands of tokens, these methods are unable to
provide informative explanations. Language models must consider various
features to predict a token, such as its part of speech, number, tense, or
semantics. Existing explanation methods conflate evidence for all these
features into a single explanation, which is less interpretable for human
understanding.
  To disentangle the different decisions in language modeling, we focus on
explaining language models contrastively: we look for salient input tokens that
explain why the model predicted one token instead of another. We demonstrate
that contrastive explanations are quantifiably better than non-contrastive
explanations in verifying major grammatical phenomena, and that they
significantly improve contrastive model simulatability for human observers. We
also identify groups of contrastive decisions where the model uses similar
evidence, and we are able to characterize what input tokens models use during
various language generation decisions.
",3.0
736,6,108369,2001.08085,query expansion for imformation retrieval,"Hardik Joshi, Jyoti Pareek","Experiments on Manual Thesaurus based Query Expansion for Ad-hoc
  Monolingual Gujarati Information Retrieval Tasks","  In this paper, we present the experimental work done on Query Expansion (QE)
for retrieval tasks of Gujarati text documents. In information retrieval, it is
very difficult to estimate the exact user need, query expansion adds terms to
the original query, which provides more information about the user need. There
are various approaches to query expansion. In our work, manual thesaurus based
query expansion was performed to evaluate the performance of widely used
information retrieval models for Gujarati text documents. Results show that
query expansion improves the recall of text documents.
",4.0
737,14,154405,2104.12465,text summarization model,"Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring","GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video
  Summarization","  Traditional video summarization methods generate fixed video representations
regardless of user interest. Therefore such methods limit users' expectations
in content search and exploration scenarios. Multi-modal video summarization is
one of the methods utilized to address this problem. When multi-modal video
summarization is used to help video exploration, a text-based query is
considered as one of the main drivers of video summary generation, as it is
user-defined. Thus, encoding the text-based query and the video effectively are
both important for the task of multi-modal video summarization. In this work, a
new method is proposed that uses a specialized attention network and
contextualized word representations to tackle this task. The proposed model
consists of a contextualized video summary controller, multi-modal attention
mechanisms, an interactive attention network, and a video summary generator.
Based on the evaluation of the existing multi-modal video summarization
benchmark, experimental results show that the proposed model is effective with
the increase of +5.88% in accuracy and +4.06% increase of F1-score, compared
with the state-of-the-art method.
",2.0
738,5,94924,1908.01009,matrix completion,"Xiangju Qin, Paul Blomstedt and Samuel Kaski",Scalable Bayesian Non-linear Matrix Completion,"  Matrix completion aims to predict missing elements in a partially observed
data matrix which in typical applications, such as collaborative filtering, is
large and extremely sparsely observed. A standard solution is matrix
factorization, which predicts unobserved entries as linear combinations of
latent variables. We generalize to non-linear combinations in massive-scale
matrices. Bayesian approaches have been proven beneficial in linear matrix
completion, but not applied in the more general non-linear case, due to limited
scalability. We introduce a Bayesian non-linear matrix completion algorithm,
which is based on a recent Bayesian formulation of Gaussian process latent
variable models. To solve the challenges regarding scalability and computation,
we propose a data-parallel distributed computational approach with a restricted
communication scheme. We evaluate our method on challenging out-of-matrix
prediction tasks using both simulated and real-world data.
",5.0
739,4,15400,1209.607,pre-trained language model,"Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman","Movie Popularity Classification based on Inherent Movie Attributes using
  C4.5,PART and Correlation Coefficient","  Abundance of movie data across the internet makes it an obvious candidate for
machine learning and knowledge discovery. But most researches are directed
towards bi-polar classification of movie or generation of a movie
recommendation system based on reviews given by viewers on various internet
sites. Classification of movie popularity based solely on attributes of a movie
i.e. actor, actress, director rating, language, country and budget etc. has
been less highlighted due to large number of attributes that are associated
with each movie and their differences in dimensions. In this paper, we propose
classification scheme of pre-release movie popularity based on inherent
attributes using C4.5 and PART classifier algorithm and define the relation
between attributes of post release movies using correlation coefficient.
",0.0
740,15,185012,2202.03097,relevance feedback for imformation retrieval,"Jiarui Jin, Xianyu Chen, Weinan Zhang, Junjie Huang, Ziming Feng, Yong
  Yu","Learn over Past, Evolve for Future: Search-based Time-aware
  Recommendation with Sequential Behavior Data","  The personalized recommendation is an essential part of modern e-commerce,
where user's demands are not only conditioned by their profile but also by
their recent browsing behaviors as well as periodical purchases made some time
ago. In this paper, we propose a novel framework named Search-based Time-Aware
Recommendation (STARec), which captures the evolving demands of users over time
through a unified search-based time-aware model. More concretely, we first
design a search-based module to retrieve a user's relevant historical
behaviors, which are then mixed up with her recent records to be fed into a
time-aware sequential network for capturing her time-sensitive demands. Besides
retrieving relevant information from her personal history, we also propose to
search and retrieve similar user's records as an additional reference. All
these sequential records are further fused to make the final recommendation.
Beyond this framework, we also develop a novel label trick that uses the
previous labels (i.e., user's feedbacks) as the input to better capture the
user's browsing pattern. We conduct extensive experiments on three real-world
commercial datasets on click-through-rate prediction tasks against
state-of-the-art methods. Experimental results demonstrate the superiority and
efficiency of our proposed framework and techniques. Furthermore, results of
online experiments on a daily item recommendation platform of Company X show
that STARec gains average performance improvement of around 6% and 1.5% in its
two main item recommendation scenarios on CTR metric respectively.
",1.0
741,0,49079,1701.01394,learning to rank with partitioned preference,Andrew V. Knyazev,On spectral partitioning of signed graphs,"  We argue that the standard graph Laplacian is preferable for spectral
partitioning of signed graphs compared to the signed Laplacian. Simple examples
demonstrate that partitioning based on signs of components of the leading
eigenvectors of the signed Laplacian may be meaningless, in contrast to
partitioning based on the Fiedler vector of the standard graph Laplacian for
signed graphs. We observe that negative eigenvalues are beneficial for spectral
partitioning of signed graphs, making the Fiedler vector easier to compute.
",0.0
742,12,170155,2109.11024,COVID-19 and social media,"Kin Wai Ng, Sameera Horawalavithana, and Adriana Iamnitchi",Social-Media Activity Forecasting with Exogenous Information Signals,"  Due to their widespread adoption, social media platforms present an ideal
environment for studying and understanding social behavior, especially on
information spread. Modeling social media activity has numerous practical
implications such as supporting efforts to analyze strategic information
operations, designing intervention techniques to mitigate disinformation, or
delivering critical information during disaster relief operations. In this
paper we propose a modeling technique that forecasts topic-specific daily
volume of social media activities by using both exogenous signals, such as news
or armed conflicts records, and endogenous data from the social media platform
we model. Empirical evaluations with real datasets from two different platforms
and two different contexts each composed of multiple interrelated topics
demonstrate the effectiveness of our solution.
",1.0
743,18,41373,1604.01839,infomation retrieval time complexity,"Arya Mazumdar, Barna Saha",Clustering Via Crowdsourcing,"  In recent years, crowdsourcing, aka human aided computation has emerged as an
effective platform for solving problems that are considered complex for
machines alone. Using human is time-consuming and costly due to monetary
compensations. Therefore, a crowd based algorithm must judiciously use any
information computed through an automated process, and ask minimum number of
questions to the crowd adaptively.
  One such problem which has received significant attention is {\em entity
resolution}. Formally, we are given a graph $G=(V,E)$ with unknown edge set $E$
where $G$ is a union of $k$ (again unknown, but typically large $O(n^\alpha)$,
for $\alpha>0$) disjoint cliques $G_i(V_i, E_i)$, $i =1, \dots, k$. The goal is
to retrieve the sets $V_i$s by making minimum number of pair-wise queries $V
\times V\to\{\pm1\}$ to an oracle (the crowd). When the answer to each query is
correct, e.g. via resampling, then this reduces to finding connected components
in a graph. On the other hand, when crowd answers may be incorrect, it
corresponds to clustering over minimum number of noisy inputs. Even, with
perfect answers, a simple lower and upper bound of $\Theta(nk)$ on query
complexity can be shown. A major contribution of this paper is to reduce the
query complexity to linear or even sublinear in $n$ when mild side information
is provided by a machine, and even in presence of crowd errors which are not
correctable via resampling. We develop new information theoretic lower bounds
on the query complexity of clustering with side information and errors, and our
upper bounds closely match with them. Our algorithms are naturally
parallelizable, and also give near-optimal bounds on the number of adaptive
rounds required to match the query complexity.
",0.0
744,16,168710,2109.04386,activation function in neutral networks,"Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, Ashish Kumar Pandey",ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions,"  An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends also on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct and Pserf. Experiments suggest that the proposed functions improve the
network performance significantly compared to the widely used activations like
ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and
5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in
CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet
V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.
",5.0
745,15,43735,1606.0766,relevance feedback for imformation retrieval,"Christina Lioma and Birger Larsen and Casper Petersen and Jakob Grue
  Simonsen","Deep Learning Relevance: Creating Relevant Information (as Opposed to
  Retrieving it)","  What if Information Retrieval (IR) systems did not just retrieve relevant
information that is stored in their indices, but could also ""understand"" it and
synthesise it into a single document? We present a preliminary study that makes
a first step towards answering this question. Given a query, we train a
Recurrent Neural Network (RNN) on existing relevant information to that query.
We then use the RNN to ""deep learn"" a single, synthetic, and we assume,
relevant document for that query. We design a crowdsourcing experiment to
assess how relevant the ""deep learned"" document is, compared to existing
relevant documents. Users are shown a query and four wordclouds (of three
existing relevant documents and our deep learned synthetic document). The
synthetic document is ranked on average most relevant of all.
",1.0
746,19,128269,2008.04793,artificial intelligence for low carbon,Andrzej Cichocki and Alexander P. Kuleshov,"Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of
  AI/AGI Using Multiple Intelligences and Learning Styles","  This article discusses some trends and concepts in developing new generation
of future Artificial General Intelligence (AGI) systems which relate to complex
facets and different types of human intelligence, especially social, emotional,
attentional and ethical intelligence. We describe various aspects of multiple
human intelligences and learning styles, which may impact on a variety of AI
problem domains. Using the concept of 'multiple intelligences' rather than a
single type of intelligence, we categorize and provide working definitions of
various AGI depending on their cognitive skills or capacities. Future AI
systems will be able not only to communicate with human users and each other,
but also to efficiently exchange knowledge and wisdom with abilities of
cooperation, collaboration and even co-creating something new and valuable and
have meta-learning capacities. Multi-agent systems such as these can be used to
solve problems that would be difficult to solve by any individual intelligent
agent.
  Key words: Artificial General Intelligence (AGI), multiple intelligences,
learning styles, physical intelligence, emotional intelligence, social
intelligence, attentional intelligence, moral-ethical intelligence, responsible
decision making, creative-innovative intelligence, cognitive functions,
meta-learning of AI systems.
",1.0
747,1,160223,2106.07813,advanced search engine,"Ashlee Milton, Garrett Allen, Maria Soledad Pera","To Infinity and Beyond! Accessibility is the Future for Kids' Search
  Engines","  Research in the area of search engines for children remains in its infancy.
Seminal works have studied how children use mainstream search engines, as well
as how to design and evaluate custom search engines explicitly for children.
These works, however, tend to take a one-size-fits-all view, treating children
as a unit. Nevertheless, even at the same age, children are known to possess
and exhibit different capabilities. These differences affect how children
access and use search engines. To better serve children, in this vision paper,
we spotlight accessibility and discuss why current research on children and
search engines does not, but should, focus on this significant matter.
",3.0
748,15,206495,2208.06955,relevance feedback for imformation retrieval,"Nima Sadri, Gordon V. Cormack",Continuous Active Learning Using Pretrained Transformers,"  Pre-trained and fine-tuned transformer models like BERT and T5 have improved
the state of the art in ad-hoc retrieval and question-answering, but not as yet
in high-recall information retrieval, where the objective is to retrieve
substantially all relevant documents. We investigate whether the use of
transformer-based models for reranking and/or featurization can improve the
Baseline Model Implementation of the TREC Total Recall Track, which represents
the current state of the art for high-recall information retrieval. We also
introduce CALBERT, a model that can be used to continuously fine-tune a
BERT-based model based on relevance feedback.
",1.0
749,19,26646,1405.3939,artificial intelligence for low carbon,Aadhityan A,"A Novel Method for Developing Robotics via Artificial Intelligence and
  Internet of Things","  This paper describe about a new methodology for developing and improving the
robotics field via artificial intelligence and internet of things. Now a day,
we can say Artificial Intelligence take the world into robotics. Almost all
industries use robots for lot of works. They are use co-operative robots to
make different kind of works. But there was some problem to make robot for
multi tasks. So there was a necessary new methodology to made multi tasking
robots. It will be done only by artificial intelligence and internet of things.
",1.0
750,2,180470,2112.10612,random forests,"Marissa Baxter, Lisa Ha, Kirill Perfiliev, and Natalie Sayre",Context-Based Music Recommendation Algorithm Evaluation,"  Artificial Intelligence (AI ) has been very successful in creating and
predicting music playlists for online users based on their data; data received
from users experience using the app such as searching the songs they like.
There are lots of current technological advancements in AI due to the
competition between music platform owners such as Spotify, Pandora, and more.
In this paper, 6 machine learning algorithms and their individual accuracy for
predicting whether a user will like a song are explored across 3 different
platforms including Weka, SKLearn, and Orange. The algorithms explored include
Logistic Regression, Naive Bayes, Sequential Minimal Optimization (SMO),
Multilayer Perceptron (Neural Network), Nearest Neighbor, and Random Forest.
With the analysis of the specific characteristics of each song provided by the
Spotify API [1], Random Forest is the most successful algorithm for predicting
whether a user will like a song with an accuracy of 84%. This is higher than
the accuracy of 82.72% found by Mungekar using the Random Forest technique and
slightly different characteristics of a song [2]. The characteristics in
Mungekars Random Forest algorithm focus more on the artist and popularity
rather than the sonic features of the songs. Removing the popularity aspect and
focusing purely on the sonic qualities improve the accuracy of recommendations.
Finally, this paper shows how song prediction can be accomplished without any
monetary investments, and thus, inspires an idea of what amazing results can be
accomplished with full financial research.
",3.0
751,19,195286,2205.0487,artificial intelligence for low carbon,"Terrence Chan, Carla Arus Gomez, Anish Kothikar, Pedro Baiz","Joint Study of Above Ground Biomass and Soil Organic Carbon for Total
  Carbon Estimation using Satellite Imagery in Scotland","  Land Carbon verification has long been a challenge in the carbon credit
market. Carbon verification methods currently available are expensive, and may
generate low-quality credit. Scalable and accurate remote sensing techniques
enable new approaches to monitor changes in Above Ground Biomass (AGB) and Soil
Organic Carbon (SOC). The majority of state-of-the-art research employs remote
sensing on AGB and SOC separately, although some studies indicate a positive
correlation between the two. We intend to combine the two domains in our
research to improve state-of-the-art total carbon estimation and to provide
insight into the voluntary carbon trading market. We begin by establishing
baseline model in our study area in Scotland, using state-of-the-art
methodologies in the SOC and AGB domains. The effects of feature engineering
techniques such as variance inflation factor and feature selection on machine
learning models are then investigated. This is extended by combining predictor
variables from the two domains. Finally, we leverage the possible correlation
between AGB and SOC to establish a relationship between the two and propose
novel models in an attempt outperform the state-of-the-art results. We compared
three machine learning techniques, boosted regression tree, random forest, and
xgboost. These techniques have been demonstrated to be the most effective in
both domains.
",5.0
752,15,216400,cs/0007041,relevance feedback for imformation retrieval,"Gianni Amati, Konstantinos Georgatos",Relevance as Deduction: A Logical View of Information Retrieval,"  The problem of Information Retrieval is, given a set of documents D and a
query q, providing an algorithm for retrieving all documents in D relevant to
q. However, retrieval should depend and be updated whenever the user is able to
provide as an input a preferred set of relevant documents; this process is
known as em relevance feedback. Recent work in IR has been paying great
attention to models which employ a logical approach; the advantage being that
one can have a simple computable characterization of retrieval on the basis of
a pure logical analysis of retrieval. Most of the logical models make use of
probabilities or similar belief functions in order to introduce the inductive
component whereby uncertainty is treated. Their general paradigm is the
following: em find the nature of conditional $d\imp q$ and then define a
probability on the top of it. We just reverse this point of view; first use the
numerical information, frequencies or probabilities, then define your own
logical consequence. More generally, we claim that retrieval is a form of
deduction. We introduce a simple but powerful logical framework of relevance
feedback, derived from the well founded area of nonmonotonic logic. This
description can help us evaluate, describe and compare from a theoretical point
of view previous approaches based on conditionals or probabilities.
",4.0
753,19,120405,2006.01601,artificial intelligence for low carbon,"Alexander J. M. Kell, A. Stephen McGough, Matthew Forshaw","Optimizing carbon tax for decentralized electricity markets using an
  agent-based model","  Averting the effects of anthropogenic climate change requires a transition
from fossil fuels to low-carbon technology. A way to achieve this is to
decarbonize the electricity grid. However, further efforts must be made in
other fields such as transport and heating for full decarbonization. This would
reduce carbon emissions due to electricity generation, and also help to
decarbonize other sources such as automotive and heating by enabling a
low-carbon alternative. Carbon taxes have been shown to be an efficient way to
aid in this transition. In this paper, we demonstrate how to to find optimal
carbon tax policies through a genetic algorithm approach, using the electricity
market agent-based model ElecSim. To achieve this, we use the NSGA-II genetic
algorithm to minimize average electricity price and relative carbon intensity
of the electricity mix. We demonstrate that it is possible to find a range of
carbon taxes to suit differing objectives. Our results show that we are able to
minimize electricity cost to below \textsterling10/MWh as well as carbon
intensity to zero in every case. In terms of the optimal carbon tax strategy,
we found that an increasing strategy between 2020 and 2035 was preferable. Each
of the Pareto-front optimal tax strategies are at least above
\textsterling81/tCO2 for every year. The mean carbon tax strategy was
\textsterling240/tCO2.
",4.0
754,18,75923,1810.10769,infomation retrieval time complexity,"Jaspreet Singh, Wolfgang Nejdl, Avishek Anand",Expedition: A Time-Aware Exploratory Search System Designed for Scholars,"  Archives are an important source of study for various scholars. Digitization
and the web have made archives more accessible and led to the development of
several time-aware exploratory search systems. However these systems have been
designed for more general users rather than scholars. Scholars have more
complex information needs in comparison to general users. They also require
support for corpus creation during their exploration process. In this paper we
present Expedition - a time-aware exploratory search system that addresses the
requirements and information needs of scholars. Expedition possesses a suite of
ad-hoc and diversity based retrieval models to address complex information
needs; a newspaper-style user interface to allow for larger textual previews
and comparisons; entity filters to more naturally refine a result list and an
interactive annotated timeline which can be used to better identify periods of
importance.
",2.0
755,14,157069,2105.10311,text summarization model,"Junyi Li, Tianyi Tang, Wayne Xin Zhao and Ji-Rong Wen",Pretrained Language Models for Text Generation: A Survey,"  Text generation has become one of the most important yet challenging tasks in
natural language processing (NLP). The resurgence of deep learning has greatly
advanced this field by neural generation models, especially the paradigm of
pretrained language models (PLMs). In this paper, we present an overview of the
major advances achieved in the topic of PLMs for text generation. As the
preliminaries, we present the general task definition and briefly describe the
mainstream architectures of PLMs for text generation. As the core content, we
discuss how to adapt existing PLMs to model different input data and satisfy
special properties in the generated text. We further summarize several
important fine-tuning strategies for text generation. Finally, we present
several future directions and conclude this paper. Our survey aims to provide
text generation researchers a synthesis and pointer to related research.
",3.0
756,18,9850,1109.1498,infomation retrieval time complexity,"E. Di Sciascio, F. M. Donini, M. Mongiello",Structured Knowledge Representation for Image Retrieval,"  We propose a structured approach to the problem of retrieval of images by
content and present a description logic that has been devised for the semantic
indexing and retrieval of images containing complex objects. As other
approaches do, we start from low-level features extracted with image analysis
to detect and characterize regions in an image. However, in contrast with
feature-based approaches, we provide a syntax to describe segmented regions as
basic objects and complex objects as compositions of basic ones. Then we
introduce a companion extensional semantics for defining reasoning services,
such as retrieval, classification, and subsumption. These services can be used
for both exact and approximate matching, using similarity measures. Using our
logical approach as a formal specification, we implemented a complete
client-server image retrieval system, which allows a user to pose both queries
by sketch and queries by example. A set of experiments has been carried out on
a testbed of images to assess the retrieval capabilities of the system in
comparison with expert users ranking. Results are presented adopting a
well-established measure of quality borrowed from textual information
retrieval.
",1.0
757,6,53302,1705.04803,query expansion for imformation retrieval,"Federico Nanni, Bhaskar Mitra, Matt Magnusson and Laura Dietz",Benchmark for Complex Answer Retrieval,"  Retrieving paragraphs to populate a Wikipedia article is a challenging task.
The new TREC Complex Answer Retrieval (TREC CAR) track introduces a
comprehensive dataset that targets this retrieval scenario. We present early
results from a variety of approaches -- from standard information retrieval
methods (e.g., tf-idf) to complex systems that using query expansion using
knowledge bases and deep neural networks. The goal is to offer future
participants of this track an overview of some promising approaches to tackle
this problem.
",3.0
758,8,77122,1811.04441,node embedding for graph,"Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, Bowen Zhou","End-to-end Structure-Aware Convolutional Networks for Knowledge Base
  Completion","  Knowledge graph embedding has been an active research topic for knowledge
base completion, with progressive improvement from the initial TransE, TransH,
DistMult et al to the current state-of-the-art ConvE. ConvE uses 2D convolution
over embeddings and multiple layers of nonlinear features to model knowledge
graphs. The model can be efficiently trained and scalable to large knowledge
graphs. However, there is no structure enforcement in the embedding space of
ConvE. The recent graph convolutional network (GCN) provides another way of
learning graph node embedding by successfully utilizing graph connectivity
structure. In this work, we propose a novel end-to-end Structure-Aware
Convolutional Network (SACN) that takes the benefit of GCN and ConvE together.
SACN consists of an encoder of a weighted graph convolutional network (WGCN),
and a decoder of a convolutional network called Conv-TransE. WGCN utilizes
knowledge graph node structure, node attributes and edge relation types. It has
learnable weights that adapt the amount of information from neighbors used in
local aggregation, leading to more accurate embeddings of graph nodes. Node
attributes in the graph are represented as additional nodes in the WGCN. The
decoder Conv-TransE enables the state-of-the-art ConvE to be translational
between entities and relations while keeps the same link prediction performance
as ConvE. We demonstrate the effectiveness of the proposed SACN on standard
FB15k-237 and WN18RR datasets, and it gives about 10% relative improvement over
the state-of-the-art ConvE in terms of HITS@1, HITS@3 and HITS@10.
",4.0
759,0,190338,2203.12744,learning to rank with partitioned preference,Alessandro Pindozzi,Trying to bridge the gap between skyline and top-k queries,"  There are two most common paradigms that are used in order to identify
records of preference in a multi-objective settings, one relies on dominance,
like the skyline operator, the other instead, on a utility function defined
over the records' attributes, typically using top-k queries. Although they are
very popular, we have to take in account their main disadvantages, which bring
us to describe three hard requirements: personalization, controllable output
size, and flexibility in preference specification. In fact Skyline queries are
simple to specify but they are not equipped with any means to accommodate user
preferences or to control the cardinality of the result set. Ranking queries
adopt, instead, a specific scoring function to rank tuples, and can easily
control the output size, but it is difficult to specify correctly the weights
of this scoring function in order to give different importance to the
attributes. In this paper we describe three different approaches which try to
satisfy the three hard requirements mentioned above embracing the advantages
either of the Skyline queries or of the ranking queries. These approaches are
namely: Flexible Skyline, ORD-ORU and UTK.
",1.0
760,13,120802,2006.03644,social network analysis with natrual language processing,Abeer AlDayel and Walid Magdy,Stance Detection on Social Media: State of the Art and Trends,"  Stance detection on social media is an emerging opinion mining paradigm for
various social and political applications in which sentiment analysis may be
sub-optimal. There has been a growing research interest for developing
effective methods for stance detection methods varying among multiple
communities including natural language processing, web science, and social
computing. This paper surveys the work on stance detection within those
communities and situates its usage within current opinion mining techniques in
social media. It presents an exhaustive review of stance detection techniques
on social media, including the task definition, different types of targets in
stance detection, features set used, and various machine learning approaches
applied. The survey reports state-of-the-art results on the existing benchmark
datasets on stance detection, and discusses the most effective approaches. In
addition, this study explores the emerging trends and different applications of
stance detection on social media. The study concludes by discussing the gaps in
the current existing research and highlights the possible future directions for
stance detection on social media.
",3.0
761,5,11096,1112.5629,matrix completion,Brian Eriksson and Laura Balzano and Robert Nowak,High-Rank Matrix Completion and Subspace Clustering with Missing Data,"  This paper considers the problem of completing a matrix with many missing
entries under the assumption that the columns of the matrix belong to a union
of multiple low-rank subspaces. This generalizes the standard low-rank matrix
completion problem to situations in which the matrix rank can be quite high or
even full rank. Since the columns belong to a union of subspaces, this problem
may also be viewed as a missing-data version of the subspace clustering
problem. Let X be an n x N matrix whose (complete) columns lie in a union of at
most k subspaces, each of rank <= r < n, and assume N >> kn. The main result of
the paper shows that under mild assumptions each column of X can be perfectly
recovered with high probability from an incomplete version so long as at least
CrNlog^2(n) entries of X are observed uniformly at random, with C>1 a constant
depending on the usual incoherence conditions, the geometrical arrangement of
subspaces, and the distribution of columns over the subspaces. The result is
illustrated with numerical experiments and an application to Internet distance
matrix completion and topology identification.
",5.0
762,13,51408,1703.05053,social network analysis with natrual language processing,"Mauro Coletto, Kiran Garimella, Aristides Gionis, Claudio Lucchese",A Motif-based Approach for Identifying Controversy,"  Among the topics discussed in Social Media, some lead to controversy. A
number of recent studies have focused on the problem of identifying controversy
in social media mostly based on the analysis of textual content or rely on
global network structure. Such approaches have strong limitations due to the
difficulty of understanding natural language, and of investigating the global
network structure. In this work we show that it is possible to detect
controversy in social media by exploiting network motifs, i.e., local patterns
of user interaction. The proposed approach allows for a language-independent
and fine- grained and efficient-to-compute analysis of user discussions and
their evolution over time. The supervised model exploiting motif patterns can
achieve 85% accuracy, with an improvement of 7% compared to baseline
structural, propagation-based and temporal network features.
",1.0
763,18,42780,1605.08034,infomation retrieval time complexity,Yang Wang and Zhiqiang Xu,"Generalized phase retrieval : measurement number, matrix recovery and
  beyond","  In this paper, we develop a framework of generalized phase retrieval in which
one aims to reconstruct a vector ${\mathbf x}$ in ${\mathbb R}^d$ or ${\mathbb
C}^d$ through quadratic samples ${\mathbf x}^*A_1{\mathbf x}, \dots, {\mathbf
x}^*A_N{\mathbf x}$. The generalized phase retrieval includes as special cases
the standard phase retrieval as well as the phase retrieval by orthogonal
projections. We first explore the connections among generalized phase
retrieval, low-rank matrix recovery and nonsingular bilinear form. Motivated by
the connections, we present results on the minimal measurement number needed
for recovering a matrix that lies in a set $W\in {\mathbb C}^{d\times d}$.
Applying the results to phase retrieval, we show that generic $d \times d$
matrices $A_1,\ldots, A_N$ have the phase retrieval property if $N\geq 2d-1$ in
the real case and $N \geq 4d-4$ in the complex case for very general classes of
$A_1,\ldots,A_N$, e.g. matrices with prescribed ranks or orthogonal
projections. Our method also leads to a novel proof for the classical
Stiefel-Hopf condition on nonsingular bilinear form. We also give lower bounds
on the minimal measurement number required for generalized phase retrieval. For
several classes of dimensions $d$ we obtain the precise values of the minimal
measurement number. Our work unifies and enhances results from the standard
phase retrieval, phase retrieval by projections and low-rank matrix recovery.
",0.0
764,18,50399,1702.05425,infomation retrieval time complexity,"Jonathan A. Marshall, Lawrence C. Rafsky",Exact clustering in linear time,"  The time complexity of data clustering has been viewed as fundamentally
quadratic, slowing with the number of data items, as each item is compared for
similarity to preceding items. Clustering of large data sets has been
infeasible without resorting to probabilistic methods or to capping the number
of clusters. Here we introduce MIMOSA, a novel class of algorithms which
achieve linear time computational complexity on clustering tasks. MIMOSA
algorithms mark and match partial-signature keys in a hash table to obtain
exact, error-free cluster retrieval. Benchmark measurements, on clustering a
data set of 10,000,000 news articles by news topic, found that a MIMOSA
implementation finished more than four orders of magnitude faster than a
standard centroid implementation.
",1.0
765,4,196285,2205.09394,pre-trained language model,"Xiang Li, Xiaojiang Zhou, Yao Xiao, Peihao Huang, Dayao Chen, Sheng
  Chen, Yunsen Xian","AutoFAS: Automatic Feature and Architecture Selection for Pre-Ranking
  System","  Industrial search and recommendation systems mostly follow the classic
multi-stage information retrieval paradigm: matching, pre-ranking, ranking, and
re-ranking stages. To account for system efficiency, simple vector-product
based models are commonly deployed in the pre-ranking stage. Recent works
consider distilling the high knowledge of large ranking models to small
pre-ranking models for better effectiveness. However, two major challenges in
pre-ranking system still exist: (i) without explicitly modeling the performance
gain versus computation cost, the predefined latency constraint in the
pre-ranking stage inevitably leads to suboptimal solutions; (ii) transferring
the ranking teacher's knowledge to a pre-ranking student with a predetermined
handcrafted architecture still suffers from the loss of model performance. In
this work, a novel framework AutoFAS is proposed which jointly optimizes the
efficiency and effectiveness of the pre-ranking model: (i) AutoFAS for the
first time simultaneously selects the most valuable features and network
architectures using Neural Architecture Search (NAS) technique; (ii) equipped
with ranking model guided reward during NAS procedure, AutoFAS can select the
best pre-ranking architecture for a given ranking teacher without any
computation overhead. Experimental results in our real world search system show
AutoFAS consistently outperforms the previous state-of-the-art (SOTA)
approaches at a lower computing cost. Notably, our model has been adopted in
the pre-ranking module in the search system of Meituan, bringing significant
improvements.
",0.0
766,15,15167,1209.2274,relevance feedback for imformation retrieval,"Reza Tavoli, Fariborz Mahmoudi",PCA-Based Relevance Feedback in Document Image Retrieval,"  Research has been devoted in the past few years to relevance feedback as an
effective solution to improve performance of information retrieval systems.
Relevance feedback refers to an interactive process that helps to improve the
retrieval performance. In this paper we propose the use of relevance feedback
to improve document image retrieval System (DIRS) performance. This paper
compares a variety of strategies for positive and negative feedback. In
addition, feature subspace is extracted and updated during the feedback process
using a Principal Component Analysis (PCA) technique and based on user's
feedback. That is, in addition to reducing the dimensionality of feature
spaces, a proper subspace for each type of features is obtained in the feedback
process to further improve the retrieval accuracy. Experiments show that using
relevance Feedback in DIR achieves better performance than common DIR.
",5.0
767,19,182722,2201.06947,artificial intelligence for low carbon,"Salwa Saafi, Olga Vikhrova, G\'abor Fodor, Jiri Hosek, Sergey Andreev","AI-Aided Integrated Terrestrial and Non-Terrestrial 6G Solutions for
  Sustainable Maritime Networking","  The maritime industry is experiencing a technological revolution that affects
shipbuilding, operation of both seagoing and inland vessels, cargo management,
and working practices in harbors. This ongoing transformation is driven by the
ambition to make the ecosystem more sustainable and cost-efficient.
Digitalization and automation help achieve these goals by transforming shipping
and cruising into a much more cost- and energy-efficient, and decarbonized
industry segment. The key enablers in these processes are always-available
connectivity and content delivery services, which can not only aid shipping
companies in improving their operational efficiency and reducing carbon
emissions but also contribute to enhanced crew welfare and passenger
experience. Due to recent advancements in integrating high-capacity and
ultra-reliable terrestrial and non-terrestrial networking technologies,
ubiquitous maritime connectivity is becoming a reality. To cope with the
increased complexity of managing these integrated systems, this article
advocates the use of artificial intelligence and machine learning-based
approaches to meet the service requirements and energy efficiency targets in
various maritime communications scenarios.
",4.0
768,3,3707,909.1764,database management system,"Uwe Roehm (University of Sydney), Jose Blakeley (Microsoft)",Data Management for High-Throughput Genomics,"  Today's sequencing technology allows sequencing an individual genome within a
few weeks for a fraction of the costs of the original Human Genome project.
Genomics labs are faced with dozens of TB of data per week that have to be
automatically processed and made available to scientists for further analysis.
This paper explores the potential and the limitations of using relational
database systems as the data processing platform for high-throughput genomics.
In particular, we are interested in the storage management for high-throughput
sequence data and in leveraging SQL and user-defined functions for data
analysis inside a database system. We give an overview of a database design for
high-throughput genomics, how we used a SQL Server database in some
unconventional ways to prototype this scenario, and we will discuss some
initial findings about the scalability and performance of such a more
database-centric approach.
",5.0
769,3,216833,cs/0306020,database management system,"Adeyemi Adesanya, Tofigh Azemoon, Jacek Becla, Andrew Hanushevsky,
  Adil Hasan, Wilko Kroeger, Artem Trunov, Daniel Wang, Igor Gaponenko, Simon
  Patton, David Quarrie","On the Verge of One Petabyte - the Story Behind the BaBar Database
  System","  The BaBar database has pioneered the use of a commercial ODBMS within the HEP
community. The unique object-oriented architecture of Objectivity/DB has made
it possible to manage over 700 terabytes of production data generated since
May'99, making the BaBar database the world's largest known database. The
ongoing development includes new features, addressing the ever-increasing
luminosity of the detector as well as other changing physics requirements.
Significant efforts are focused on reducing space requirements and operational
costs. The paper discusses our experience with developing a large scale
database system, emphasizing universal aspects which may be applied to any
large scale system, independently of underlying technology used.
",5.0
770,0,161524,2106.13054,learning to rank with partitioned preference,Bruno Escoffier and Olivier Spanjaard and Magdal\'ena Tydrichov\'a,"Weighted majority tournaments and Kemeny ranking with 2-dimensional
  Euclidean preferences","  The assumption that voters' preferences share some common structure is a
standard way to circumvent NP-hardness results in social choice problems. While
the Kemeny ranking problem is NP-hard in the general case, it is known to
become easy if the preferences are 1-dimensional Euclidean. In this note, we
prove that the Kemeny ranking problem remains NP-hard for $k$-dimensional
Euclidean preferences with $k\!\ge\!2$ under norms $\ell_1$, $\ell_2$ and
$\ell_\infty$, by showing that any weighted tournament (resp. weighted
bipartite tournament) with weights of same parity (resp. even weights) is
inducible as the weighted majority tournament of a profile of 2-Euclidean
preferences under norm $\ell_2$ (resp. $\ell_1,\ell_{\infty}$), computable in
polynomial time. More generally, this result regarding weighted tournaments
implies, essentially, that hardness results relying on the (weighted) majority
tournament that hold in the general case (e.g., NP-hardness of Slater ranking)
are still true for 2-dimensional Euclidean preferences.
",1.0
771,4,117861,2005.02593,pre-trained language model,"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao,
  Jingbo Zhu, Tongran Liu, Changliang Li","Learning Architectures from an Extended Search Space for Language
  Modeling","  Neural architecture search (NAS) has advanced significantly in recent years
but most NAS systems restrict search to learning architectures of a recurrent
or convolutional cell. In this paper, we extend the search space of NAS. In
particular, we present a general approach to learn both intra-cell and
inter-cell architectures (call it ESS). For a better search result, we design a
joint learning method to perform intra-cell and inter-cell NAS simultaneously.
We implement our model in a differentiable architecture search system. For
recurrent neural language modeling, it outperforms a strong baseline
significantly on the PTB and WikiText data, with a new state-of-the-art on PTB.
Moreover, the learned architectures show good transferability to other systems.
E.g., they improve state-of-the-art systems on the CoNLL and WNUT named entity
recognition (NER) tasks and CoNLL chunking task, indicating a promising line of
research on large-scale pre-learned architectures.
",5.0
772,5,99643,1910.02576,matrix completion,"Jinchi Chen, Weiguo Gao, Ke Wei","Exact matrix completion based on low rank Hankel structure in the
  Fourier domain","  Matrix completion is about recovering a matrix from its partial revealed
entries, and it can often be achieved by exploiting the inherent simplicity or
low dimensional structure of the target matrix. For instance, a typical notion
of matrix simplicity is low rank. In this paper we study matrix completion
based on another low dimensional structure, namely the low rank Hankel
structure in the Fourier domain. It is shown that matrices with this structure
can be exactly recovered by solving a convex optimization program provided the
sampling complexity is nearly optimal. Empirical results are also presented to
justify the effectiveness of the convex method.
",5.0
773,8,122417,2006.0943,node embedding for graph,"Soheil Kolouri, Navid Naderializadeh, Gustavo K. Rohde, Heiko Hoffmann",Wasserstein Embedding for Graph Learning,"  We present Wasserstein Embedding for Graph Learning (WEGL), a novel and fast
framework for embedding entire graphs in a vector space, in which various
machine learning models are applicable for graph-level prediction tasks. We
leverage new insights on defining similarity between graphs as a function of
the similarity between their node embedding distributions. Specifically, we use
the Wasserstein distance to measure the dissimilarity between node embeddings
of different graphs. Unlike prior work, we avoid pairwise calculation of
distances between graphs and reduce the computational complexity from quadratic
to linear in the number of graphs. WEGL calculates Monge maps from a reference
distribution to each node embedding and, based on these maps, creates a
fixed-sized vector representation of the graph. We evaluate our new graph
embedding approach on various benchmark graph-property prediction tasks,
showing state-of-the-art classification performance while having superior
computational efficiency. The code is available at
https://github.com/navid-naderi/WEGL.
",5.0
774,10,114497,2003.14046,web archive,"Xinyue Wang, Zhiwu Xie","The Case For Alternative Web Archival Formats To Expedite The
  Data-To-Insight Cycle","  The WARC file format is widely used by web archives to preserve collected web
content for future use. With the rapid growth of web archives and the
increasing interest to reuse these archives as big data sources for statistical
and analytical research, the speed to turn these data into insights becomes
critical. In this paper we show that the WARC format carries significant
performance penalties for batch processing workload. We trace the root cause of
these penalties to its data structure, encoding, and addressing method. We then
run controlled experiments to illustrate how severe these problems can be.
Indeed, performance gain of one to two orders of magnitude can be achieved
simply by reformatting WARC files into Parquet or Avro formats. While these
results do not necessarily constitute an endorsement for Avro or Parquet, the
time has come for the web archiving community to consider replacing WARC with
more efficient web archival formats.
",3.0
775,1,33849,1505.05187,advanced search engine,Abhay Prakash and Dhaval Patel,Techniques for Deep Query Understanding,"  Query Understanding concerns about inferring the precise intent of search by
the user with his formulated query, which is challenging because the queries
are often very short and ambiguous. The report discusses the various kind of
queries that can be put to a Search Engine and illustrates the Role of Query
Understanding for return of relevant results. With different advances in
techniques for deep understanding of queries as well as documents, the Search
Technology has witnessed three major era. A lot of interesting real world
examples have been used to illustrate the role of Query Understanding in each
of them. The Query Understanding Module is responsible to correct the mistakes
done by user in the query, to guide him in formulation of query with precise
intent, and to precisely infer the intent of the user query. The report
describes the complete architecture to handle aforementioned three tasks, and
then discusses basic as well as recent advanced techniques for each of the
component, through appropriate papers from reputed conferences and journals.
",4.0
776,10,8602,1105.3459,web archive,"Robert Sanderson, Mark Phillips, Herbert Van de Sompel",Analyzing the Persistence of Referenced Web Resources with Memento,"  In this paper we present the results of a study into the persistence and
availability of web resources referenced from papers in scholarly repositories.
Two repositories with different characteristics, arXiv and the UNT digital
library, are studied to determine if the nature of the repository, or of its
content, has a bearing on the availability of the web resources cited by that
content. Memento makes it possible to automate discovery of archived resources
and to consider the time between the publication of the research and the
archiving of the referenced URLs. This automation allows us to process more
than 160000 URLs, the largest known such study, and the repository metadata
allows consideration of the results by discipline. The results are startling:
45% (66096) of the URLs referenced from arXiv still exist, but are not
preserved for future generations, and 28% of resources referenced by UNT papers
have been lost. Moving forwards, we provide some initial recommendations,
including that repositories should publish URL lists extracted from papers that
could be used as seeds for web archiving systems.
",2.0
777,0,8707,1105.5464,learning to rank with partitioned preference,"W. W. Cohen, R. E. Schapire, Y. Singer",Learning to Order Things,"  There are many applications in which it is desirable to order rather than
classify instances. Here we consider the problem of learning how to order
instances given feedback in the form of preference judgments, i.e., statements
to the effect that one instance should be ranked ahead of another. We outline a
two-stage approach in which one first learns by conventional means a binary
preference function indicating whether it is advisable to rank one instance
before another. Here we consider an on-line algorithm for learning preference
functions that is based on Freund and Schapire's 'Hedge' algorithm. In the
second stage, new instances are ordered so as to maximize agreement with the
learned preference function. We show that the problem of finding the ordering
that agrees best with a learned preference function is NP-complete.
Nevertheless, we describe simple greedy algorithms that are guaranteed to find
a good approximation. Finally, we show how metasearch can be formulated as an
ordering problem, and present experimental results on learning a combination of
'search experts', each of which is a domain-specific query expansion strategy
for a web search engine.
",3.0
778,14,155508,2105.02935,text summarization model,Vedant Bahel and Achamma Thomas,Text similarity analysis for evaluation of descriptive answers,"  Keeping in mind the necessity of intelligent system in educational sector,
this paper proposes a text analysis based automated approach for automatic
evaluation of the descriptive answers in an examination. In particular, the
research focuses on the use of intelligent concepts of Natural Language
Processing and Data Mining for computer aided examination evaluation system.
The paper present an architecture for fair evaluation of answer sheet. In this
architecture, the examiner creates a sample answer sheet for given sets of
question. By using the concept of text summarization, text semantics and
keywords summarization, the final score for each answer is calculated. The text
similarity model is based on Siamese Manhattan LSTM (MaLSTM). The results of
this research were compared to manually graded assignments and other existing
system. This approach was found to be very efficient in order to be implemented
in an institution or in an university.
",3.0
779,6,193596,2204.11314,query expansion for imformation retrieval,"Antonio Mallia, Joel Mackenzie, Torsten Suel, Nicola Tonellotto",Faster Learned Sparse Retrieval with Guided Traversal,"  Neural information retrieval architectures based on transformers such as BERT
are able to significantly improve system effectiveness over traditional sparse
models such as BM25. Though highly effective, these neural approaches are very
expensive to run, making them difficult to deploy under strict latency
constraints. To address this limitation, recent studies have proposed new
families of learned sparse models that try to match the effectiveness of
learned dense models, while leveraging the traditional inverted index data
structure for efficiency. Current learned sparse models learn the weights of
terms in documents and, sometimes, queries; however, they exploit different
vocabulary structures, document expansion techniques, and query expansion
strategies, which can make them slower than traditional sparse models such as
BM25. In this work, we propose a novel indexing and query processing technique
that exploits a traditional sparse model's ""guidance"" to efficiently traverse
the index, allowing the more effective learned model to execute fewer scoring
operations. Our experiments show that our guided processing heuristic is able
to boost the efficiency of the underlying learned sparse model by a factor of
four without any measurable loss of effectiveness.
",1.0
780,7,95842,1908.06951,gradient boosting,"Zhiyuan He, Danchen Lin, Thomas Lau, and Mike Wu",Gradient Boosting Machine: A Survey,"  In this survey, we discuss several different types of gradient boosting
algorithms and illustrate their mathematical frameworks in detail: 1.
introduction of gradient boosting leads to 2. objective function optimization,
3. loss function estimations, and 4. model constructions. 5. application of
boosting in ranking.
",5.0
781,10,49949,1702.00619,web archive,"Tarcisio Souza and Elena Demidova and Thomas Risse and Helge Holzmann
  and Gerhard Gossen and Julian Szymanski","Semantic URL Analytics to Support Efficient Annotation of Large Scale
  Web Archives","  Long-term Web archives comprise Web documents gathered over longer time
periods and can easily reach hundreds of terabytes in size. Semantic
annotations such as named entities can facilitate intelligent access to the Web
archive data. However, the annotation of the entire archive content on this
scale is often infeasible. The most efficient way to access the documents
within Web archives is provided through their URLs, which are typically stored
in dedicated index files.The URLs of the archived Web documents can contain
semantic information and can offer an efficient way to obtain initial semantic
annotations for the archived documents. In this paper, we analyse the
applicability of semantic analysis techniques such as named entity extraction
to the URLs in a Web archive. We evaluate the precision of the named entity
extraction from the URLs in the Popular German Web dataset and analyse the
proportion of the archived URLs from 1,444 popular domains in the time interval
from 2000 to 2012 to which these techniques are applicable. Our results
demonstrate that named entity recognition can be successfully applied to a
large number of URLs in our Web archive and provide a good starting point to
efficiently annotate large scale collections of Web documents.
",3.0
782,18,108481,2001.08677,infomation retrieval time complexity,"Paulo Rocha, Diego Pinheiro, Martin Cadeiras, Carmelo Bastos-Filho","Towards Automatic Clustering Analysis using Traces of Information Gain:
  The InfoGuide Method","  Clustering analysis has become a ubiquitous information retrieval tool in a
wide range of domains, but a more automatic framework is still lacking. Though
internal metrics are the key players towards a successful retrieval of
clusters, their effectiveness on real-world datasets remains not fully
understood, mainly because of their unrealistic assumptions underlying
datasets. We hypothesized that capturing {\it traces of information gain}
between increasingly complex clustering retrievals---{\it InfoGuide}---enables
an automatic clustering analysis with improved clustering retrievals. We
validated the {\it InfoGuide} hypothesis by capturing the traces of information
gain using the Kolmogorov-Smirnov statistic and comparing the clusters
retrieved by {\it InfoGuide} against those retrieved by other commonly used
internal metrics in artificially-generated, benchmarks, and real-world
datasets. Our results suggested that {\it InfoGuide} can enable a more
automatic clustering analysis and may be more suitable for retrieving clusters
in real-world datasets displaying nontrivial statistical properties.
",1.0
783,0,63875,1802.07606,learning to rank with partitioned preference,"Luisa M Zintgraf, Diederik M Roijers, Sjoerd Linders, Catholijn M
  Jonker, Ann Now\'e","Ordered Preference Elicitation Strategies for Supporting Multi-Objective
  Decision Making","  In multi-objective decision planning and learning, much attention is paid to
producing optimal solution sets that contain an optimal policy for every
possible user preference profile. We argue that the step that follows, i.e,
determining which policy to execute by maximising the user's intrinsic utility
function over this (possibly infinite) set, is under-studied. This paper aims
to fill this gap. We build on previous work on Gaussian processes and pairwise
comparisons for preference modelling, extend it to the multi-objective decision
support scenario, and propose new ordered preference elicitation strategies
based on ranking and clustering. Our main contribution is an in-depth
evaluation of these strategies using computer and human-based experiments. We
show that our proposed elicitation strategies outperform the currently used
pairwise methods, and found that users prefer ranking most. Our experiments
further show that utilising monotonicity information in GPs by using a linear
prior mean at the start and virtual comparisons to the nadir and ideal points,
increases performance. We demonstrate our decision support framework in a
real-world study on traffic regulation, conducted with the city of Amsterdam.
",0.0
784,6,155733,2105.03938,query expansion for imformation retrieval,"Chen Qu, Hamed Zamani, Liu Yang, W. Bruce Croft and Erik
  Learned-Miller",Passage Retrieval for Outside-Knowledge Visual Question Answering,"  In this work, we address multi-modal information needs that contain text
questions and images by focusing on passage retrieval for outside-knowledge
visual question answering. This task requires access to outside knowledge,
which in our case we define to be a large unstructured passage collection. We
first conduct sparse retrieval with BM25 and study expanding the question with
object names and image captions. We verify that visual clues play an important
role and captions tend to be more informative than object names in sparse
retrieval. We then construct a dual-encoder dense retriever, with the query
encoder being LXMERT, a multi-modal pre-trained transformer. We further show
that dense retrieval significantly outperforms sparse retrieval that uses
object expansion. Moreover, dense retrieval matches the performance of sparse
retrieval that leverages human-generated captions.
",1.0
785,6,86198,1904.06672,query expansion for imformation retrieval,"Ahsaas Bajaj, Shubham Krishna, Mukund Rungta, Hemant Tiwari and Vanraj
  Vala","RelEmb: A relevance-based application embedding for Mobile App retrieval
  and categorization","  Information Retrieval Systems have revolutionized the organization and
extraction of Information. In recent years, mobile applications (apps) have
become primary tools of collecting and disseminating information. However,
limited research is available on how to retrieve and organize mobile apps on
users' devices. In this paper, authors propose a novel method to estimate
app-embeddings which are then applied to tasks like app clustering,
classification, and retrieval. Usage of app-embedding for query expansion,
nearest neighbor analysis enables unique and interesting use cases to enhance
end-user experience with mobile apps.
",2.0
786,4,189096,2203.06918,pre-trained language model,"Daeyoung Kim, Seongsu Bae, Seungho Kim, Edward Choi","Uncertainty-Aware Text-to-Program for Question Answering on Structured
  Electronic Health Records","  Question Answering on Electronic Health Records (EHR-QA) has a significant
impact on the healthcare domain, and it is being actively studied. Previous
research on structured EHR-QA focuses on converting natural language queries
into query language such as SQL or SPARQL (NLQ2Query), so the problem scope is
limited to pre-defined data types by the specific query language. In order to
expand the EHR-QA task beyond this limitation to handle multi-modal medical
data and solve complex inference in the future, more primitive systemic
language is needed. In this paper, we design the program-based model
(NLQ2Program) for EHR-QA as the first step towards the future direction. We
tackle MIMICSPARQL*, the graph-based EHR-QA dataset, via a program-based
approach in a semi-supervised manner in order to overcome the absence of gold
programs. Without the gold program, our proposed model shows comparable
performance to the previous state-of-the-art model, which is an NLQ2Query model
(0.9% gain). In addition, for a reliable EHR-QA model, we apply the uncertainty
decomposition method to measure the ambiguity in the input question. We
empirically confirmed data uncertainty is most indicative of the ambiguity in
the input question.
",4.0
787,6,43732,1606.07608,query expansion for imformation retrieval,"Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, Utpal Garain",Using Word Embeddings for Automatic Query Expansion,"  In this paper a framework for Automatic Query Expansion (AQE) is proposed
using distributed neural language model word2vec. Using semantic and contextual
relation in a distributed and unsupervised framework, word2vec learns a low
dimensional embedding for each vocabulary entry. Using such a framework, we
devise a query expansion technique, where related terms to a query are obtained
by K-nearest neighbor approach. We explore the performance of the AQE methods,
with and without feedback query expansion, and a variant of simple K-nearest
neighbor in the proposed framework. Experiments on standard TREC ad-hoc data
(Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with
query set 451-550) shows significant improvement over standard term-overlapping
based retrieval methods. However the proposed method fails to achieve
comparable performance with statistical co-occurrence based feedback method
such as RM3. We have also found that the word2vec based query expansion methods
perform similarly with and without any feedback information.
",5.0
788,12,62130,1801.04437,COVID-19 and social media,Rodrigo Costas,"Towards the social media studies of science: social media metrics,
  present and future","  In this paper we aim at providing a general reflection around the present and
future of social media metrics (or altmetrics) and how they could evolve into a
new discipline focused on the study of the relationships and interactions
between science and social media, in what could be seen as the social media
studies of science.
",1.0
789,8,84715,1903.08889,node embedding for graph,Uriel Singer and Ido Guy and Kira Radinsky,Node Embedding over Temporal Graphs,"  In this work, we present a method for node embedding in temporal graphs. We
propose an algorithm that learns the evolution of a temporal graph's nodes and
edges over time and incorporates this dynamics in a temporal node embedding
framework for different graph prediction tasks. We present a joint loss
function that creates a temporal embedding of a node by learning to combine its
historical temporal embeddings, such that it optimizes per given task (e.g.,
link prediction). The algorithm is initialized using static node embeddings,
which are then aligned over the representations of a node at different time
points, and eventually adapted for the given task in a joint optimization. We
evaluate the effectiveness of our approach over a variety of temporal graphs
for the two fundamental tasks of temporal link prediction and multi-label node
classification, comparing to competitive baselines and algorithmic
alternatives. Our algorithm shows performance improvements across many of the
datasets and baselines and is found particularly effective for graphs that are
less cohesive, with a lower clustering coefficient.
",5.0
790,9,110891,2002.08131,language model for long documents,"Jeremy Barnes and Vinit Ravishankar and Lilja {\O}vrelid and Erik
  Velldal","A Systematic Comparison of Architectures for Document-Level Sentiment
  Classification","  Documents are composed of smaller pieces - paragraphs, sentences, and tokens
- that have complex relationships between one another. Sentiment classification
models that take into account the structure inherent in these documents have a
theoretical advantage over those that do not. At the same time, transfer
learning models based on language model pretraining have shown promise for
document classification. However, these two paradigms have not been
systematically compared and it is not clear under which circumstances one
approach is better than the other. In this work we empirically compare
hierarchical models and transfer learning for document-level sentiment
classification. We show that non-trivial hierarchical models outperform
previous baselines and transfer learning on document-level sentiment
classification in five languages.
",1.0
791,3,58284,1710.01077,database management system,"S{\o}ren Kejser Jensen, Torben Bach Pedersen, Christian Thomsen",Time Series Management Systems: A Survey,"  The collection of time series data increases as more monitoring and
automation are being deployed. These deployments range in scale from an
Internet of things (IoT) device located in a household to enormous distributed
Cyber-Physical Systems (CPSs) producing large volumes of data at high velocity.
To store and analyze these vast amounts of data, specialized Time Series
Management Systems (TSMSs) have been developed to overcome the limitations of
general purpose Database Management Systems (DBMSs) for times series
management. In this paper, we present a thorough analysis and classification of
TSMSs developed through academic or industrial research and documented through
publications. Our classification is organized into categories based on the
architectures observed during our analysis. In addition, we provide an overview
of each system with a focus on the motivational use case that drove the
development of the system, the functionality for storage and querying of time
series a system implements, the components the system is composed of, and the
capabilities of each system with regard to Stream Processing and Approximate
Query Processing (AQP). Last, we provide a summary of research directions
proposed by other researchers in the field and present our vision for a next
generation TSMS.
",4.0
792,19,52185,1704.02114,artificial intelligence for low carbon,"Utku Kose, Selcuk Sert","Improving content marketing processes with the approaches by artificial
  intelligence","  Content marketing is todays one of the most remarkable approaches in the
context of marketing processes of companies. Value of this kind of marketing
has improved in time, thanks to the latest developments regarding to computer
and communication technologies. Nowadays, especially social media based
platforms have a great importance on enabling companies to design multimedia
oriented, interactive content. But on the other hand, there is still something
more to do for improved content marketing approaches. In this context,
objective of this study is to focus on intelligent content marketing, which can
be done by using artificial intelligence. Artificial Intelligence is todays one
of the most remarkable research fields and it can be used easily as
multidisciplinary. So, this study has aimed to discuss about its potential on
improving content marketing. In detail, the study has enabled readers to
improve their awareness about the intersection point of content marketing and
artificial intelligence. Furthermore, the authors have introduced some example
models of intelligent content marketing, which can be achieved by using current
Web technologies and artificial intelligence techniques.
",1.0
793,19,154527,2104.13155,artificial intelligence for low carbon,"Li Weigang, Liriam Enamoto, Denise Leyi Li, Geraldo Pereira Rocha
  Filho","Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","  This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.
",0.0
794,11,25275,1402.72,PageRank for web search,"Leena Giri G, Srikanth P L, S H Manjula, K R Venugopal, L M Patnaik","Mathematical Model of Semantic Look - An Efficient Context Driven Search
  Engine","  The WorldWideWeb (WWW) is a huge conservatory of web pages. Search Engines
are key applications that fetch web pages for the user query. In the current
generation web architecture, search engines treat keywords provided by the user
as isolated keywords without considering the context of the user query. This
results in a lot of unrelated pages or links being displayed to the user.
Semantic Web is based on the current web with a revised framework to display a
more precise result set as response to a user query. The current web pages need
to be annotated by finding relevant meta data to be added to each of them, so
that they become useful to Semantic Web search engines. Semantic Look explores
the context of user query by processing the Semantic information recorded in
the web pages. It is compared with an existing algorithm called OntoLook and it
is shown that Semantic Look is a better optimized search engine by being more
than twice as fast as OntoLook.
",1.0
795,10,48608,1612.05413,web archive,Gerhard Gossen and Elena Demidova and Thomas Risse,Analyzing Web Archives Through Topic and Event Focused Sub-collections,"  Web archives capture the history of the Web and are therefore an important
source to study how societal developments have been reflected on the Web.
However, the large size of Web archives and their temporal nature pose many
challenges to researchers interested in working with these collections. In this
work, we describe the challenges of working with Web archives and propose the
research methodology of extracting and studying sub-collections of the archive
focused on specific topics and events. We discuss the opportunities and
challenges of this approach and suggest a framework for creating
sub-collections.
",5.0
796,15,79595,1812.0887,relevance feedback for imformation retrieval,"Keping Bi, Qingyao Ai and W. Bruce Croft","Iterative Relevance Feedback for Answer Passage Retrieval with
  Passage-level Semantic Match","  Relevance feedback techniques assume that users provide relevance judgments
for the top k (usually 10) documents and then re-rank using a new query model
based on those judgments. Even though this is effective, there has been little
research recently on this topic because requiring users to provide substantial
feedback on a result list is impractical in a typical web search scenario. In
new environments such as voice-based search with smart home devices, however,
feedback about result quality can potentially be obtained during users'
interactions with the system. Since there are severe limitations on the length
and number of results that can be presented in a single interaction in this
environment, the focus should move from browsing result lists to iterative
retrieval and from retrieving documents to retrieving answers. In this paper,
we study iterative relevance feedback techniques with a focus on retrieving
answer passages. We first show that iterative feedback is more effective than
the top-k approach for answer retrieval. Then we propose an iterative feedback
model based on passage-level semantic match and show that it can produce
significant improvements compared to both word-based iterative feedback models
and those based on term-level semantic similarity.
",5.0
797,13,55261,1707.01184,social network analysis with natrual language processing,"Souvick Ghosh, Satanu Ghosh, and Dipankar Das",Sentiment Identification in Code-Mixed Social Media Text,"  Sentiment analysis is the Natural Language Processing (NLP) task dealing with
the detection and classification of sentiments in texts. While some tasks deal
with identifying the presence of sentiment in the text (Subjectivity analysis),
other tasks aim at determining the polarity of the text categorizing them as
positive, negative and neutral. Whenever there is a presence of sentiment in
the text, it has a source (people, group of people or any entity) and the
sentiment is directed towards some entity, object, event or person. Sentiment
analysis tasks aim to determine the subject, the target and the polarity or
valence of the sentiment. In our work, we try to automatically extract
sentiment (positive or negative) from Facebook posts using a machine learning
approach.While some works have been done in code-mixed social media data and in
sentiment analysis separately, our work is the first attempt (as of now) which
aims at performing sentiment analysis of code-mixed social media text. We have
used extensive pre-processing to remove noise from raw text. Multilayer
Perceptron model has been used to determine the polarity of the sentiment. We
have also developed the corpus for this task by manually labeling Facebook
posts with their associated sentiments.
",3.0
798,5,171023,2109.15154,matrix completion,"Anish Agarwal, Munther Dahleh, Devavrat Shah, Dennis Shen",Causal Matrix Completion,"  Matrix completion is the study of recovering an underlying matrix from a
sparse subset of noisy observations. Traditionally, it is assumed that the
entries of the matrix are ""missing completely at random"" (MCAR), i.e., each
entry is revealed at random, independent of everything else, with uniform
probability. This is likely unrealistic due to the presence of ""latent
confounders"", i.e., unobserved factors that determine both the entries of the
underlying matrix and the missingness pattern in the observed matrix. For
example, in the context of movie recommender systems -- a canonical application
for matrix completion -- a user who vehemently dislikes horror films is
unlikely to ever watch horror films. In general, these confounders yield
""missing not at random"" (MNAR) data, which can severely impact any inference
procedure that does not correct for this bias. We develop a formal causal model
for matrix completion through the language of potential outcomes, and provide
novel identification arguments for a variety of causal estimands of interest.
We design a procedure, which we call ""synthetic nearest neighbors"" (SNN), to
estimate these causal estimands. We prove finite-sample consistency and
asymptotic normality of our estimator. Our analysis also leads to new
theoretical results for the matrix completion literature. In particular, we
establish entry-wise, i.e., max-norm, finite-sample consistency and asymptotic
normality results for matrix completion with MNAR data. As a special case, this
also provides entry-wise bounds for matrix completion with MCAR data. Across
simulated and real data, we demonstrate the efficacy of our proposed estimator.
",4.0
799,4,208012,2208.14554,pre-trained language model,James A. Michaelov and Benjamin K. Bergen,"Do language models make human-like predictions about the coreferents of
  Italian anaphoric zero pronouns?","  Some languages allow arguments to be omitted in certain contexts. Yet human
language comprehenders reliably infer the intended referents of these zero
pronouns, in part because they construct expectations about which referents are
more likely. We ask whether Neural Language Models also extract the same
expectations. We test whether 12 contemporary language models display
expectations that reflect human behavior when exposed to sentences with zero
pronouns from five behavioral experiments conducted in Italian by Carminati
(2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the
human behavior from all the experiments, with others successfully modeling some
of the results. This result suggests that human expectations about coreference
can be derived from exposure to language, and also indicates features of
language models that allow them to better reflect human behavior.
",5.0
800,15,59345,1711.00248,relevance feedback for imformation retrieval,"Zhuoxiang Chen, Zhe Xu, Ya Zhang, Xiao Gu",Query-free Clothing Retrieval via Implicit Relevance Feedback,"  Image-based clothing retrieval is receiving increasing interest with the
growth of online shopping. In practice, users may often have a desired piece of
clothing in mind (e.g., either having seen it before on the street or requiring
certain specific clothing attributes) but may be unable to supply an image as a
query. We model this problem as a new type of image retrieval task in which the
target image resides only in the user's mind (called ""mental image retrieval""
hereafter). Because of the absence of an explicit query image, we propose to
solve this problem through relevance feedback. Specifically, a new Bayesian
formulation is proposed that simultaneously models the retrieval target and its
high-level representation in the mind of the user (called the ""user metric""
hereafter) as posterior distributions of pre-fetched shop images and
heterogeneous features extracted from multiple clothing attributes,
respectively. Requiring only clicks as user feedback, the proposed algorithm is
able to account for the variability in human decision-making. Experiments with
real users demonstrate the effectiveness of the proposed algorithm.
",4.0
801,11,9156,1106.6215,PageRank for web search,"Leonardo Ermann, Alexei D. Chepelianskii and Dima L. Shepelyansky",Towards two-dimensional search engines,"  We study the statistical properties of various directed networks using
ranking of their nodes based on the dominant vectors of the Google matrix known
as PageRank and CheiRank. On average PageRank orders nodes proportionally to a
number of ingoing links, while CheiRank orders nodes proportionally to a number
of outgoing links. In this way the ranking of nodes becomes two-dimensional
that paves the way for development of two-dimensional search engines of new
type. Statistical properties of information flow on PageRank-CheiRank plane are
analyzed for networks of British, French and Italian Universities, Wikipedia,
Linux Kernel, gene regulation and other networks. A special emphasis is done
for British Universities networks using the large database publicly available
at UK. Methods of spam links control are also analyzed.
",4.0
802,15,29087,1410.0471,relevance feedback for imformation retrieval,"Zakria Hussain, Arto Klami, Jussi Kujala, Alex P. Leung, Kitsuchart
  Pasupa, Peter Auer, Samuel Kaski, Jorma Laaksonen and John Shawe-Taylor",PinView: Implicit Feedback in Content-Based Image Retrieval,"  This paper describes PinView, a content-based image retrieval system that
exploits implicit relevance feedback collected during a search session. PinView
contains several novel methods to infer the intent of the user. From relevance
feedback, such as eye movements or pointer clicks, and visual features of
images, PinView learns a similarity metric between images which depends on the
current interests of the user. It then retrieves images with a specialized
online learning algorithm that balances the tradeoff between exploring new
images and exploiting the already inferred interests of the user. We have
integrated PinView to the content-based image retrieval system PicSOM, which
enables applying PinView to real-world image databases. With the new algorithms
PinView outperforms the original PicSOM, and in online experiments with real
users the combination of implicit and explicit feedback gives the best results.
",4.0
803,15,23241,1311.5013,relevance feedback for imformation retrieval,"Srivatsan Sridharan, Kausal Malladi, Yamini Muralitharan","Data Mining Model for the Data Retrieval from Central Server
  Configuration","  A server, which is to keep track of heavy document traffic, is unable to
filter the documents that are most relevant and updated for continuous text
search queries. This paper focuses on handling continuous text extraction
sustaining high document traffic. The main objective is to retrieve recent
updated documents that are most relevant to the query by applying sliding
window technique. Our solution indexes the streamed documents in the main
memory with structure based on the principles of inverted file, and processes
document arrival and expiration events with incremental threshold-based method.
It also ensures elimination of duplicate document retrieval using unsupervised
duplicate detection. The documents are ranked based on user feedback and given
higher priority for retrieval.
",1.0
804,11,49983,1702.01076,PageRank for web search,"Helge Holzmann, Avishek Anand",Tempas: Temporal Archive Search Based on Tags,"  Limited search and access patterns over Web archives have been well
documented. One of the key reasons is the lack of understanding of the user
access patterns over such collections, which in turn is attributed to the lack
of effective search interfaces. Current search interfaces for Web archives are
(a) either purely navigational or (b) have sub-optimal search experience due to
ineffective retrieval models or query modeling. We identify that external
longitudinal resources, such as social bookmarking data, are crucial sources to
identify important and popular websites in the past. To this extent we present
Tempas, a tag-based temporal search engine for Web archives.
  Websites are posted at specific times of interest on several external
platforms, such as bookmarking sites like Delicious. Attached tags not only act
as relevant descriptors useful for retrieval, but also encode the time of
relevance. With Tempas we tackle the challenge of temporally searching a Web
archive by indexing tags and time. We allow temporal selections for search
terms, rank documents based on their popularity and also provide meaningful
query recommendations by exploiting tag-tag and tag-document co-occurrence
statistics in arbitrary time windows. Finally, Tempas operates as a fairly
non-invasive indexing framework. By not dealing with contents from the actual
Web archive it constitutes an attractive and low-overhead approach for quick
access into Web archives.
",1.0
805,7,211141,2209.15146,gradient boosting,"Gideon Vos, Kelly Trinh, Zoltan Sarnyai, Mostafa Rahimi Azghadi","Ensemble Machine Learning Model Trained on a New Synthesized Dataset
  Generalizes Well for Stress Prediction Using Wearable Devices","  Introduction. We investigate the generalization ability of models built on
datasets containing a small number of subjects, recorded in single study
protocols. Next, we propose and evaluate methods combining these datasets into
a single, large dataset. Finally, we propose and evaluate the use of ensemble
techniques by combining gradient boosting with an artificial neural network to
measure predictive power on new, unseen data.
  Methods. Sensor biomarker data from six public datasets were utilized in this
study. To test model generalization, we developed a gradient boosting model
trained on one dataset (SWELL), and tested its predictive power on two datasets
previously used in other studies (WESAD, NEURO). Next, we merged four small
datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of
99 subjects,. In addition, we utilized random sampling combined with another
dataset (EXAM) to build a larger training dataset consisting of 200 synthesized
subjects,. Finally, we developed an ensemble model that combines our gradient
boosting model with an artificial neural network, and tested it on two
additional, unseen publicly available stress datasets (WESAD and Toadstool).
  Results. Our method delivers a robust stress measurement system capable of
achieving 85% predictive accuracy on new, unseen validation data, achieving a
25% performance improvement over single models trained on small datasets.
  Conclusion. Models trained on small, single study protocol datasets do not
generalize well for use on new, unseen data and lack statistical power.
Ma-chine learning models trained on a dataset containing a larger number of
varied study subjects capture physiological variance better, resulting in more
robust stress detection.
",4.0
806,7,122504,2006.09745,gradient boosting,"Thomas Parnell, Andreea Anghel, Malgorzata Lazuka, Nikolas Ioannou,
  Sebastian Kurella, Peshal Agarwal, Nikolaos Papandreou, Haralampos Pozidis",SnapBoost: A Heterogeneous Boosting Machine,"  Modern gradient boosting software frameworks, such as XGBoost and LightGBM,
implement Newton descent in a functional space. At each boosting iteration,
their goal is to find the base hypothesis, selected from some base hypothesis
class, that is closest to the Newton descent direction in a Euclidean sense.
Typically, the base hypothesis class is fixed to be all binary decision trees
up to a given depth. In this work, we study a Heterogeneous Newton Boosting
Machine (HNBM) in which the base hypothesis class may vary across boosting
iterations. Specifically, at each boosting iteration, the base hypothesis class
is chosen, from a fixed set of subclasses, by sampling from a probability
distribution. We derive a global linear convergence rate for the HNBM under
certain assumptions, and show that it agrees with existing rates for Newton's
method when the Newton direction can be perfectly fitted by the base hypothesis
at each boosting iteration. We then describe a particular realization of a
HNBM, SnapBoost, that, at each boosting iteration, randomly selects between
either a decision tree of variable depth or a linear regressor with random
Fourier features. We describe how SnapBoost is implemented, with a focus on the
training complexity. Finally, we present experimental results, using OpenML and
Kaggle datasets, that show that SnapBoost is able to achieve better
generalization loss than competing boosting frameworks, without taking
significantly longer to tune.
",4.0
807,4,135145,2010.1059,pre-trained language model,Yugam Bajaj and Puru Malhotra,American Sign Language Identification Using Hand Trackpoint Analysis,"  Sign Language helps people with Speaking and Hearing Disabilities communicate
with others efficiently. Sign Language identification is a challenging area in
the field of computer vision and recent developments have been able to achieve
near perfect results for the task, though some challenges are yet to be solved.
In this paper we propose a novel machine learning based pipeline for American
Sign Language identification using hand track points. We convert a hand gesture
into a series of hand track point coordinates that serve as an input to our
system. In order to make the solution more efficient, we experimented with 28
different combinations of pre-processing techniques, each run on three
different machine learning algorithms namely k-Nearest Neighbours, Random
Forests and a Neural Network. Their performance was contrasted to determine the
best pre-processing scheme and algorithm pair. Our system achieved an Accuracy
of 95.66% to identify American sign language gestures.
",5.0
808,7,8536,1105.2054,gradient boosting,Alexander Grubb and J. Andrew Bagnell,Generalized Boosting Algorithms for Convex Optimization,"  Boosting is a popular way to derive powerful learners from simpler hypothesis
classes. Following previous work (Mason et al., 1999; Friedman, 2000) on
general boosting frameworks, we analyze gradient-based descent algorithms for
boosting with respect to any convex objective and introduce a new measure of
weak learner performance into this setting which generalizes existing work. We
present the weak to strong learning guarantees for the existing gradient
boosting work for strongly-smooth, strongly-convex objectives under this new
measure of performance, and also demonstrate that this work fails for
non-smooth objectives. To address this issue, we present new algorithms which
extend this boosting approach to arbitrary convex loss functions and give
corresponding weak to strong convergence results. In addition, we demonstrate
experimental results that support our analysis and demonstrate the need for the
new algorithms we present.
",3.0
809,4,152901,2104.05218,pre-trained language model,Kevin Yang and Dan Klein,FUDGE: Controlled Text Generation With Future Discriminators,"  We propose Future Discriminators for Generation (FUDGE), a flexible and
modular method for controlled text generation. Given a pre-existing model G for
generating text from a distribution of interest, FUDGE enables conditioning on
a desired attribute a (for example, formality) while requiring access only to
G's output logits. FUDGE learns an attribute predictor operating on a partial
sequence, and uses this predictor's outputs to adjust G's original
probabilities. We show that FUDGE models terms corresponding to a Bayesian
decomposition of the conditional distribution of G given attribute a. Moreover,
FUDGE can easily compose predictors for multiple desired attributes. We
evaluate FUDGE on three tasks -- couplet completion in poetry, topic control in
language generation, and formality change in machine translation -- and observe
gains in all three tasks.
",3.0
810,19,183403,2201.10035,artificial intelligence for low carbon,"Alexander Thebelt, Johannes Wiebe, Jan Kronqvist, Calvin Tsay, Ruth
  Misener","Maximizing information from chemical engineering data sets: Applications
  to machine learning","  It is well-documented how artificial intelligence can have (and already is
having) a big impact on chemical engineering. But classical machine learning
approaches may be weak for many chemical engineering applications. This review
discusses how challenging data characteristics arise in chemical engineering
applications. We identify four characteristics of data arising in chemical
engineering applications that make applying classical artificial intelligence
approaches difficult: (1) high variance, low volume data, (2) low variance,
high volume data, (3) noisy/corrupt/missing data, and (4) restricted data with
physics-based limitations. For each of these four data characteristics, we
discuss applications where these data characteristics arise and show how
current chemical engineering research is extending the fields of data science
and machine learning to incorporate these challenges. Finally, we identify
several challenges for future research.
",1.0
811,5,52897,1705.00399,matrix completion,Natali Ruchansky and Mark Crovella and Evimaria Terzi,Matrix completion with queries,"  In many applications, e.g., recommender systems and traffic monitoring, the
data comes in the form of a matrix that is only partially observed and low
rank. A fundamental data-analysis task for these datasets is matrix completion,
where the goal is to accurately infer the entries missing from the matrix. Even
when the data satisfies the low-rank assumption, classical matrix-completion
methods may output completions with significant error -- in that the
reconstructed matrix differs significantly from the true underlying matrix.
Often, this is due to the fact that the information contained in the observed
entries is insufficient. In this work, we address this problem by proposing an
active version of matrix completion, where queries can be made to the true
underlying matrix. Subsequently, we design Order&Extend, which is the first
algorithm to unify a matrix-completion approach and a querying strategy into a
single algorithm. Order&Extend is able identify and alleviate insufficient
information by judiciously querying a small number of additional entries. In an
extensive experimental evaluation on real-world datasets, we demonstrate that
our algorithm is efficient and is able to accurately reconstruct the true
matrix while asking only a small number of queries.
",5.0
812,0,188073,2203.01746,learning to rank with partitioned preference,"Phuc Thai, My T. Thai, Tam Vu, Thang N. Dinh",SaPHyRa: A Learning Theory Approach to Ranking Nodes in Large Networks,"  Ranking nodes based on their centrality stands a fundamental, yet,
challenging problem in large-scale networks. Approximate methods can quickly
estimate nodes' centrality and identify the most central nodes, but the ranking
for the majority of remaining nodes may be meaningless. For example, ranking
for less-known websites in search queries is known to be noisy and unstable. To
this end, we investigate a new node ranking problem with two important
distinctions: a) ranking quality, rather than the centrality estimation
quality, as the primary objective; and b) ranking only nodes of interest, e.g.,
websites that matched search criteria. We propose Sample space Partitioning
Hypothesis Ranking, or SaPHyRa, that transforms node ranking into a hypothesis
ranking in machine learning. This transformation maps nodes' centrality to the
expected risks of hypotheses, opening doors for theoretical machine learning
(ML) tools. The key of SaPHyRa is to partition the sample space into exact and
approximate subspaces. The exact subspace contains samples related to the nodes
of interest, increasing both estimation and ranking qualities. The approximate
space can be efficiently sampled with ML-based techniques to provide
theoretical guarantees on the estimation error. Lastly, we present SaPHyRa_bc,
an illustration of SaPHyRa on ranking nodes' betweenness centrality (BC). By
combining a novel bi-component sampling, a 2-hop sample partitioning, and
improved bounds on the Vapnik-Chervonenkis dimension, SaPHyRa_bc can
effectively rank any node subset in BC. Its performance is up to 200x faster
than state-of-the-art methods in approximating BC, while its rank correlation
to the ground truth is improved by multifold.
",0.0
813,7,147482,2102.09305,gradient boosting,"Elad Hazan, Karan Singh",Boosting for Online Convex Optimization,"  We consider the decision-making framework of online convex optimization with
a very large number of experts. This setting is ubiquitous in contextual and
reinforcement learning problems, where the size of the policy class renders
enumeration and search within the policy class infeasible.
  Instead, we consider generalizing the methodology of online boosting. We
define a weak learning algorithm as a mechanism that guarantees
multiplicatively approximate regret against a base class of experts. In this
access model, we give an efficient boosting algorithm that guarantees
near-optimal regret against the convex hull of the base class. We consider both
full and partial (a.k.a. bandit) information feedback models. We also give an
analogous efficient boosting algorithm for the i.i.d. statistical setting.
  Our results simultaneously generalize online boosting and gradient boosting
guarantees to contextual learning model, online convex optimization and bandit
linear optimization settings.
",3.0
814,0,30827,1501.00199,learning to rank with partitioned preference,"Alex Beutel, Amr Ahmed and Alexander J. Smola",ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly,"  Matrix completion and approximation are popular tools to capture a user's
preferences for recommendation and to approximate missing data. Instead of
using low-rank factorization we take a drastically different approach, based on
the simple insight that an additive model of co-clusterings allows one to
approximate matrices efficiently. This allows us to build a concise model that,
per bit of model learned, significantly beats all factorization approaches to
matrix approximation. Even more surprisingly, we find that summing over small
co-clusterings is more effective in modeling matrices than classic
co-clustering, which uses just one large partitioning of the matrix.
  Following Occam's razor principle suggests that the simple structure induced
by our model better captures the latent preferences and decision making
processes present in the real world than classic co-clustering or matrix
factorization. We provide an iterative minimization algorithm, a collapsed
Gibbs sampler, theoretical guarantees for matrix approximation, and excellent
empirical evidence for the efficacy of our approach. We achieve
state-of-the-art results on the Netflix problem with a fraction of the model
complexity.
",0.0
815,1,22146,1309.4345,advanced search engine,"M. Brzezi\'nski-Spiczak, K. Dobosz, M. Lis, M. Pintal",Music Files Search System,"  This paper introduces a project of advanced system of music retrieval from
the Internet. The system uses combination of text search (by author, title and
other information about the music file included in id3 tag description or
similar for other file types) with more intuitive and novel method of melody
search using query by humming. The patterns for storing text and melody
information as well as improved clustering algorithm for the pattern space were
proposed. The search engine is planned to optimise the query due to the data
input by user, thanks to the structure of text and melody index database. The
system is planned to be a plug-in for popular digital music players or an
independent player. An advanced system of recommendation based on information
gathered from user's profile and search history is an integral part of the
system. The recommendation mechanism uses scrobbling methods and is responsible
for making suggestions of songs unknown to the user but similar to his
preferred music styles and positioning search results.
",5.0
816,8,110854,2002.07962,node embedding for graph,"Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan",Inductive Representation Learning on Temporal Graphs,"  Inductive representation learning on temporal graphs is an important step
toward salable machine learning on real-world dynamic networks. The evolving
nature of temporal dynamic graphs requires handling new nodes as well as
capturing temporal patterns. The node embeddings, which are now functions of
time, should represent both the static node features and the evolving
topological structures. Moreover, node and topological features can be temporal
as well, whose patterns the node embeddings should also capture. We propose the
temporal graph attention (TGAT) layer to efficiently aggregate
temporal-topological neighborhood features as well as to learn the time-feature
interactions. For TGAT, we use the self-attention mechanism as building block
and develop a novel functional time encoding technique based on the classical
Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network
recognizes the node embeddings as functions of time and is able to inductively
infer embeddings for both new and observed nodes as the graph evolves. The
proposed approach handles both node classification and link prediction task,
and can be naturally extended to include the temporal edge features. We
evaluate our method with transductive and inductive tasks under temporal
settings with two benchmark and one industrial dataset. Our TGAT model compares
favorably to state-of-the-art baselines as well as the previous temporal graph
embedding approaches.
",3.0
817,15,1929,809.4834,relevance feedback for imformation retrieval,"Jose Torres, Luis Paulo Reis",Relevance Feedback in Conceptual Image Retrieval: A User Evaluation,"  The Visual Object Information Retrieval (VOIR) system described in this paper
implements an image retrieval approach that combines two layers, the conceptual
and the visual layer. It uses terms from a textual thesaurus to represent the
conceptual information and also works with image regions, the visual
information. The terms are related with the image regions through a weighted
association enabling the execution of concept-level queries. VOIR uses
region-based relevance feedback to improve the quality of the results in each
query session and to discover new associations between text and image. This
paper describes a user-centred and task-oriented comparative evaluation of VOIR
which was undertaken considering three distinct versions of VOIR: a full-fledge
version; one supporting relevance feedback only at image level; and a third
version not supporting relevance feedback at all. The evaluation performed
showed the usefulness of region based relevance feedback in the context of VOIR
prototype.
",4.0
818,7,123433,2006.13346,gradient boosting,"Michael Rapp, Eneldo Loza Menc\'ia, Johannes F\""urnkranz, Vu-Linh
  Nguyen, Eyke H\""ullermeier",Learning Gradient Boosted Multi-label Classification Rules,"  In multi-label classification, where the evaluation of predictions is less
straightforward than in single-label classification, various meaningful, though
different, loss functions have been proposed. Ideally, the learning algorithm
should be customizable towards a specific choice of the performance measure.
Modern implementations of boosting, most prominently gradient boosted decision
trees, appear to be appealing from this point of view. However, they are mostly
limited to single-label classification, and hence not amenable to multi-label
losses unless these are label-wise decomposable. In this work, we develop a
generalization of the gradient boosting framework to multi-output problems and
propose an algorithm for learning multi-label classification rules that is able
to minimize decomposable as well as non-decomposable loss functions. Using the
well-known Hamming loss and subset 0/1 loss as representatives, we analyze the
abilities and limitations of our approach on synthetic data and evaluate its
predictive performance on multi-label benchmarks.
",4.0
819,16,117543,2005.00817,activation function in neutral networks,"Andrea Apicella, Francesco Donnarumma, Francesco Isgr\`o and Roberto
  Prevete",A survey on modern trainable activation functions,"  In neural networks literature, there is a strong interest in identifying and
defining activation functions which can improve neural network performance. In
recent years there has been a renovated interest of the scientific community in
investigating activation functions which can be trained during the learning
process, usually referred to as ""trainable"", ""learnable"" or ""adaptable""
activation functions. They appear to lead to better network performance.
Diverse and heterogeneous models of trainable activation function have been
proposed in the literature. In this paper, we present a survey of these models.
Starting from a discussion on the use of the term ""activation function"" in
literature, we propose a taxonomy of trainable activation functions, highlight
common and distinctive proprieties of recent and past models, and discuss main
advantages and limitations of this type of approach. We show that many of the
proposed approaches are equivalent to adding neuron layers which use fixed
(non-trainable) activation functions and some simple local rule that
constraints the corresponding weight layers.
",5.0
820,13,135647,2010.12421,social network analysis with natrual language processing,"Francesco Barbieri and Jose Camacho-Collados and Leonardo Neves and
  Luis Espinosa-Anke","TweetEval: Unified Benchmark and Comparative Evaluation for Tweet
  Classification","  The experimental landscape in natural language processing for social media is
too fragmented. Each year, new shared tasks and datasets are proposed, ranging
from classics like sentiment analysis to irony detection or emoji prediction.
Therefore, it is unclear what the current state of the art is, as there is no
standardized evaluation protocol, neither a strong set of baselines trained on
such domain-specific data. In this paper, we propose a new evaluation framework
(TweetEval) consisting of seven heterogeneous Twitter-specific classification
tasks. We also provide a strong set of baselines as starting point, and compare
different language modeling pre-training strategies. Our initial experiments
show the effectiveness of starting off with existing pre-trained generic
language models, and continue training them on Twitter corpora.
",5.0
821,9,74991,1810.03947,language model for long documents,"Pankaj Gupta and Yatin Chaudhary and Florian Buettner and Hinrich
  Sch\""utze","textTOvec: Deep Contextualized Neural Autoregressive Topic Models of
  Language with Distributed Compositional Prior","  We address two challenges of probabilistic topic modelling in order to better
estimate the probability of a word in a given context, i.e., P(word|context):
(1) No Language Structure in Context: Probabilistic topic models ignore word
order by summarizing a given context as a ""bag-of-word"" and consequently the
semantics of words in the context is lost. The LSTM-LM learns a vector-space
representation of each word by accounting for word order in local collocation
patterns and models complex characteristics of language (e.g., syntax and
semantics), while the TM simultaneously learns a latent representation from the
entire document and discovers the underlying thematic structure. We unite two
complementary paradigms of learning the meaning of word occurrences by
combining a TM (e.g., DocNADE) and a LM in a unified probabilistic framework,
named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of
documents: In settings with a small number of word occurrences (i.e., lack of
context) in short text or data sparsity in a corpus of few documents, the
application of TMs is challenging. We address this challenge by incorporating
external knowledge into neural autoregressive topic models via a language
modelling approach: we use word embeddings as input of a LSTM-LM with the aim
to improve the word-topic mapping on a smaller and/or short-text corpus. The
proposed DocNADE extension is named as ctx-DocNADEe.
  We present novel neural autoregressive topic model variants coupled with
neural LMs and embeddings priors that consistently outperform state-of-the-art
generative TMs in terms of generalization (perplexity), interpretability (topic
coherence) and applicability (retrieval and classification) over 6 long-text
and 8 short-text datasets from diverse domains.
",3.0
822,11,16456,1212.1068,PageRank for web search,"Leonardo Ermann, Klaus M. Frahm and Dima L. Shepelyansky",Spectral properties of Google matrix of Wikipedia and other networks,"  We study the properties of eigenvalues and eigenvectors of the Google matrix
of the Wikipedia articles hyperlink network and other real networks. With the
help of the Arnoldi method we analyze the distribution of eigenvalues in the
complex plane and show that eigenstates with significant eigenvalue modulus are
located on well defined network communities. We also show that the correlator
between PageRank and CheiRank vectors distinguishes different organizations of
information flow on BBC and Le Monde web sites.
",3.0
823,19,212440,2210.04554,artificial intelligence for low carbon,"Ben Dixon, Mar\'ia P\'erez-Ortiz, Jacob Bieker",Comparing the carbon costs and benefits of low-resource solar nowcasting,"  Solar PV yield nowcasting is used to help anticipate peaks and troughs in
demand to support grid integration. This paper compares multiple low-resource
approaches to nowcasting solar PV yield, using a dataset of UK satellite
imagery and solar PV energy readings over a 1 to 4-hour time range. The paper
also estimates the carbon emissions generated and averted by deploying models,
and finds that even small models that could be deployable in low-resource
settings may have a benefit several orders of magnitude greater than its carbon
cost. The paper also examines prediction errors and the activations in a CNN.
",3.0
824,10,165830,2108.03311,web archive,Sawood Alam and Michele C. Weigle and Michael L. Nelson,Profiling Web Archival Voids for Memento Routing,"  Prior work on web archive profiling were focused on Archival Holdings to
describe what is present in an archive. This work defines and explores Archival
Voids to establish a means to represent portions of URI spaces that are not
present in a web archive. Archival Holdings and Archival Voids profiles can
work independently or as complements to each other to maximize the Accuracy of
Memento Aggregators. We discuss various sources of truth that can be used to
create Archival Voids profiles. We use access logs from Arquivo.pt to create
various Archival Voids profiles and analyze them against our MemGator access
logs for evaluation. We find that we could have avoided more than 8% of
additional False Positives on top of the 60% Accuracy we got from profiling
Archival Holdings in our prior work, if Arquivo.pt were to provide an Archival
Voids profile based on URIs that were requested hundreds of times and never
returned any success responses.
",3.0
825,4,140447,2012.03864,pre-trained language model,Lizhen Tan and Olga Golovneva,"Evaluating Cross-Lingual Transfer Learning Approaches in Multilingual
  Conversational Agent Models","  With the recent explosion in popularity of voice assistant devices, there is
a growing interest in making them available to user populations in additional
countries and languages. However, to provide the highest accuracy and best
performance for specific user populations, most existing voice assistant models
are developed individually for each region or language, which requires linear
investment of effort. In this paper, we propose a general multilingual model
framework for Natural Language Understanding (NLU) models, which can help
bootstrap new language models faster and reduce the amount of effort required
to develop each language separately. We explore how different deep learning
architectures affect multilingual NLU model performance. Our experimental
results show that these multilingual models can reach same or better
performance compared to monolingual models across language-specific test data
while require less effort in creating features and model maintenance.
",5.0
826,9,189376,2203.08244,language model for long documents,Ha-Thanh Nguyen,Toward Improving Attentive Neural Networks in Legal Text Processing,"  In recent years, thanks to breakthroughs in neural network techniques
especially attentive deep learning models, natural language processing has made
many impressive achievements. However, automated legal word processing is still
a difficult branch of natural language processing. Legal sentences are often
long and contain complicated legal terminologies. Hence, models that work well
on general documents still face challenges in dealing with legal documents. We
have verified the existence of this problem with our experiments in this work.
In this dissertation, we selectively present the main achievements in improving
attentive neural networks in automatic legal document processing. Language
models tend to grow larger and larger, though, without expert knowledge, these
models can still fail in domain adaptation, especially for specialized fields
like law.
",1.0
827,13,193336,2204.10185,social network analysis with natrual language processing,"Ali Raheman, Anton Kolonin, Igors Fridkins, Ikram Ansari, Mukul
  Vishwas",Social Media Sentiment Analysis for Cryptocurrency Market Prediction,"  In this paper, we explore the usability of different natural language
processing models for the sentiment analysis of social media applied to
financial market prediction, using the cryptocurrency domain as a reference. We
study how the different sentiment metrics are correlated with the price
movements of Bitcoin. For this purpose, we explore different methods to
calculate the sentiment metrics from a text finding most of them not very
accurate for this prediction task. We find that one of the models outperforms
more than 20 other public ones and makes it possible to fine-tune it
efficiently given its interpretable nature. Thus we confirm that interpretable
artificial intelligence and natural language processing methods might be more
valuable practically than non-explainable and non-interpretable ones. In the
end, we analyse potential causal connections between the different sentiment
metrics and the price movements.
",5.0
828,9,219069,cs/9902022,language model for long documents,"Ulrich Schiel, Ianna M. Sodre Ferreira de Souza and Edberto Ferneda",Semi-Automatic Indexing of Multilingual Documents,"  With the growing significance of digital libraries and the Internet, more and
more electronic texts become accessible to a wide and geographically disperse
public. This requires adequate tools to facilitate indexing, storage, and
retrieval of documents written in different languages. We present a method for
semi-automatic indexing of electronic documents and construction of a
multilingual thesaurus, which can be used for query formulation and information
retrieval. We use special dictionaries and user interaction in order to solve
ambiguities and find adequate canonical terms in the language and adequate
abstract language-independent terms. The abstract thesaurus is updated
incrementally by new indexed documents and is used to search document
concerning terms in a query to the document base.
",1.0
829,13,129888,2008.13549,social network analysis with natrual language processing,Laksh Advani and Clement Lu and Suraj Maharjan,"C1 at SemEval-2020 Task 9: SentiMix: Sentiment Analysis for Code-Mixed
  Social Media Text using Feature Engineering","  In today's interconnected and multilingual world, code-mixing of languages on
social media is a common occurrence. While many Natural Language Processing
(NLP) tasks like sentiment analysis are mature and well designed for
monolingual text, techniques to apply these tasks to code-mixed text still
warrant exploration. This paper describes our feature engineering approach to
sentiment analysis in code-mixed social media text for SemEval-2020 Task 9:
SentiMix. We tackle this problem by leveraging a set of hand-engineered
lexical, sentiment, and metadata features to design a classifier that can
disambiguate between ""positive"", ""negative"" and ""neutral"" sentiment. With this
model, we are able to obtain a weighted F1 score of 0.65 for the ""Hinglish""
task and 0.63 for the ""Spanglish"" tasks
",4.0
830,19,180826,2112.12582,artificial intelligence for low carbon,"Lauren M. Sanders (1), Jason H. Yang (2), Ryan T. Scott (3), Amina Ann
  Qutub (4), Hector Garcia Martin (5 and 6 and 7), Daniel C. Berrios (3), Jaden
  J.A. Hastings (8), Jon Rask (9), Graham Mackintosh (10), Adrienne L.
  Hoarfrost (11), Stuart Chalk (12), John Kalantari (13), Kia Khezeli (13),
  Erik L. Antonsen (14), Joel Babdor (15), Richard Barker (16), Sergio E.
  Baranzini (17), Afshin Beheshti (3), Guillermo M. Delgado-Aparicio (18),
  Benjamin S. Glicksberg (19), Casey S. Greene (20), Melissa Haendel (21), Arif
  A. Hamid (22), Philip Heller (23), Daniel Jamieson (24), Katelyn J. Jarvis
  (25), Svetlana V. Komarova (26), Matthieu Komorowski (27), Prachi Kothiyal
  (28), Ashish Mahabal (29), Uri Manor (30), Christopher E. Mason (8), Mona
  Matar (31), George I. Mias (32), Jack Miller (3), Jerry G. Myers Jr. (31),
  Charlotte Nelson (17), Jonathan Oribello (1), Seung-min Park (33), Patricia
  Parsons-Wingerter (34), R. K. Prabhu (35), Robert J. Reynolds (36), Amanda
  Saravia-Butler (37), Suchi Saria (38 and 39), Aenor Sawyer (24), Nitin Kumar
  Singh (40), Frank Soboczenski (41), Michael Snyder (42), Karthik Soman (17),
  Corey A. Theriot (43 and 44), David Van Valen (45), Kasthuri Venkateswaran
  (40), Liz Warren (46), Liz Worthey (47), Marinka Zitnik (48), Sylvain V.
  Costes (49) ((1) Blue Marble Space Institute of Science, Space Biosciences
  Division, NASA Ames Research Center, Moffett Field, CA, USA., (2) Center for
  Emerging and Re-Emerging Pathogens, Department of Microbiology, Biochemistry
  and Molecular Genetics, Rutgers New Jersey Medical School, Newark, NJ, USA.,
  (3) KBR, Space Biosciences Division, NASA Ames Research Center, Moffett
  Field, CA, USA., (4) AI MATRIX Consortium, Department of Biomedical
  Engineering, University of Texas, San Antonio and UT Health Sciences, San
  Antonio, TX, USA., (5) Biological Systems and Engineering Division, Lawrence
  Berkeley National Lab, Berkeley, CA, USA., (6) DOE Agile BioFoundry,
  Emeryville, CA, USA., (7) Joint BioEnergy Institute, Emeryville, CA, USA.,
  (8) Department of Physiology and Biophysics, Weill Cornell Medicine, New
  York, NY, USA., (9) Office of the Center Director, NASA Ames Research Center,
  Moffett Field, CA, USA., (10) Bay Area Environmental Research Institute, NASA
  Ames Research Center, Moffett Field, CA, USA., (11) Universities Space
  Research Association (USRA), Space Biosciences Division, NASA Ames Research
  Center, Moffett Field, CA, USA., (12) Department of Chemistry, University of
  North Florida, Jacksonville, FL, USA., (13) Center for Individualized
  Medicine, Department of Surgery, Department of Quantitative Health Sciences,
  Mayo Clinic, Rochester, MN, USA., (14) Department of Emergency Medicine,
  Center for Space Medicine, Baylor College of Medicine, Houston, TX, USA.,
  (15) Department of Microbiology and Immunology, Department of Otolaryngology,
  Head and Neck Surgery, University of California San Francisco, San Francisco,
  CA, USA., (16) The Gilroy AstroBiology Research Group, The University of
  Wisconsin - Madison, Madison, WI, USA., (17) Weill Institute for
  Neurosciences, Department of Neurology, University of California San
  Francisco, San Francisco, CA, USA., (18) Data Science Analytics, Georgia
  Institute of Technology, Lima, Peru, (19) Hasso Plattner Institute for
  Digital Health at Mount Sinai, Department of Genetics and Genomic Sciences,
  Icahn School of Medicine at Mount Sinai, New York, NY, USA., (20) Center for
  Health AI, Department of Biochemistry and Molecular Genetics, University of
  Colorado School of Medicine, Anschutz Medical Campus, Aurora, CO, USA., (21)
  Center for Health AI, University of Colorado School of Medicine, Anschutz
  Medical Campus, Aurora, CO, USA., (22) Department of Neuroscience, University
  of Minnesota, Minneapolis, MN, USA., (23) Department of Computer Science,
  College of Science, San Jos\'e State University, San Jose, CA, USA., (24)
  Biorelate, Manchester, United Kingdom., (25) UC Space Health, Department of
  Orthopaedic Surgery, University of California, San Francisco, San Francisco,
  CA, USA., (26) Faculty of Dental Medicine and Oral Health Sciences, McGill
  University, Montreal, Quebec, Canada., (27) Faculty of Medicine, Dept of
  Surgery and Cancer, Imperial College London, London, United Kingdom., (28)
  SymbioSeq LLC, NASA Johnson Space Center, Ashburn, VA, USA., (29) Center for
  Data Driven Discovery, California Institute of Technology, Pasadena, CA,
  USA., (30) Waitt Advanced Biophotonics Center, Chan-Zuckerberg Imaging
  Scientist Fellow, Salk Institute for Biological Studies, La Jolla, CA, USA.,
  (31) Human Research Program Cross Cutting Computational Modeling Project,
  NASA John H. Glenn Research Center, Cleveland, OH, USA., (32) Institute for
  Quantitative Health Science and Engineering, Department of Biochemistry and
  Molecular Biology, Michigan State University, East Lansing, MI, USA., (33)
  Department of Urology, Department of Radiology, Stanford University School of
  Medicine, Stanford, CA, USA., (34) Low Exploration Gravity Technology, NASA
  John H. Glenn Research Center, Cleveland, OH, USA., (35) Universities Space
  Research Association (USRA), Human Research Program Cross-cutting
  Computational Modeling Project, NASA John H. Glenn Research Center,
  Cleveland, OH, USA., (36) Mortality Research & Consulting, Inc., Houston, TX,
  USA., (37) Logyx, Space Biosciences Division, NASA Ames Research Center,
  Moffett Field, CA, USA., (38) Computer Science, Statistics, and Health
  Policy, Johns Hopkins University, Baltimore, MD, USA., (39) ML, AI and
  Healthcare Lab, Bayesian Health, New York, NY, USA., (40) Biotechnology and
  Planetary Protection Group, Jet Propulsion Laboratory, Pasadena, CA, USA.,
  (41) SPHES, Medical Faculty, King's College London, London, United Kingdom.,
  (42) Department of Genetics, Stanford School of Medicine, Stanford, CA USA.,
  (43) Department of Preventive Medicine and Community Health, UTMB, Galveston,
  TX USA., (44) Human Health and Performance Directorate, NASA Johnson Space
  Center, Houston, TX, USA., (45) Department of Biology, California Institute
  of Technology, Pasadena, CA, USA., (46) ISS National Laboratory, Center for
  the Advancement of Science in Space, Melbourne, FL, USA., (47) UAB Center for
  Computational Biology and Data Science, University of Alabama, Birmingham,
  Birmingham, AL, USA., (48) Department of Biomedical Informatics, Harvard
  Medical School, Harvard Data Science, Broad Institute of MIT and Harvard,
  Harvard University, Boston, MA, USA., (49) Space Biosciences Division, NASA
  Ames Research Center, Moffett Field, CA, USA.)","Beyond Low Earth Orbit: Biological Research, Artificial Intelligence,
  and Self-Driving Labs","  Space biology research aims to understand fundamental effects of spaceflight
on organisms, develop foundational knowledge to support deep space exploration,
and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem
of plants, crops, microbes, animals, and humans for sustained multi-planetary
life. To advance these aims, the field leverages experiments, platforms, data,
and model organisms from both spaceborne and ground-analog studies. As research
is extended beyond low Earth orbit, experiments and platforms must be maximally
autonomous, light, agile, and intelligent to expedite knowledge discovery. Here
we present a summary of recommendations from a workshop organized by the
National Aeronautics and Space Administration on artificial intelligence,
machine learning, and modeling applications which offer key solutions toward
these space biology challenges. In the next decade, the synthesis of artificial
intelligence into the field of space biology will deepen the biological
understanding of spaceflight effects, facilitate predictive modeling and
analytics, support maximally autonomous and reproducible experiments, and
efficiently manage spaceborne data and metadata, all with the goal to enable
life to thrive in deep space.
",1.0
831,14,191619,2204.01849,text summarization model,"Divakar Yadav, Jalpa Desai, Arun Kumar Yadav",Automatic Text Summarization Methods: A Comprehensive Review,"  One of the most pressing issues that have arisen due to the rapid growth of
the Internet is known as information overloading. Simplifying the relevant
information in the form of a summary will assist many people because the
material on any topic is plentiful on the Internet. Manually summarising
massive amounts of text is quite challenging for humans. So, it has increased
the need for more complex and powerful summarizers. Researchers have been
trying to improve approaches for creating summaries since the 1950s, such that
the machine-generated summary matches the human-created summary. This study
provides a detailed state-of-the-art analysis of text summarization concepts
such as summarization approaches, techniques used, standard datasets,
evaluation metrics and future scopes for research. The most commonly accepted
approaches are extractive and abstractive, studied in detail in this work.
Evaluating the summary and increasing the development of reusable resources and
infrastructure aids in comparing and replicating findings, adding competition
to improve the outcomes. Different evaluation methods of generated summaries
are also discussed in this study. Finally, at the end of this study, several
challenges and research opportunities related to text summarization research
are mentioned that may be useful for potential researchers working in this
area.
",5.0
832,16,91541,1906.06903,activation function in neutral networks,Ilsang Ohn and Yongdai Kim,"Smooth function approximation by deep neural networks with general
  activation functions","  There has been a growing interest in expressivity of deep neural networks.
However, most of the existing work about this topic focuses only on the
specific activation function such as ReLU or sigmoid. In this paper, we
investigate the approximation ability of deep neural networks with a broad
class of activation functions. This class of activation functions includes most
of frequently used activation functions. We derive the required depth, width
and sparsity of a deep neural network to approximate any H\""older smooth
function upto a given approximation error for the large class of activation
functions. Based on our approximation error analysis, we derive the minimax
optimality of the deep neural network estimators with the general activation
functions in both regression and classification problems.
",5.0
833,10,49999,1702.01187,web archive,"Helge Holzmann, Nina Tahmasebi, Thomas Risse",Named Entity Evolution Recognition on the Blogosphere,"  Advancements in technology and culture lead to changes in our language. These
changes create a gap between the language known by users and the language
stored in digital archives. It affects user's possibility to firstly find
content and secondly interpret that content. In previous work we introduced our
approach for Named Entity Evolution Recognition~(NEER) in newspaper
collections. Lately, increasing efforts in Web preservation lead to increased
availability of Web archives covering longer time spans. However, language on
the Web is more dynamic than in traditional media and many of the basic
assumptions from the newspaper domain do not hold for Web data. In this paper
we discuss the limitations of existing methodology for NEER. We approach these
by adapting an existing NEER method to work on noisy data like the Web and the
Blogosphere in particular. We develop novel filters that reduce the noise and
make use of Semantic Web resources to obtain more information about terms. Our
evaluation shows the potentials of the proposed approach.
",0.0
834,10,167480,2108.12092,web archive,"Kritika Garg, Himarsha R. Jayanetti, Sawood Alam, Michele C. Weigle,
  Michael L. Nelson","Replaying Archived Twitter: When your bird is broken, will it bring you
  down?","  Historians and researchers trust web archives to preserve social media
content that no longer exists on the live web. However, what we see on the live
web and how it is replayed in the archive are not always the same. In this
paper, we document and analyze the problems in archiving Twitter ever since
Twitter forced the use of its new UI in June 2020. Most web archives were
unable to archive the new UI, resulting in archived Twitter pages displaying
Twitter's ""Something went wrong"" error. The challenges in archiving the new UI
forced web archives to continue using the old UI. To analyze the potential loss
of information in web archival data due to this change, we used the personal
Twitter account of the 45th President of the United States, @realDonaldTrump,
which was suspended by Twitter on January 8, 2021. Trump's account was heavily
labeled by Twitter for spreading misinformation, however we discovered that
there is no evidence in web archives to prove that some of his tweets ever had
a label assigned to them. We also studied the possibility of temporal
violations in archived versions of the new UI, which may result in the replay
of pages that never existed on the live web. Our goal is to educate researchers
who may use web archives and caution them when drawing conclusions based on
archived Twitter pages.
",4.0
835,15,62023,1801.03627,relevance feedback for imformation retrieval,Bilal Abu-Salih,"Applying Vector Space Model (VSM) Techniques in Information Retrieval
  for Arabic Language","  Information Retrieval (IR) allows the storage, management, processing and
retrieval of information, documents, websites, etc. Building an IR system for
any language is imperative. This is evident through the massive conducted
efforts to build IR systems using any of its models that are valid for certain
languages. This report presents an implementation for a core IR technique which
is Vector Space Model (VSM). We have chosen VSM model for our project since it
is a term weighting scheme, and the retrieved documents could be sorted
according to their relevancy degree. One other significant feature for such
technique is the ability to get a relevance feedback from the users of the
system; users can judge whether the retrieved document is relative to their
need or not. The developed system has been validated through building an Arabic
IR website using server side scripting. The experiments verifies the
effectiveness of our system to apply all techniques of vector space model and
valid over Arabic language.
",3.0
836,1,12774,1204.5373,advanced search engine,Shlomo Geva and Christopher M. De Vries,TopSig: Topology Preserving Document Signatures,"  Performance comparisons between File Signatures and Inverted Files for text
retrieval have previously shown several significant shortcomings of file
signatures relative to inverted files. The inverted file approach underpins
most state-of-the-art search engine algorithms, such as Language and
Probabilistic models. It has been widely accepted that traditional file
signatures are inferior alternatives to inverted files. This paper describes
TopSig, a new approach to the construction of file signatures. Many advances in
semantic hashing and dimensionality reduction have been made in recent times,
but these were not so far linked to general purpose, signature file based,
search engines. This paper introduces a different signature file approach that
builds upon and extends these recent advances. We are able to demonstrate
significant improvements in the performance of signature file based indexing
and retrieval, performance that is comparable to that of state of the art
inverted file based systems, including Language models and BM25. These findings
suggest that file signatures offer a viable alternative to inverted files in
suitable settings and from the theoretical perspective it positions the file
signatures model in the class of Vector Space retrieval models.
",3.0
837,8,198311,2206.00637,node embedding for graph,"Beni Egressy, Roger Wattenhofer",Graph Neural Networks with Precomputed Node Features,"  Most Graph Neural Networks (GNNs) cannot distinguish some graphs or indeed
some pairs of nodes within a graph. This makes it impossible to solve certain
classification tasks. However, adding additional node features to these models
can resolve this problem. We introduce several such augmentations, including
(i) positional node embeddings, (ii) canonical node IDs, and (iii) random
features. These extensions are motivated by theoretical results and
corroborated by extensive testing on synthetic subgraph detection tasks. We
find that positional embeddings significantly outperform other extensions in
these tasks. Moreover, positional embeddings have better sample efficiency,
perform well on different graph distributions and even outperform learning with
ground truth node positions. Finally, we show that the different augmentations
perform competitively on established GNN benchmarks, and advise on when to use
them.
",5.0
838,3,5602,1006.1309,database management system,"S.M. Joshi, S. Sanyal, S. Banerjee, S. Srikumar",Using Grid Files for a Relational Database Management System,"  This paper describes our experience with using Grid files as the main storage
organization for a relational database management system. We primarily focus on
the following two aspects. (i) Strategies for implementing grid files
efficiently. (ii) Methods for efficiency evaluating queries posed to a database
organized using grid files.
",5.0
839,11,42164,1605.01378,PageRank for web search,"Alon Sela, Louis Shekhtman, Shlomo Havlin, Irad Ben-Gal",Comparing the diversity of information by word-of-mouth vs. web spread,"  Many studies have explored spreading and diffusion through complex networks.
The following study examines a specific case of spreading of opinions in modern
society through two spreading schemes, defined as being either through
word-of-mouth (WOM), or through online search engines (WEB). We apply both
modelling and real experimental results and compare the opinions people adopt
through an exposure to their friend`s opinions, as opposed to the opinions they
adopt when using a search engine based on the PageRank algorithm. A simulated
study shows that when members in a population adopt decisions through the use
of the WEB scheme, the population ends up with a few dominant views, while
other views are barely expressed. In contrast, when members adopt decisions
based on the WOM scheme, there is a far more diverse distribution of opinions
in that population. The simulative results are further supported by an online
experiment which finds that people searching information through a search
engine end up with far more homogenous opinions as compared to those asking
their friends.
",2.0
840,18,4406,1001.083,infomation retrieval time complexity,Christopher M. De Vries and Shlomo Geva,K-tree: Large Scale Document Clustering,"  We introduce K-tree in an information retrieval context. It is an efficient
approximation of the k-means clustering algorithm. Unlike k-means it forms a
hierarchy of clusters. It has been extended to address issues with sparse
representations. We compare performance and quality to CLUTO using document
collections. The K-tree has a low time complexity that is suitable for large
document collections. This tree structure allows for efficient disk based
implementations where space requirements exceed that of main memory.
",3.0
841,2,127834,2008.02479,random forests,Richard A. Davis and Mikkel S. Nielsen,Modeling of time series using random forests: theoretical developments,"  In this paper we study asymptotic properties of random forests within the
framework of nonlinear time series modeling. While random forests have been
successfully applied in various fields, the theoretical justification has not
been considered for their use in a time series setting. Under mild conditions,
we prove a uniform concentration inequality for regression trees built on
nonlinear autoregressive processes and, subsequently, we use this result to
prove consistency for a large class of random forests. The results are
supported by various simulations.
",5.0
846,4,194187,2204.14066,pre-trained language model,"Rick Szostak, Richard P. Smiraglia, Andrea Scharnhorst, Aida Slavic,
  Daniel Mart\'inez-\'Avila, Tobias Renwick",Classifications as Linked Open Data. Challenges and Opportunities,"  Linked Data (LD) as a web--based technology enables in principle the
seamless, machine--supported integration, interplay and augmentation of all
kinds of knowledge, into what has been labeled a huge knowledge graph. Despite
decades of web technology and, more recently, the LD approach, the task to
fully exploit these new technologies in the public domain is only commencing.
One specific challenge is to transfer techniques developed preweb to order our
knowledge into the realm of Linked Open Data (LOD) This paper illustrates two
different models in which a general analytico--synthetic classification can be
published and made available as LD. In both cases, an LD solution deals with
the intricacies of a pre--coordinated indexing language.
",1.0
877,0,93248,1907.03976,learning to rank with partitioned preference,"Daniel S. Brown, Wonjoon Goo, and Scott Niekum","Better-than-Demonstrator Imitation Learning via Automatically-Ranked
  Demonstrations","  The performance of imitation learning is typically upper-bounded by the
performance of the demonstrator. While recent empirical results demonstrate
that ranked demonstrations allow for better-than-demonstrator performance,
preferences over demonstrations may be difficult to obtain, and little is known
theoretically about when such methods can be expected to successfully
extrapolate beyond the performance of the demonstrator. To address these
issues, we first contribute a sufficient condition for better-than-demonstrator
imitation learning and provide theoretical results showing why preferences over
demonstrations can better reduce reward function ambiguity when performing
inverse reinforcement learning. Building on this theory, we introduce
Disturbance-based Reward Extrapolation (D-REX), a ranking-based imitation
learning method that injects noise into a policy learned through behavioral
cloning to automatically generate ranked demonstrations. These ranked
demonstrations are used to efficiently learn a reward function that can then be
optimized using reinforcement learning. We empirically validate our approach on
simulated robot and Atari imitation learning benchmarks and show that D-REX
outperforms standard imitation learning approaches and can significantly
surpass the performance of the demonstrator. D-REX is the first imitation
learning approach to achieve significant extrapolation beyond the
demonstrator's performance without additional side-information or supervision,
such as rewards or human preferences. By generating rankings automatically, we
show that preference-based inverse reinforcement learning can be applied in
traditional imitation learning settings where only unlabeled demonstrations are
available.
",1.0
888,19,105654,1912.05612,artificial intelligence for low carbon,"Amin Bemani, Alireza Baghban, Shahaboddin Shamshirband, Amir Mosavi,
  Peter Csiba, Annamaria R. Varkonyi-Koczy","Applying ANN, ANFIS, and LSSVM Models for Estimation of Acid Solvent
  Solubility in Supercritical CO$_2$","  In the present work, a novel and the robust computational investigation is
carried out to estimate solubility of different acids in supercritical carbon
dioxide. Four different algorithms such as radial basis function artificial
neural network, Multi-layer Perceptron (MLP) artificial neural network (ANN),
Least squares support vector machine (LSSVM) and adaptive neuro-fuzzy inference
system (ANFIS) are developed to predict the solubility of different acids in
carbon dioxide based on the temperature, pressure, hydrogen number, carbon
number, molecular weight, and acid dissociation constant of acid. In the
purpose of best evaluation of proposed models, different graphical and
statistical analyses and also a novel sensitivity analysis are carried out. The
present study proposed the great manners for best acid solubility estimation in
supercritical carbon dioxide, which can be helpful for engineers and chemists
to predict operational conditions in industries.
",5.0
889,3,200048,2206.07304,database management system,Baihan Lin,"Knowledge Management System with NLP-Assisted Annotations: A Brief
  Survey and Outlook","  Knowledge management systems are in high demand for industrial researchers,
chemical or research enterprises, or evidence-based decision making. However,
existing systems have limitations in categorizing and organizing paper insights
or relationships. Traditional databases are usually disjoint with logging
systems, which limit its utility in generating concise, collated overviews. In
this work, we briefly survey existing approaches of this problem space and
propose a unified framework that utilizes relational databases to log
hierarchical information to facilitate the research and writing process, or
generate useful knowledge from references or insights from connected concepts.
This framework of knowledge management system enables novel functionalities
encompassing improved hierarchical notetaking, AI-assisted brainstorming, and
multi-directional relationships. Potential applications include managing
inventories and changes for manufacture or research enterprises, or generating
analytic reports with evidence-based decision making.
",5.0
905,8,156035,2105.0532,node embedding for graph,"Yiming Wang, Dongxia Chang, Zhiqian Fu, and Yao Zhao","Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph
  Clustering","  Recently, there has been considerable research interest in graph clustering
aimed at data partition using the graph information. However, one limitation of
the most of graph-based methods is that they assume the graph structure to
operate is fixed and reliable. And there are inevitably some edges in the graph
that are not conducive to graph clustering, which we call spurious edges. This
paper is the first attempt to employ graph pooling technique for node
clustering and we propose a novel dual graph embedding network (DGEN), which is
designed as a two-step graph encoder connected by a graph pooling layer to
learn the graph embedding. In our model, it is assumed that if a node and its
nearest neighboring node are close to the same clustering center, this node is
an informative node and this edge can be considered as a cluster-friendly edge.
Based on this assumption, the neighbor cluster pooling (NCPool) is devised to
select the most informative subset of nodes and the corresponding edges based
on the distance of nodes and their nearest neighbors to the cluster centers.
This can effectively alleviate the impact of the spurious edges on the
clustering. Finally, to obtain the clustering assignment of all nodes, a
classifier is trained using the clustering results of the selected nodes.
Experiments on five benchmark graph datasets demonstrate the superiority of the
proposed method over state-of-the-art algorithms.
",2.0
909,4,5492,1005.4752,pre-trained language model,Djoerd Hiemstra and Vojkan Mihajlovic,"A database approach to information retrieval: The remarkable
  relationship between language models and region models","  In this report, we unify two quite distinct approaches to information
retrieval: region models and language models. Region models were developed for
structured document retrieval. They provide a well-defined behaviour as well as
a simple query language that allows application developers to rapidly develop
applications. Language models are particularly useful to reason about the
ranking of search results, and for developing new ranking approaches. The
unified model allows application developers to define complex language modeling
approaches as logical queries on a textual database. We show a remarkable
one-to-one relationship between region queries and the language models they
represent for a wide variety of applications: simple ad-hoc search,
cross-language retrieval, video retrieval, and web search.
",2.0
925,4,66028,1804.0398,pre-trained language model,"Kris Cao, Angeliki Lazaridou, Marc Lanctot, Joel Z Leibo, Karl Tuyls,
  Stephen Clark",Emergent Communication through Negotiation,"  Multi-agent reinforcement learning offers a way to study how communication
could emerge in communities of agents needing to solve specific problems. In
this paper, we study the emergence of communication in the negotiation
environment, a semi-cooperative model of agent interaction. We introduce two
communication protocols -- one grounded in the semantics of the game, and one
which is \textit{a priori} ungrounded and is a form of cheap talk. We show that
self-interested agents can use the pre-grounded communication channel to
negotiate fairly, but are unable to effectively use the ungrounded channel.
However, prosocial agents do learn to use cheap talk to find an optimal
negotiating strategy, suggesting that cooperation is necessary for language to
emerge. We also study communication behaviour in a setting where one agent
interacts with agents in a community with different levels of prosociality and
show how agent identifiability can aid negotiation.
",0.0
929,7,64068,1802.09031,gradient boosting,"Atsushi Nitanda, Taiji Suzuki",Functional Gradient Boosting based on Residual Network Perception,"  Residual Networks (ResNets) have become state-of-the-art models in deep
learning and several theoretical studies have been devoted to understanding why
ResNet works so well. One attractive viewpoint on ResNet is that it is
optimizing the risk in a functional space by combining an ensemble of effective
features. In this paper, we adopt this viewpoint to construct a new gradient
boosting method, which is known to be very powerful in data analysis. To do so,
we formalize the gradient boosting perspective of ResNet mathematically using
the notion of functional gradients and propose a new method called ResFGB for
classification tasks by leveraging ResNet perception. Two types of
generalization guarantees are provided from the optimization perspective: one
is the margin bound and the other is the expected risk bound by the
sample-splitting technique. Experimental results show superior performance of
the proposed method over state-of-the-art methods such as LightGBM.
",5.0
954,2,166238,2108.05276,random forests,"Gilles Audemard and Steve Bellart and Louenas Bounia and Fr\'ed\'eric
  Koriche and Jean-Marie Lagniez and Pierre Marquis",Trading Complexity for Sparsity in Random Forest Explanations,"  Random forests have long been considered as powerful model ensembles in
machine learning. By training multiple decision trees, whose diversity is
fostered through data and feature subsampling, the resulting random forest can
lead to more stable and reliable predictions than a single decision tree. This
however comes at the cost of decreased interpretability: while decision trees
are often easily interpretable, the predictions made by random forests are much
more difficult to understand, as they involve a majority vote over hundreds of
decision trees. In this paper, we examine different types of reasons that
explain ""why"" an input instance is classified as positive or negative by a
Boolean random forest. Notably, as an alternative to sufficient reasons taking
the form of prime implicants of the random forest, we introduce majoritary
reasons which are prime implicants of a strict majority of decision trees. For
these different abductive explanations, the tractability of the generation
problem (finding one reason) and the minimization problem (finding one shortest
reason) are investigated. Experiments conducted on various datasets reveal the
existence of a trade-off between runtime complexity and sparsity. Sufficient
reasons - for which the identification problem is DP-complete - are slightly
larger than majoritary reasons that can be generated using a simple linear-
time greedy algorithm, and significantly larger than minimal majoritary reasons
that can be approached using an anytime P ARTIAL M AX SAT algorithm.
",5.0
958,19,76914,1811.03163,artificial intelligence for low carbon,Tim Miller,Contrastive Explanation: A Structural-Model Approach,"  This paper presents a model of contrastive explanation using structural
casual models. The topic of causal explanation in artificial intelligence has
gathered interest in recent years as researchers and practitioners aim to
increase trust and understanding of intelligent decision-making. While
different sub-fields of artificial intelligence have looked into this problem
with a sub-field-specific view, there are few models that aim to capture
explanation more generally. One general model is based on structural causal
models. It defines an explanation as a fact that, if found to be true, would
constitute an actual cause of a specific event. However, research in philosophy
and social sciences shows that explanations are contrastive: that is, when
people ask for an explanation of an event -- the fact -- they (sometimes
implicitly) are asking for an explanation relative to some contrast case; that
is, ""Why P rather than Q?"". In this paper, we extend the structural causal
model approach to define two complementary notions of contrastive explanation,
and demonstrate them on two classical problems in artificial intelligence:
classification and planning. We believe that this model can help researchers in
subfields of artificial intelligence to better understand contrastive
explanation.
",1.0
972,0,85998,1904.05325,learning to rank with partitioned preference,"Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla",Risk Aware Ranking for Top-$k$ Recommendations,"  Given an incomplete ratings data over a set of users and items, the
preference completion problem aims to estimate a personalized total preference
order over a subset of the items. In practical settings, a ranked list of
top-$k$ items from the estimated preference order is recommended to the end
user in the decreasing order of preference for final consumption. We analyze
this model and observe that such a ranking model results in suboptimal
performance when the payoff associated with the recommended items is different.
We propose a novel and very efficient algorithm for the preference ranking
considering the uncertainty regarding the payoffs of the items. Once the
preference scores for the users are obtained using any preference learning
algorithm, we show that ranking the items using a risk seeking utility function
results in the best ranking performance.
",2.0
982,0,121199,2006.05067,learning to rank with partitioned preference,"Jiaqi Ma, Xinyang Yi, Weijing Tang, Zhe Zhao, Lichan Hong, Ed H. Chi,
  Qiaozhu Mei","Learning-to-Rank with Partitioned Preference: Fast Estimation for the
  Plackett-Luce Model","  We investigate the Plackett-Luce (PL) model based listwise learning-to-rank
(LTR) on data with partitioned preference, where a set of items are sliced into
ordered and disjoint partitions, but the ranking of items within a partition is
unknown. Given $N$ items with $M$ partitions, calculating the likelihood of
data with partitioned preference under the PL model has a time complexity of
$O(N+S!)$, where $S$ is the maximum size of the top $M-1$ partitions. This
computational challenge restrains most existing PL-based listwise LTR methods
to a special case of partitioned preference, top-$K$ ranking, where the exact
order of the top $K$ items is known. In this paper, we exploit a random utility
model formulation of the PL model, and propose an efficient numerical
integration approach for calculating the likelihood and its gradients with a
time complexity $O(N+S^3)$. We demonstrate that the proposed method outperforms
well-known LTR baselines and remains scalable through both simulation
experiments and applications to real-world eXtreme Multi-Label classification
tasks.
",5.0
985,11,136247,2010.14903,PageRank for web search,"Giovanni De Toni, Cristian Consonni, Alberto Montresor","A general method for estimating the prevalence of
  Influenza-Like-Symptoms with Wikipedia data","  Influenza is an acute respiratory seasonal disease that affects millions of
people worldwide and causes thousands of deaths in Europe alone. Being able to
estimate in a fast and reliable way the impact of an illness on a given country
is essential to plan and organize effective countermeasures, which is now
possible by leveraging unconventional data sources like web searches and
visits. In this study, we show the feasibility of exploiting information about
Wikipedia's page views of a selected group of articles and machine learning
models to obtain accurate estimates of influenza-like illnesses incidence in
four European countries: Italy, Germany, Belgium, and the Netherlands. We
propose a novel language-agnostic method, based on two algorithms, Personalized
PageRank and CycleRank, to automatically select the most relevant Wikipedia
pages to be monitored without the need for expert supervision. We then show how
our model is able to reach state-of-the-art results by comparing it with
previous solutions.
",3.0
998,19,140951,2012.06338,artificial intelligence for low carbon,Alex James,"The Why, What and How of Artificial General Intelligence Chip
  Development","  The AI chips increasingly focus on implementing neural computing at low power
and cost. The intelligent sensing, automation, and edge computing applications
have been the market drivers for AI chips. Increasingly, the generalisation,
performance, robustness, and scalability of the AI chip solutions are compared
with human-like intelligence abilities. Such a requirement to transit from
application-specific to general intelligence AI chip must consider several
factors. This paper provides an overview of this cross-disciplinary field of
study, elaborating on the generalisation of intelligence as understood in
building artificial general intelligence (AGI) systems. This work presents a
listing of emerging AI chip technologies, classification of edge AI
implementations, and the funnel design flow for AGI chip development. Finally,
the design consideration required for building an AGI chip is listed along with
the methods for testing and validating it.
",0.0
1003,14,62496,1801.07736,text summarization model,"William Fedus, Ian Goodfellow and Andrew M. Dai",MaskGAN: Better Text Generation via Filling in the______,"  Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.
",2.0
1024,11,15530,1210.1626,PageRank for web search,Hengshuai Yao,Discovering and Leveraging the Most Valuable Links for Ranking,"  On the Web, visits of a page are often introduced by one or more valuable
linking sources. Indeed, good back links are valuable resources for Web pages
and sites. We propose to discovering and leveraging the best backlinks of pages
for ranking. Similar to PageRank, MaxRank scores are updated {recursively}. In
particular, with probability $\lambda$, the MaxRank of a document is updated
from the backlink source with the maximum score; with probability $1-\lambda$,
the MaxRank of a document is updated from a random backlink source. MaxRank has
an interesting relation to PageRank. When $\lambda=0$, MaxRank reduces to
PageRank; when $\lambda=1$, MaxRank only looks at the best backlink it thinks.
Empirical results on Wikipedia shows that the global authorities are very
influential; Overall large $\lambda$s (but smaller than 1) perform best: the
convergence is dramatically faster than PageRank, but the performance is still
comparable. We study the influence of these sources and propose a few measures
such as the times of being the best backlink for others, and related properties
of the proposed algorithm. The introduction of best backlink sources provides
new insights for link analysis. Besides ranking, our method can be used to
discover the most valuable linking sources for a page or Website, which is
useful for both search engines and site owners.
",5.0
1028,14,117199,2004.14135,text summarization model,"Khalid N. Elmadani, Mukhtar Elgezouli, Anas Showk",BERT Fine-tuning For Arabic Text Summarization,"  Fine-tuning a pretrained BERT model is the state of the art method for
extractive/abstractive text summarization, in this paper we showcase how this
fine-tuning method can be applied to the Arabic language to both construct the
first documented model for abstractive Arabic text summarization and show its
performance in Arabic extractive summarization. Our model works with
multilingual BERT (as Arabic language does not have a pretrained BERT of its
own). We show its performance in English corpus first before applying it to
Arabic corpora in both extractive and abstractive tasks.
",5.0
1029,14,134529,2010.08021,text summarization model,"Aman Khullar, Udit Arora","MAST: Multimodal Abstractive Summarization with Trimodal Hierarchical
  Attention","  This paper presents MAST, a new model for Multimodal Abstractive Text
Summarization that utilizes information from all three modalities -- text,
audio and video -- in a multimodal video. Prior work on multimodal abstractive
text summarization only utilized information from the text and video
modalities. We examine the usefulness and challenges of deriving information
from the audio modality and present a sequence-to-sequence trimodal
hierarchical attention-based model that overcomes these challenges by letting
the model pay more attention to the text modality. MAST outperforms the current
state of the art model (video-text) by 2.51 points in terms of Content F1 score
and 1.00 points in terms of Rouge-L score on the How2 dataset for multimodal
language understanding.
",5.0
1062,10,22200,1309.5503,web archive,Scott G. Ainsworth and Michael L. Nelson,"Evaluating Sliding and Sticky Target Policies by Measuring Temporal
  Drift in Acyclic Walks Through a Web Archive","  When a user views an archived page using the archive's user interface (UI),
the user selects a datetime to view from a list. The archived web page, if
available, is then displayed. From this display, the web archive UI attempts to
simulate the web browsing experience by smoothly transitioning between archived
pages. During this process, the target datetime changes with each link
followed; drifting away from the datetime originally selected. When browsing
sparsely-archived pages, this nearly-silent drift can be many years in just a
few clicks. We conducted 200,000 acyclic walks of archived pages, following up
to 50 links per walk, comparing the results of two target datetime policies.
The Sliding Target policy allows the target datetime to change as it does in
archive UIs such as the Internet Archive's Wayback Machine. The Sticky Target
policy, represented by the Memento API, keeps the target datetime the same
throughout the walk. We found that the Sliding Target policy drift increases
with the number of walk steps, number of domains visited, and choice (number of
links available). However, the Sticky Target policy controls temporal drift,
holding it to less than 30 days on average regardless of walk length or number
of domains visited. The Sticky Target policy shows some increase as choice
increases, but this may be caused by other factors. We conclude that based on
walk length, the Sticky Target policy generally produces at least 30 days less
drift than the Sliding Target policy.
",5.0
1066,18,127641,2008.01503,infomation retrieval time complexity,"Ming-Wei Li, Qing-Yuan Jiang, Wu-Jun Li",Multiple Code Hashing for Efficient Image Retrieval,"  Due to its low storage cost and fast query speed, hashing has been widely
used in large-scale image retrieval tasks. Hash bucket search returns data
points within a given Hamming radius to each query, which can enable search at
a constant or sub-linear time cost. However, existing hashing methods cannot
achieve satisfactory retrieval performance for hash bucket search in complex
scenarios, since they learn only one hash code for each image. More
specifically, by using one hash code to represent one image, existing methods
might fail to put similar image pairs to the buckets with a small Hamming
distance to the query when the semantic information of images is complex. As a
result, a large number of hash buckets need to be visited for retrieving
similar images, based on the learned codes. This will deteriorate the
efficiency of hash bucket search. In this paper, we propose a novel hashing
framework, called multiple code hashing (MCH), to improve the performance of
hash bucket search. The main idea of MCH is to learn multiple hash codes for
each image, with each code representing a different region of the image.
Furthermore, we propose a deep reinforcement learning algorithm to learn the
parameters in MCH. To the best of our knowledge, this is the first work that
proposes to learn multiple hash codes for each image in image retrieval.
Experiments demonstrate that MCH can achieve a significant improvement in hash
bucket search, compared with existing methods that learn only one hash code for
each image.
",2.0
1072,7,55660,1707.05023,gradient boosting,"G\'erard Biau (LSTA, LPMA), Beno\^it Cadre (ENS Rennes, IRMAR)",Optimization by gradient boosting,"  Gradient boosting is a state-of-the-art prediction technique that
sequentially produces a model in the form of linear combinations of simple
predictors---typically decision trees---by solving an infinite-dimensional
convex optimization problem. We provide in the present paper a thorough
analysis of two widespread versions of gradient boosting, and introduce a
general framework for studying these algorithms from the point of view of
functional optimization. We prove their convergence as the number of iterations
tends to infinity and highlight the importance of having a strongly convex risk
functional to minimize. We also present a reasonable statistical context
ensuring consistency properties of the boosting predictors as the sample size
grows. In our approach, the optimization procedures are run forever (that is,
without resorting to an early stopping strategy), and statistical
regularization is basically achieved via an appropriate $L^2$ penalization of
the loss and strong convexity arguments.
",5.0
1076,19,152238,2104.01865,artificial intelligence for low carbon,"Thimal Kempitiya, Seppo Sierla, Daswin De Silva, Matti Yli-Ojanpera,
  Damminda Alahakoon, Valeriy Vyatkin","An Artificial Intelligence Framework for Bidding Optimization with
  Uncertainty in Multiple Frequency Reserve Markets","  The global ambitions of a carbon-neutral society necessitate a stable and
robust smart grid that capitalises on frequency reserves of renewable energy.
Frequency reserves are resources that adjust power production or consumption in
real time to react to a power grid frequency deviation. Revenue generation
motivates the availability of these resources for managing such deviations.
However, limited research has been conducted on data-driven decisions and
optimal bidding strategies for trading such capacities in multiple frequency
reserves markets. We address this limitation by making the following research
contributions. Firstly, a generalised model is designed based on an extensive
study of critical characteristics of global frequency reserves markets.
Secondly, three bidding strategies are proposed, based on this market model, to
capitalise on price peaks in multi-stage markets. Two strategies are proposed
for non-reschedulable loads, in which case the bidding strategy aims to select
the market with the highest anticipated price, and the third bidding strategy
focuses on rescheduling loads to hours on which highest reserve market prices
are anticipated. The third research contribution is an Artificial Intelligence
(AI) based bidding optimization framework that implements these three
strategies, with novel uncertainty metrics that supplement data-driven price
prediction. Finally, the framework is evaluated empirically using a case study
of multiple frequency reserves markets in Finland. The results from this
evaluation confirm the effectiveness of the proposed bidding strategies and the
AI-based bidding optimization framework in terms of cumulative revenue
generation, leading to an increased availability of frequency reserves.
",5.0
1080,14,192788,2204.07551,text summarization model,"Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke",Summarization with Graphical Elements,"  Automatic text summarization has experienced substantial progress in recent
years. With this progress, the question has arisen whether the types of
summaries that are typically generated by automatic summarization models align
with users' needs. Ter Hoeve et al (2020) answer this question negatively.
Amongst others, they recommend focusing on generating summaries with more
graphical elements. This is in line with what we know from the
psycholinguistics literature about how humans process text. Motivated from
these two angles, we propose a new task: summarization with graphical elements,
and we verify that these summaries are helpful for a critical mass of people.
We collect a high quality human labeled dataset to support research into the
task. We present a number of baseline methods that show that the task is
interesting and challenging. Hence, with this work we hope to inspire a new
line of research within the automatic summarization community.
",2.0
1085,16,83165,1902.09037,activation function in neutral networks,"Ivan Chelombiev, Conor Houghton, Cian O'Donnell",Adaptive Estimators Show Information Compression in Deep Neural Networks,"  To improve how neural networks function it is crucial to understand their
learning process. The information bottleneck theory of deep learning proposes
that neural networks achieve good generalization by compressing their
representations to disregard information that is not relevant to the task.
However, empirical evidence for this theory is conflicting, as compression was
only observed when networks used saturating activation functions. In contrast,
networks with non-saturating activation functions achieved comparable levels of
task performance but did not show compression. In this paper we developed more
robust mutual information estimation techniques, that adapt to hidden activity
of neural networks and produce more sensitive measurements of activations from
all functions, especially unbounded functions. Using these adaptive estimation
techniques, we explored compression in networks with a range of different
activation functions. With two improved methods of estimation, firstly, we show
that saturation of the activation function is not required for compression, and
the amount of compression varies between different activation functions. We
also find that there is a large amount of variation in compression between
different network initializations. Secondary, we see that L2 regularization
leads to significantly increased compression, while preventing overfitting.
Finally, we show that only compression of the last layer is positively
correlated with generalization.
",4.0
1086,7,198316,2206.00664,gradient boosting,"Bernhard Sch\""afl, Lukas Gruber, Angela Bitto-Nemling, Sepp Hochreiter",Hopular: Modern Hopfield Networks for Tabular Data,"  While Deep Learning excels in structured data as encountered in vision and
natural language processing, it failed to meet its expectations on tabular
data. For tabular data, Support Vector Machines (SVMs), Random Forests, and
Gradient Boosting are the best performing techniques with Gradient Boosting in
the lead. Recently, we saw a surge of Deep Learning methods that were tailored
to tabular data but still underperform compared to Gradient Boosting on
small-sized datasets. We suggest ""Hopular"", a novel Deep Learning architecture
for medium- and small-sized datasets, where each layer is equipped with
continuous modern Hopfield networks. The modern Hopfield networks use stored
data to identify feature-feature, feature-target, and sample-sample
dependencies. Hopular's novelty is that every layer can directly access the
original input as well as the whole training set via stored data in the
Hopfield networks. Therefore, Hopular can step-wise update its current model
and the resulting prediction at every layer like standard iterative learning
algorithms. In experiments on small-sized tabular datasets with less than 1,000
samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in
particular several Deep Learning methods. In experiments on medium-sized
tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost,
LightGBM and a state-of-the art Deep Learning method designed for tabular data.
Thus, Hopular is a strong alternative to these methods on tabular data.
",0.0
1092,3,107503,2001.03284,database management system,"Jang You Park, YongHee Jung, Wei Ding, Kwang Woo Nam",GeoCMS : Towards a Geo-Tagged Media Management System,"  In this paper, we propose the design and implementation of the new geotagged
media management system. A large amount of daily geo-tagged media data
generated by user's smart phone, mobile device, dash cam and camera. Geotagged
media, such as geovideos and geophotos, can be captured with spatial temporal
information such as time, location, visible area, camera direction, moving
direction and visible distance information. Due to the increase in geo-tagged
multimedia data, the researches for efficient managing and mining geo-tagged
multimedia are newly expected to be a new area in database and data mining.
This paper proposes a geo-tagged media management system, so called Open
GeoCMS(Geotagged media Contents Management System). Open GeoCMS is a new
framework to manage geotagged media data on the web. Our framework supports
various types which are for moving point, moving photo - a sequence of photos
by a drone, moving double and moving video. Also, GeoCMS has the label viewer
and editor system for photos and videos. The Open GeoCMS have been developed as
an open source system.
",2.0
1093,13,16869,1212.6808,social network analysis with natrual language processing,Richard Colbaugh and Kristin Glass,Early Warning Analysis for Social Diffusion Events,"  There is considerable interest in developing predictive capabilities for
social diffusion processes, for instance to permit early identification of
emerging contentious situations, rapid detection of disease outbreaks, or
accurate forecasting of the ultimate reach of potentially viral ideas or
behaviors. This paper proposes a new approach to this predictive analytics
problem, in which analysis of meso-scale network dynamics is leveraged to
generate useful predictions for complex social phenomena. We begin by deriving
a stochastic hybrid dynamical systems (S-HDS) model for diffusion processes
taking place over social networks with realistic topologies; this modeling
approach is inspired by recent work in biology demonstrating that S-HDS offer a
useful mathematical formalism with which to represent complex, multi-scale
biological network dynamics. We then perform formal stochastic reachability
analysis with this S-HDS model and conclude that the outcomes of social
diffusion processes may depend crucially upon the way the early dynamics of the
process interacts with the underlying network's community structure and
core-periphery structure. This theoretical finding provides the foundations for
developing a machine learning algorithm that enables accurate early warning
analysis for social diffusion events. The utility of the warning algorithm, and
the power of network-based predictive metrics, are demonstrated through an
empirical investigation of the propagation of political memes over social media
networks. Additionally, we illustrate the potential of the approach for
security informatics applications through case studies involving early warning
analysis of large-scale protests events and politically-motivated cyber
attacks.
",1.0
1095,5,52896,1705.00375,matrix completion,Natali Ruchansky and Mark Crovella and Evimaria Terzi,Targeted matrix completion,"  Matrix completion is a problem that arises in many data-analysis settings
where the input consists of a partially-observed matrix (e.g., recommender
systems, traffic matrix analysis etc.). Classical approaches to matrix
completion assume that the input partially-observed matrix is low rank. The
success of these methods depends on the number of observed entries and the rank
of the matrix; the larger the rank, the more entries need to be observed in
order to accurately complete the matrix. In this paper, we deal with matrices
that are not necessarily low rank themselves, but rather they contain low-rank
submatrices. We propose Targeted, which is a general framework for completing
such matrices. In this framework, we first extract the low-rank submatrices and
then apply a matrix-completion algorithm to these low-rank submatrices as well
as the remainder matrix separately. Although for the completion itself we use
state-of-the-art completion methods, our results demonstrate that Targeted
achieves significantly smaller reconstruction errors than other classical
matrix-completion methods. One of the key technical contributions of the paper
lies in the identification of the low-rank submatrices from the input
partially-observed matrices.
",5.0
1119,5,81578,1901.10429,matrix completion,"Duc Minh Nguyen, Robert Calderbank, Nikos Deligiannis",Geometric Matrix Completion with Deep Conditional Random Fields,"  The problem of completing high-dimensional matrices from a limited set of
observations arises in many big data applications, especially, recommender
systems. Existing matrix completion models generally follow either a memory- or
a model-based approach, whereas, geometric matrix completion models combine the
best from both approaches. Existing deep-learning-based geometric models yield
good performance, but, in order to operate, they require a fixed structure
graph capturing the relationships among the users and items. This graph is
typically constructed by evaluating a pre-defined similarity metric on the
available observations or by using side information, e.g., user profiles. In
contrast, Markov-random-fields-based models do not require a fixed structure
graph but rely on handcrafted features to make predictions. When no side
information is available and the number of available observations becomes very
low, existing solutions are pushed to their limits. In this paper, we propose a
geometric matrix completion approach that addresses these challenges. We
consider matrix completion as a structured prediction problem in a conditional
random field (CRF), which is characterized by a maximum a posterior (MAP)
inference, and we propose a deep model that predicts the missing entries by
solving the MAP inference problem. The proposed model simultaneously learns the
similarities among matrix entries, computes the CRF potentials, and solves the
inference problem. Its training is performed in an end-to-end manner, with a
method to supervise the learning of entry similarities. Comprehensive
experiments demonstrate the superior performance of the proposed model compared
to various state-of-the-art models on popular benchmark datasets and underline
its superior capacity to deal with highly incomplete matrices.
",5.0
1121,13,125924,2007.081,social network analysis with natrual language processing,"Paul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan
  Salakhutdinov, Louis-Philippe Morency",Towards Debiasing Sentence Representations,"  As natural language processing methods are increasingly deployed in
real-world scenarios such as healthcare, legal systems, and social science, it
becomes necessary to recognize the role they potentially play in shaping social
biases and stereotypes. Previous work has revealed the presence of social
biases in widely used word embeddings involving gender, race, religion, and
other social constructs. While some methods were proposed to debias these
word-level embeddings, there is a need to perform debiasing at the
sentence-level given the recent shift towards new contextualized sentence
representations such as ELMo and BERT. In this paper, we investigate the
presence of social biases in sentence-level representations and propose a new
method, Sent-Debias, to reduce these biases. We show that Sent-Debias is
effective in removing biases, and at the same time, preserves performance on
sentence-level downstream tasks such as sentiment analysis, linguistic
acceptability, and natural language understanding. We hope that our work will
inspire future research on characterizing and removing social biases from
widely adopted sentence representations for fairer NLP.
",2.0
1123,1,22024,1309.2434,advanced search engine,"Max Kemman, Martijn Kleppe, Stef Scagliola",Just Google It - Digital Research Practices of Humanities Scholars,"  The transition from analogue to digital archives and the recent explosion of
online content offers researchers novel ways of engaging with data. The crucial
question for ensuring a balance between the supply and demand-side of data, is
whether this trend connects to existing scholarly practices and to the average
search skills of researchers. To gain insight into this process we conducted a
survey among nearly three hundred (N= 288) humanities scholars in the
Netherlands and Belgium with the aim of finding answers to the following
questions: 1) To what extent are digital databases and archives used? 2) What
are the preferences in search functionalities 3) Are there differences in
search strategies between novices and experts of information retrieval? Our
results show that while scholars actively engage in research online they mainly
search for text and images. General search systems such as Google and JSTOR are
predominant, while large-scale collections such as Europeana are rarely
consulted. Searching with keywords is the dominant search strategy and advanced
search options are rarely used. When comparing novice and more experienced
searchers, the first tend to have a more narrow selection of search engines,
and mostly use keywords. Our overall findings indicate that Google is the key
player among available search engines. This dominant use illustrates the
paradoxical attitude of scholars toward Google: while provenance and context
are deemed key academic requirements, the workings of the Google algorithm
remain unclear. We conclude that Google introduces a black box into digital
scholarly practices, indicating scholars will become increasingly dependent on
such black boxed algorithms. This calls for a reconsideration of the academic
principles of provenance and context.
",4.0
1138,17,68090,1805.0937,robustness of neutral networks,"Fuxun Yu, Zirui Xu, Yanzhi Wang, Chenchen Liu, Xiang Chen","Towards Robust Training of Neural Networks by Regularizing Adversarial
  Gradients","  In recent years, neural networks have demonstrated outstanding effectiveness
in a large amount of applications.However, recent works have shown that neural
networks are susceptible to adversarial examples, indicating possible flaws
intrinsic to the network structures. To address this problem and improve the
robustness of neural networks, we investigate the fundamental mechanisms behind
adversarial examples and propose a novel robust training method via regulating
adversarial gradients. The regulation effectively squeezes the adversarial
gradients of neural networks and significantly increases the difficulty of
adversarial example generation.Without any adversarial example involved, the
robust training method could generate naturally robust networks, which are
near-immune to various types of adversarial examples. Experiments show the
naturally robust networks can achieve optimal accuracy against Fast Gradient
Sign Method (FGSM) and C\&W attacks on MNIST, Cifar10, and Google Speech
Command dataset. Moreover, our proposed method also provides neural networks
with consistent robustness against transferable attacks.
",5.0
1162,2,59222,1710.11004,random forests,"Masaya Hibino, Akisato Kimura, Takayoshi Yamashita, Yuji Yamauchi,
  Hironobu Fujiyoshi",Denoising random forests,"  This paper proposes a novel type of random forests called a denoising random
forests that are robust against noises contained in test samples. Such
noise-corrupted samples cause serious damage to the estimation performances of
random forests, since unexpected child nodes are often selected and the leaf
nodes that the input sample reaches are sometimes far from those for a clean
sample. Our main idea for tackling this problem originates from a binary
indicator vector that encodes a traversal path of a sample in the forest. Our
proposed method effectively employs this vector by introducing denoising
autoencoders into random forests. A denoising autoencoder can be trained with
indicator vectors produced from clean and noisy input samples, and non-leaf
nodes where incorrect decisions are made can be identified by comparing the
input and output of the trained denoising autoencoder. Multiple traversal paths
with respect to the nodes with incorrect decisions caused by the noises can
then be considered for the estimation.
",5.0
1170,11,14359,1207.3628,PageRank for web search,"Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay","Identify Web-page Content meaning using Knowledge based System for Dual
  Meaning Words","  Meaning of Web-page content plays a big role while produced a search result
from a search engine. Most of the cases Web-page meaning stored in title or
meta-tag area but those meanings do not always match with Web-page content. To
overcome this situation we need to go through the Web-page content to identify
the Web-page meaning. In such cases, where Webpage content holds dual meaning
words that time it is really difficult to identify the meaning of the Web-page.
In this paper, we are introducing a new design and development mechanism of
identifying the Web-page content meaning which holds dual meaning words in
their Web-page content.
",1.0
1213,10,21631,1308.2433,web archive,"Zhiwu Xie, Herbert Van de Sompel, Jinyang Liu, Johann van Reenen,
  Ramiro Jordan",Archiving the Relaxed Consistency Web,"  The historical, cultural, and intellectual importance of archiving the web
has been widely recognized. Today, all countries with high Internet penetration
rate have established high-profile archiving initiatives to crawl and archive
the fast-disappearing web content for long-term use. As web technologies
evolve, established web archiving techniques face challenges. This paper
focuses on the potential impact of the relaxed consistency web design on
crawler driven web archiving. Relaxed consistent websites may disseminate,
albeit ephemerally, inaccurate and even contradictory information. If captured
and preserved in the web archives as historical records, such information will
degrade the overall archival quality. To assess the extent of such quality
degradation, we build a simplified feed-following application and simulate its
operation with synthetic workloads. The results indicate that a non-trivial
portion of a relaxed consistency web archive may contain observable
inconsistency, and the inconsistency window may extend significantly longer
than that observed at the data store. We discuss the nature of such quality
degradation and propose a few possible remedies.
",5.0
1227,11,45407,1609.00004,PageRank for web search,Krishanu Deyasi,On the initial value of PageRank,"  PageRank is used by Google for ranking web pages to present search results
for a user query. Here, we have shown that the PageRank of a vertex depends on
its initial value which is also known as an intrinsic, non-network
contribution. We analytically proved that PageRank value of vertices become
proportional to their degrees if the intrinsic, non-network contributions of
the vertices are proportional to their degrees or zeros. Simulated and
empirical data are used to support our study. In addition, we have shown that
localization of PageRank depends on the intrinsic, non-network contribution.
",5.0
1237,2,130842,2009.05567,random forests,Jonathan Brophy and Daniel Lowd,Machine Unlearning for Random Forests,"  Responding to user data deletion requests, removing noisy examples, or
deleting corrupted training data are just a few reasons for wanting to delete
instances from a machine learning (ML) model. However, efficiently removing
this data from an ML model is generally difficult. In this paper, we introduce
data removal-enabled (DaRE) forests, a variant of random forests that enables
the removal of training data with minimal retraining. Model updates for each
DaRE tree in the forest are exact, meaning that removing instances from a DaRE
model yields exactly the same model as retraining from scratch on updated data.
  DaRE trees use randomness and caching to make data deletion efficient. The
upper levels of DaRE trees use random nodes, which choose split attributes and
thresholds uniformly at random. These nodes rarely require updates because they
only minimally depend on the data. At the lower levels, splits are chosen to
greedily optimize a split criterion such as Gini index or mutual information.
DaRE trees cache statistics at each node and training data at each leaf, so
that only the necessary subtrees are updated as data is removed. For numerical
attributes, greedy nodes optimize over a random subset of thresholds, so that
they can maintain statistics while approximating the optimal threshold. By
adjusting the number of thresholds considered for greedy nodes, and the number
of random nodes, DaRE trees can trade off between more accurate predictions and
more efficient updates.
  In experiments on 13 real-world datasets and one synthetic dataset, we find
DaRE forests delete data orders of magnitude faster than retraining from
scratch while sacrificing little to no predictive power.
",4.0
1239,18,121242,2006.05228,infomation retrieval time complexity,"Antoine Maillard, Bruno Loureiro, Florent Krzakala, Lenka Zdeborov\'a","Phase retrieval in high dimensions: Statistical and computational phase
  transitions","  We consider the phase retrieval problem of reconstructing a $n$-dimensional
real or complex signal $\mathbf{X}^{\star}$ from $m$ (possibly noisy)
observations $Y_\mu = | \sum_{i=1}^n \Phi_{\mu i} X^{\star}_i/\sqrt{n}|$, for a
large class of correlated real and complex random sensing matrices
$\mathbf{\Phi}$, in a high-dimensional setting where $m,n\to\infty$ while
$\alpha = m/n=\Theta(1)$. First, we derive sharp asymptotics for the lowest
possible estimation error achievable statistically and we unveil the existence
of sharp phase transitions for the weak- and full-recovery thresholds as a
function of the singular values of the matrix $\mathbf{\Phi}$. This is achieved
by providing a rigorous proof of a result first obtained by the replica method
from statistical mechanics. In particular, the information-theoretic transition
to perfect recovery for full-rank matrices appears at $\alpha=1$ (real case)
and $\alpha=2$ (complex case). Secondly, we analyze the performance of the
best-known polynomial time algorithm for this problem -- approximate
message-passing -- establishing the existence of a statistical-to-algorithmic
gap depending, again, on the spectral properties of $\mathbf{\Phi}$. Our work
provides an extensive classification of the statistical and algorithmic
thresholds in high-dimensional phase retrieval for a broad class of random
matrices.
",0.0
1247,5,120998,2006.04373,matrix completion,"Qiaosheng Zhang, Geewon Suh, Changho Suh, Vincent Y. F. Tan","MC2G: An Efficient Algorithm for Matrix Completion with Social and Item
  Similarity Graphs","  In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.
",5.0
1268,0,188275,2203.02696,learning to rank with partitioned preference,"Nassim Belmecheri and Noureddine Aribi and Nadjib Lazaar and Yahia
  Lebbah and Samir Loudni",Boosting the Learning for Ranking Patterns,"  Discovering relevant patterns for a particular user remains a challenging
tasks in data mining. Several approaches have been proposed to learn
user-specific pattern ranking functions. These approaches generalize well, but
at the expense of the running time. On the other hand, several measures are
often used to evaluate the interestingness of patterns, with the hope to reveal
a ranking that is as close as possible to the user-specific ranking. In this
paper, we formulate the problem of learning pattern ranking functions as a
multicriteria decision making problem. Our approach aggregates different
interestingness measures into a single weighted linear ranking function, using
an interactive learning procedure that operates in either passive or active
modes. A fast learning step is used for eliciting the weights of all the
measures by mean of pairwise comparisons.
  This approach is based on Analytic Hierarchy Process (AHP), and a set of
user-ranked patterns to build a preference matrix, which compares the
importance of measures according to the user-specific interestingness. A
sensitivity based heuristic is proposed for the active learning mode, in order
to insure high quality results with few user ranking queries. Experiments
conducted on well-known datasets show that our approach significantly reduces
the running time and returns precise pattern ranking, while being robust to
user-error compared with state-of-the-art approaches.
",1.0
1271,0,202860,2207.03609,learning to rank with partitioned preference,"Gregory Canal, Blake Mason, Ramya Korlakai Vinayak, Robert Nowak","One for All: Simultaneous Metric and Preference Learning over Multiple
  Users","  This paper investigates simultaneous preference and metric learning from a
crowd of respondents. A set of items represented by $d$-dimensional feature
vectors and paired comparisons of the form ``item $i$ is preferable to item
$j$'' made by each user is given. Our model jointly learns a distance metric
that characterizes the crowd's general measure of item similarities along with
a latent ideal point for each user reflecting their individual preferences.
This model has the flexibility to capture individual preferences, while
enjoying a metric learning sample cost that is amortized over the crowd. We
first study this problem in a noiseless, continuous response setting (i.e.,
responses equal to differences of item distances) to understand the fundamental
limits of learning. Next, we establish prediction error guarantees for noisy,
binary measurements such as may be collected from human respondents, and show
how the sample complexity improves when the underlying metric is low-rank.
Finally, we establish recovery guarantees under assumptions on the response
distribution. We demonstrate the performance of our model on both simulated
data and on a dataset of color preference judgements across a large number of
users.
",0.0
1277,7,111756,2002.11896,gradient boosting,Robert Giaquinto and Arindam Banerjee,Gradient Boosted Normalizing Flows,"  By chaining a sequence of differentiable invertible transformations,
normalizing flows (NF) provide an expressive method of posterior approximation,
exact density evaluation, and sampling. The trend in normalizing flow
literature has been to devise deeper, more complex transformations to achieve
greater flexibility. We propose an alternative: Gradient Boosted Normalizing
Flows (GBNF) model a density by successively adding new NF components with
gradient boosting. Under the boosting framework, each new NF component
optimizes a sample weighted likelihood objective, resulting in new components
that are fit to the residuals of the previously trained components. The GBNF
formulation results in a mixture model structure, whose flexibility increases
as more components are added. Moreover, GBNFs offer a wider, as opposed to
strictly deeper, approach that improves existing NFs at the cost of additional
training---not more complex transformations. We demonstrate the effectiveness
of this technique for density estimation and, by coupling GBNF with a
variational autoencoder, generative modeling of images. Our results show that
GBNFs outperform their non-boosted analog, and, in some cases, produce better
results with smaller, simpler flows.
",5.0
1287,16,191834,2204.02921,activation function in neutral networks,Murilo Gustineli,A survey on recently proposed activation functions for Deep Learning,"  Artificial neural networks (ANN), typically referred to as neural networks,
are a class of Machine Learning algorithms and have achieved widespread
success, having been inspired by the biological structure of the human brain.
Neural networks are inherently powerful due to their ability to learn complex
function approximations from data. This generalization ability has been able to
impact multidisciplinary areas involving image recognition, speech recognition,
natural language processing, and others. Activation functions are a crucial
sub-component of neural networks. They define the output of a node in the
network given a set of inputs. This survey discusses the main concepts of
activation functions in neural networks, including; a brief introduction to
deep neural networks, a summary of what are activation functions and how they
are used in neural networks, their most common properties, the different types
of activation functions, some of the challenges, limitations, and alternative
solutions faced by activation functions, concluding with the final remarks.
",5.0
1288,8,63687,1802.06368,node embedding for graph,"Kento Nozawa, Masanari Kimura, Atsunori Kanemura","Node Centralities and Classification Performance for Characterizing Node
  Embedding Algorithms","  Embedding graph nodes into a vector space can allow the use of machine
learning to e.g. predict node classes, but the study of node embedding
algorithms is immature compared to the natural language processing field
because of a diverse nature of graphs. We examine the performance of node
embedding algorithms with respect to graph centrality measures that
characterize diverse graphs, through systematic experiments with four node
embedding algorithms, four or five graph centralities, and six datasets.
Experimental results give insights into the properties of node embedding
algorithms, which can be a basis for further research on this topic.
",5.0
1297,16,92037,1906.09529,activation function in neutral networks,"Mohit Goyal, Rajan Goyal, Brejesh Lall","Learning Activation Functions: A new paradigm for understanding Neural
  Networks","  The scope of research in the domain of activation functions remains limited
and centered around improving the ease of optimization or generalization
quality of neural networks (NNs). However, to develop a deeper understanding of
deep learning, it becomes important to look at the non linear component of NNs
more carefully. In this paper, we aim to provide a generic form of activation
function along with appropriate mathematical grounding so as to allow for
insights into the working of NNs in future. We propose ""Self-Learnable
Activation Functions"" (SLAF), which are learned during training and are capable
of approximating most of the existing activation functions. SLAF is given as a
weighted sum of pre-defined basis elements which can serve for a good
approximation of the optimal activation function. The coefficients for these
basis elements allow a search in the entire space of continuous functions
(consisting of all the conventional activations). We propose various training
routines which can be used to achieve performance with SLAF equipped neural
networks (SLNNs). We prove that SLNNs can approximate any neural network with
lipschitz continuous activations, to any arbitrary error highlighting their
capacity and possible equivalence with standard NNs. Also, SLNNs can be
completely represented as a collections of finite degree polynomial upto the
very last layer obviating several hyper parameters like width and depth. Since
the optimization of SLNNs is still a challenge, we show that using SLAF along
with standard activations (like ReLU) can provide performance improvements with
only a small increase in number of parameters.
",5.0
1307,4,182033,2201.03472,pre-trained language model,Peter Nightingale,Savile Row Manual,"  We describe the constraint modelling tool Savile Row, its input language and
its main features. Savile Row translates a solver-independent constraint
modelling language to the input languages for various solvers including
constraint, SAT, and SMT solvers. After a brief introduction, the manual
describes the Essence Prime language, which is the input language of Savile
Row. Then we describe the functions of the tool, its main features and options
and how to install and use it.
",3.0
1313,11,70440,1807.01857,PageRank for web search,"Mohammad Masudur Rahman, Shamima Yeasmin and Chanchal K. Roy",An IDE-Based Context-Aware Meta Search Engine,"  Traditional web search forces the developers to leave their working
environments and look for solutions in the web browsers. It often does not
consider the context of their programming problems. The context-switching
between the web browser and the working environment is time-consuming and
distracting, and the keyword-based traditional search often does not help much
in problem solving. In this paper, we propose an Eclipse IDE-based web search
solution that collects the data from three web search APIs-- Google, Yahoo,
Bing and a programming Q & A site-- Stack Overflow. It then provides search
results within IDE taking not only the content of the selected error into
account but also the problem context, popularity and search engine
recommendation of the result links. Experiments with 25 run time errors and
exceptions show that the proposed approach outperforms the keyword-based search
approaches with a recommendation accuracy of 96%. We also validate the results
with a user study involving five prospective participants where we get a result
agreement of 64.28%. While the preliminary results are promising, the approach
needs to be further validated with more errors and exceptions followed by a
user study with more participants to establish itself as a complete IDE-based
web search solution.
",1.0
1320,14,153826,2104.09428,text summarization model,"Jamal Al Qundus, Silvio Peikert, Adrian Paschke",AI supported Topic Modeling using KNIME-Workflows,"  Topic modeling algorithms traditionally model topics as list of weighted
terms. These topic models can be used effectively to classify texts or to
support text mining tasks such as text summarization or fact extraction. The
general procedure relies on statistical analysis of term frequencies. The focus
of this work is on the implementation of the knowledge-based topic modelling
services in a KNIME workflow. A brief description and evaluation of the
DBPedia-based enrichment approach and the comparative evaluation of enriched
topic models will be outlined based on our previous work. DBpedia-Spotlight is
used to identify entities in the input text and information from DBpedia is
used to extend these entities. We provide a workflow developed in KNIME
implementing this approach and perform a result comparison of topic modeling
supported by knowledge base information to traditional LDA. This topic modeling
approach allows semantic interpretation both by algorithms and by humans.
",1.0
1323,14,167782,2108.13741,text summarization model,"Huy Quoc To, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen, Anh Gia-Tuan
  Nguyen","Monolingual versus Multilingual BERTology for Vietnamese Extractive
  Multi-Document Summarization","  Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese on multi-document. We introduce a novel comparison
between different multilingual and monolingual BERT models. The experiment
results indicate that monolingual models produce promising results compared to
other multilingual models and previous text summarizing models for Vietnamese.
",5.0
1330,7,55046,1706.09516,gradient boosting,"Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika
  Dorogush, Andrey Gulin",CatBoost: unbiased boosting with categorical features,"  This paper presents the key algorithmic techniques behind CatBoost, a new
gradient boosting toolkit. Their combination leads to CatBoost outperforming
other publicly available boosting implementations in terms of quality on a
variety of datasets. Two critical algorithmic advances introduced in CatBoost
are the implementation of ordered boosting, a permutation-driven alternative to
the classic algorithm, and an innovative algorithm for processing categorical
features. Both techniques were created to fight a prediction shift caused by a
special kind of target leakage present in all currently existing
implementations of gradient boosting algorithms. In this paper, we provide a
detailed analysis of this problem and demonstrate that proposed algorithms
solve it effectively, leading to excellent empirical results.
",5.0
1340,2,20507,1306.0237,random forests,Houtao Deng,Guided Random Forest in the RRF Package,"  Random Forest (RF) is a powerful supervised learner and has been popularly
used in many applications such as bioinformatics.
  In this work we propose the guided random forest (GRF) for feature selection.
Similar to a feature selection method called guided regularized random forest
(GRRF), GRF is built using the importance scores from an ordinary RF. However,
the trees in GRRF are built sequentially, are highly correlated and do not
allow for parallel computing, while the trees in GRF are built independently
and can be implemented in parallel. Experiments on 10 high-dimensional gene
data sets show that, with a fixed parameter value (without tuning the
parameter), RF applied to features selected by GRF outperforms RF applied to
all features on 9 data sets and 7 of them have significant differences at the
0.05 level. Therefore, both accuracy and interpretability are significantly
improved. GRF selects more features than GRRF, however, leads to better
classification accuracy. Note in this work the guided random forest is guided
by the importance scores from an ordinary random forest, however, it can also
be guided by other methods such as human insights (by specifying $\lambda_i$).
GRF can be used in ""RRF"" v1.4 (and later versions), a package that also
includes the regularized random forest methods.
",5.0
1341,3,216823,cs/0305038,database management system,Nancy Hartline Bercich,The Evolution of the Computerized Database,"  Databases, collections of related data, are as old as the written word. A
database can be anything from a homemaker's metal recipe file to a
sophisticated data warehouse. Yet today, when we think of a database we
invariably think of computerized data and their DBMSs (database management
systems). How did we go from organizing our data in a simple metal filing box
or cabinet to storing our data in a sophisticated computerized database? How
did the computerized database evolve?
  This paper defines what we mean by a database. It traces the evolution of the
database, from its start as a non-computerized set of related data, to the, now
standard, computerized RDBMS (relational database management system). Early
computerized storage methods are reviewed including both the ISAM (Indexed
Sequential Access Method) and VSAM (Virtual Storage Access Method) storage
methods. Early database models are explored including the network and
hierarchical database models. Eventually, the relational, object-relational and
object-oriented databases models are discussed. An appendix of diagrams,
including hierarchical occurrence tree, network schema, ER (entity
relationship) and UML (unified modeling language) diagrams, is included to
support the text.
  This paper concludes with an exploration of current and future trends in DBMS
development. It discusses the factors affecting these trends. It delves into
the relationship between DBMSs and the increasingly popular object-oriented
development methodologies. Finally, it speculates on the future of the DBMS.
",5.0
1347,5,50501,1702.06237,matrix completion,"Aaron Potechin, David Steurer",Exact tensor completion with sum-of-squares,"  We obtain the first polynomial-time algorithm for exact tensor completion
that improves over the bound implied by reduction to matrix completion. The
algorithm recovers an unknown 3-tensor with $r$ incoherent, orthogonal
components in $\mathbb R^n$ from $r\cdot \tilde O(n^{1.5})$ randomly observed
entries of the tensor. This bound improves over the previous best one of
$r\cdot \tilde O(n^{2})$ by reduction to exact matrix completion. Our bound
also matches the best known results for the easier problem of approximate
tensor completion (Barak & Moitra, 2015).
  Our algorithm and analysis extends seminal results for exact matrix
completion (Candes & Recht, 2009) to the tensor setting via the sum-of-squares
method. The main technical challenge is to show that a small number of randomly
chosen monomials are enough to construct a degree-3 polynomial with precisely
planted orthogonal global optima over the sphere and that this fact can be
certified within the sum-of-squares proof system.
",4.0
1354,16,50676,1702.0779,activation function in neutral networks,"Mark Harmon, Diego Klabjan",Activation Ensembles for Deep Neural Networks,"  Many activation functions have been proposed in the past, but selecting an
adequate one requires trial and error. We propose a new methodology of
designing activation functions within a neural network at each layer. We call
this technique an ""activation ensemble"" because it allows the use of multiple
activation functions at each layer. This is done by introducing additional
variables, $\alpha$, at each activation layer of a network to allow for
multiple activation functions to be active at each neuron. By design,
activations with larger $\alpha$ values at a neuron is equivalent to having the
largest magnitude. Hence, those higher magnitude activations are ""chosen"" by
the network. We implement the activation ensembles on a variety of datasets
using an array of Feed Forward and Convolutional Neural Networks. By using the
activation ensemble, we achieve superior results compared to traditional
techniques. In addition, because of the flexibility of this methodology, we
more deeply explore activation functions and the features that they capture.
",5.0
1355,11,2803,903.4035,PageRank for web search,"A. Kritikopoulos, M. Sideri, I. Varlamis",BLOGRANK: Ranking Weblogs Based On Connectivity And Similarity Features,"  A large part of the hidden web resides in weblog servers. New content is
produced in a daily basis and the work of traditional search engines turns to
be insufficient due to the nature of weblogs. This work summarizes the
structure of the blogosphere and highlights the special features of weblogs. In
this paper we present a method for ranking weblogs based on the link graph and
on several similarity characteristics between weblogs. First we create an
enhanced graph of connected weblogs and add new types of edges and weights
utilising many weblog features. Then, we assign a ranking to each weblog using
our algorithm, BlogRank, which is a modified version of PageRank. For the
validation of our method we run experiments on a weblog dataset, which we
process and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).
The results suggest that the use of the enhanced graph and the BlogRank
algorithm is preferred by the users.
",5.0
1363,10,140321,2012.03397,web archive,"Yasith Jayawardana, Alexander C. Nwala, Gavindya Jayawardena, Jian Wu,
  Sampath Jayarathna, Michael L. Nelson, C. Lee Giles",Modeling Updates of Scholarly Webpages Using Archived Data,"  The vastness of the web imposes a prohibitive cost on building large-scale
search engines with limited resources. Crawl frontiers thus need to be
optimized to improve the coverage and freshness of crawled content. In this
paper, we propose an approach for modeling the dynamics of change in the web
using archived copies of webpages. To evaluate its utility, we conduct a
preliminary study on the scholarly web using 19,977 seed URLs of authors'
homepages obtained from their Google Scholar profiles. We first obtain archived
copies of these webpages from the Internet Archive (IA), and estimate when
their actual updates occurred. Next, we apply maximum likelihood to estimate
their mean update frequency ($\lambda$) values. Our evaluation shows that
$\lambda$ values derived from a short history of archived data provide a good
estimate for the true update frequency in the short-term, and that our method
provides better estimations of updates at a fraction of resources compared to
the baseline models. Based on this, we demonstrate the utility of archived data
to optimize the crawling strategy of web crawlers, and uncover important
challenges that inspire future research directions.
",5.0
1383,2,94948,1908.01109,random forests,"Ningyuan Chen, Guillermo Gallego, Zhuodong Tang",The Use of Binary Choice Forests to Model and Estimate Discrete Choices,"  We show the equivalence of discrete choice models and a forest of binary
decision trees. This suggests that standard machine learning techniques based
on random forests can serve to estimate discrete choice models with an
interpretable output: the underlying trees can be viewed as the internal choice
process of customers. Our data-driven theoretical results show that random
forests can predict the choice probability of any discrete choice model
consistently. Moreover, our algorithm predicts unseen assortments with
mechanisms and errors that can be theoretically analyzed. We also prove that
the splitting criterion in random forests, the Gini index, is capable of
recovering preference rankings of customers. The framework has unique practical
advantages: it can capture behavioral patterns such as irrationality or
sequential searches; it handles nonstandard formats of training data that
result from aggregation; it can measure product importance based on how
frequently a random customer would make decisions depending on the presence of
the product; it can also incorporate price information and customer features.
Our numerical results show that using random forests to estimate customer
choices can outperform the best parametric models in synthetic and real
datasets when presented with enough data or when the underlying discrete choice
model cannot be correctly specified by existing parametric models.
",3.0
1385,0,127380,2007.16173,learning to rank with partitioned preference,"Taher Hekmatfar, Saman Haratizadeh, Sama Goliaei",Embedding Ranking-Oriented Recommender System Graphs,"  Graph-based recommender systems (GRSs) analyze the structural information in
the graphical representation of data to make better recommendations, especially
when the direct user-item relation data is sparse. Ranking-oriented GRSs that
form a major class of recommendation systems, mostly use the graphical
representation of preference (or rank) data for measuring node similarities,
from which they can infer a recommendation list using a neighborhood-based
mechanism. In this paper, we propose PGRec, a novel graph-based
ranking-oriented recommendation framework. PGRec models the preferences of the
users over items, by a novel graph structure called PrefGraph. This graph is
then exploited by an improved embedding approach, taking advantage of both
factorization and deep learning methods, to extract vectors representing users,
items, and preferences. The resulting embedding are then used for predicting
users' unknown pairwise preferences from which the final recommendation lists
are inferred. We have evaluated the performance of the proposed method against
the state of the art model-based and neighborhood-based recommendation methods,
and our experiments show that PGRec outperforms the baseline algorithms up to
3.2% in terms of NDCG@10 in different MovieLens datasets.
",1.0
1387,8,89482,1905.11691,node embedding for graph,Valeria Fionda and Giuseppe Pirr\'o,Triple2Vec: Learning Triple Embeddings from Knowledge Graphs,"  Graph embedding techniques allow to learn high-quality feature vectors from
graph structures and are useful in a variety of tasks, from node classification
to clustering. Existing approaches have only focused on learning feature
vectors for the nodes in a (knowledge) graph. To the best of our knowledge,
none of them has tackled the problem of embedding of graph edges, that is,
knowledge graph triples. The approaches that are closer to this task have
focused on homogeneous graphs involving only one type of edge and obtain edge
embeddings by applying some operation (e.g., average) on the embeddings of the
endpoint nodes. The goal of this paper is to introduce Triple2Vec, a new
technique to directly embed edges in (knowledge) graphs. Trple2Vec builds upon
three main ingredients. The first is the notion of line graph. The line graph
of a graph is another graph representing the adjacency between edges of the
original graph. In particular, the nodes of the line graph are the edges of the
original graph. We show that directly applying existing embedding techniques on
the nodes of the line graph to learn edge embeddings is not enough in the
context of knowledge graphs. Thus, we introduce the notion of triple line
graph. The second is an edge weighting mechanism both for line graphs derived
from knowledge graphs and homogeneous graphs. The third is a strategy based on
graph walks on the weighted triple line graph that can preserve proximity
between nodes. Embeddings are finally generated by adopting the SkipGram model,
where sentences are replaced with graph walks. We evaluate our approach on
different real world (knowledge) graphs and compared it with related work.
",5.0
1391,14,194052,2204.13512,text summarization model,"Ruipeng Jia, Xingxing Zhang, Yanan Cao, Shi Wang, Zheng Lin, Furu Wei",Neural Label Search for Zero-Shot Multi-Lingual Extractive Summarization,"  In zero-shot multilingual extractive text summarization, a model is typically
trained on English summarization dataset and then applied on summarization
datasets of other languages. Given English gold summaries and documents,
sentence-level labels for extractive summarization are usually generated using
heuristics. However, these monolingual labels created on English datasets may
not be optimal on datasets of other languages, for that there is the syntactic
or semantic discrepancy between different languages. In this way, it is
possible to translate the English dataset to other languages and obtain
different sets of labels again using heuristics. To fully leverage the
information of these different sets of labels, we propose NLSSum (Neural Label
Search for Summarization), which jointly learns hierarchical weights for these
different sets of labels together with our summarization model. We conduct
multilingual zero-shot summarization experiments on MLSUM and WikiLingua
datasets, and we achieve state-of-the-art results using both human and
automatic evaluations across these two datasets.
",4.0
1392,0,37395,1511.0075,learning to rank with partitioned preference,"Franco Berbeglia, Gerardo Berbeglia, and Pascal Van Hentenryck",Market Segmentation in Online Platforms,"  This paper studies ranking policies in a stylized trial-offer marketplace
model, in which a single firm offers products and has consumers with
heterogeneous preferences. Consumer trials are influenced by past purchases and
the ranking of each product. The platform owner needs to devise a ranking
policy to display the products to maximize the number of purchases in the long
run. The model proposed attempts to understand the impact of market
segmentation in a trial-offer market with social influence. In our model,
consumer choices are based on a very general choice model known as the mixed
MNL. We analyze the long-term dynamics of this highly complex stochastic model
and we quantify the expected benefits of market segmentation. When past
purchases are displayed, consumer heterogeneity makes buyers try the
sub-optimal products, reducing the overall sales rate. We show that consumer
heterogeneity makes the ranking problem NP-hard. We then analyze the benefits
of market segmentation. We find tight bounds to the expected benefits of
offering a distinct ranking to each consumer segment. Finally, we show that the
market segmentation strategy always benefits from social influence when the
average quality ranking is used. One of the managerial implications is that the
firm is better off using an aggregate ranking policy when the variety of
consumer preference is limited, but it should perform a market segmentation
policy when consumers are highly heterogeneous. We also show that this result
is robust to relatively small consumer classification mistakes; when these are
large, an aggregate ranking is preferred.
",1.0
1393,8,108446,2001.08503,node embedding for graph,"N. Nikzad-Khasmakhi, M. A. Balafar, M.Reza Feizi-Derakhshi, Cina
  Motamed","ExEm: Expert Embedding using dominating set theory with deep learning
  approaches","  A collaborative network is a social network that is comprised of experts who
cooperate with each other to fulfill a special goal. Analyzing this network
yields meaningful information about the expertise of these experts and their
subject areas. To perform the analysis, graph embedding techniques have emerged
as an effective and promising tool. Graph embedding attempts to represent graph
nodes as low-dimensional vectors. In this paper, we propose a graph embedding
method, called ExEm, that uses dominating-set theory and deep learning
approaches to capture node representations. ExEm finds dominating nodes of the
collaborative network and constructs intelligent random walks that comprise of
at least two dominating nodes. One dominating node should appear at the
beginning of each path sampled to characterize the local neighborhoods.
Moreover, the second dominating node reflects the global structure information.
To learn the node embeddings, ExEm exploits three embedding methods including
Word2vec, fastText and the concatenation of these two. The final result is the
low-dimensional vectors of experts, called expert embeddings. The extracted
expert embeddings can be applied to many applications. In order to extend these
embeddings into the expert recommendation system, we introduce a novel strategy
that uses expert vectors to calculate experts' scores and recommend experts. At
the end, we conduct extensive experiments to validate the effectiveness of ExEm
through assessing its performance over the multi-label classification, link
prediction, and recommendation tasks on common datasets and our collected data
formed by crawling the vast author Scopus profiles. The experiments show that
ExEm outperforms the baselines especially in dense networks.
",4.0
1397,1,210273,2209.1112,advanced search engine,"Mykola Makhortykh, Aleksandra Urman, Roberto Ulloa","This is what a pandemic looks like: Visual framing of COVID-19 on search
  engines","  In today's high-choice media environment, search engines play an integral
role in informing individuals and societies about the latest events. The
importance of search algorithms is even higher at the time of crisis, when
users search for information to understand the causes and the consequences of
the current situation and decide on their course of action. In our paper, we
conduct a comparative audit of how different search engines prioritize visual
information related to COVID-19 and what consequences it has for the
representation of the pandemic. Using a virtual agent-based audit approach, we
examine image search results for the term ""coronavirus"" in English, Russian and
Chinese on five major search engines: Google, Yandex, Bing, Yahoo, and
DuckDuckGo. Specifically, we focus on how image search results relate to
generic news frames (e.g., the attribution of responsibility, human interest,
and economics) used in relation to COVID-19 and how their visual composition
varies between the search engines.
",2.0
1403,8,97150,1909.02977,node embedding for graph,"Chi Thang Duong, Hongzhi Yin, Thanh Dat Hoang, Truong Giang Le Ba,
  Matthias Weidlich, Quoc Viet Hung Nguyen, Karl Aberer",Parallel Computation of Graph Embeddings,"  Graph embedding aims at learning a vector-based representation of vertices
that incorporates the structure of the graph. This representation then enables
inference of graph properties. Existing graph embedding techniques, however, do
not scale well to large graphs. We therefore propose a framework for parallel
computation of a graph embedding using a cluster of compute nodes with resource
constraints. We show how to distribute any existing embedding technique by
first splitting a graph for any given set of constrained compute nodes and then
reconciling the embedding spaces derived for these subgraphs. We also propose a
new way to evaluate the quality of graph embeddings that is independent of a
specific inference task. Based thereon, we give a formal bound on the
difference between the embeddings derived by centralised and parallel
computation. Experimental results illustrate that our approach for parallel
computation scales well, while largely maintaining the embedding quality.
",5.0
1405,10,21775,1308.4839,web archive,Zeynep Pehlivan and Benjamin Piwowarski and St\'ephane Gan\c{c}arski,"Diversification Based Static Index Pruning - Application to Temporal
  Collections","  Nowadays, web archives preserve the history of large portions of the web. As
medias are shifting from printed to digital editions, accessing these huge
information sources is drawing increasingly more attention from national and
international institutions, as well as from the research community. These
collections are intrinsically big, leading to index files that do not fit into
the memory and an increase query response time. Decreasing the index size is a
direct way to decrease this query response time.
  Static index pruning methods reduce the size of indexes by removing a part of
the postings. In the context of web archives, it is necessary to remove
postings while preserving the temporal diversity of the archive. None of the
existing pruning approaches take (temporal) diversification into account.
  In this paper, we propose a diversification-based static index pruning
method. It differs from the existing pruning approaches by integrating
diversification within the pruning context. We aim at pruning the index while
preserving retrieval effectiveness and diversity by pruning while maximizing a
given IR evaluation metric like DCG. We show how to apply this approach in the
context of web archives. Finally, we show on two collections that search
effectiveness in temporal collections after pruning can be improved using our
approach rather than diversity oblivious approaches.
",5.0
1408,18,197897,2205.15086,infomation retrieval time complexity,"Hernan C. Vazquez, J. Andres Diaz Pace, Claudia Marcos and Santiago
  Vidal","Retrieving and Ranking Relevant JavaScript Technologies from Web
  Repositories","  The selection of software technologies is an important but complex task. We
consider developers of JavaScript (JS) applications, for whom the assessment of
JS libraries has become difficult and time-consuming due to the growing number
of technology options available. A common strategy is to browse software
repositories via search engines (e.g., NPM, or Google), although it brings some
problems. First, given a technology need, the engines might return a long list
of results, which often causes information overload issues. Second, the results
should be ranked according to criteria of interest for the developer. However,
deciding how to weight these criteria to make a decision is not
straightforward. In this work, we propose a two-phase approach for assisting
developers to retrieve and rank JS technologies in a semi-automated fashion.
The first-phase (ST-Retrieval) uses a meta-search technique for collecting JS
technologies that meet the developer's needs. The second-phase (called
ST-Rank), relies on a machine learning technique to infer, based on criteria
used by other projects in the Web, a ranking of the output of ST-Retrieval. We
evaluated our approach with NPM and obtained satisfactory results in terms of
the accuracy of the technologies retrieved and the order in which they were
ranked.
",1.0
1410,16,123177,2006.12253,activation function in neutral networks,"Victor Geadah, Giancarlo Kerg, Stefan Horoi, Guy Wolf, Guillaume
  Lajoie","Advantages of biologically-inspired adaptive neural activation in RNNs
  during learning","  Dynamic adaptation in single-neuron response plays a fundamental role in
neural coding in biological neural networks. Yet, most neural activation
functions used in artificial networks are fixed and mostly considered as an
inconsequential architecture choice. In this paper, we investigate nonlinear
activation function adaptation over the large time scale of learning, and
outline its impact on sequential processing in recurrent neural networks. We
introduce a novel parametric family of nonlinear activation functions, inspired
by input-frequency response curves of biological neurons, which allows
interpolation between well-known activation functions such as ReLU and sigmoid.
Using simple numerical experiments and tools from dynamical systems and
information theory, we study the role of neural activation features in learning
dynamics. We find that activation adaptation provides distinct task-specific
solutions and in some cases, improves both learning speed and performance.
Importantly, we find that optimal activation features emerging from our
parametric family are considerably different from typical functions used in the
literature, suggesting that exploiting the gap between these usual
configurations can help learning. Finally, we outline situations where neural
activation adaptation alone may help mitigate changes in input statistics in a
given task, suggesting mechanisms for transfer learning optimization.
",5.0
1434,7,88577,1905.07558,gradient boosting,"Arnaud Joly, Louis Wehenkel and Pierre Geurts","Gradient tree boosting with random output projections for multi-label
  classification and multi-output regression","  In many applications of supervised learning, multiple classification or
regression outputs have to be predicted jointly. We consider several extensions
of gradient boosting to address such problems. We first propose a
straightforward adaptation of gradient boosting exploiting multiple output
regression trees as base learners. We then argue that this method is only
expected to be optimal when the outputs are fully correlated, as it forces the
partitioning induced by the tree base learners to be shared by all outputs. We
then propose a novel extension of gradient tree boosting to specifically
address this issue. At each iteration of this new method, a regression tree
structure is grown to fit a single random projection of the current residuals
and the predictions of this tree are fitted linearly to the current residuals
of all the outputs, independently. Because of this linear fit, the method can
adapt automatically to any output correlation structure. Extensive experiments
are conducted with this method, as well as other algorithmic variants, on
several artificial and real problems. Randomly projecting the output space is
shown to provide a better adaptation to different output correlation patterns
and is therefore competitive with the best of the other methods in most
settings. Thanks to model sharing, the convergence speed is also improved,
reducing the computing times (or the complexity of the model) to reach a
specific accuracy.
",5.0
1437,16,130965,2009.06132,activation function in neutral networks,Zhong Li and Chao Ma and Lei Wu,"Complexity Measures for Neural Networks with General Activation
  Functions Using Path-based Norms","  A simple approach is proposed to obtain complexity controls for neural
networks with general activation functions. The approach is motivated by
approximating the general activation functions with one-dimensional ReLU
networks, which reduces the problem to the complexity controls of ReLU
networks. Specifically, we consider two-layer networks and deep residual
networks, for which path-based norms are derived to control complexities. We
also provide preliminary analyses of the function spaces induced by these norms
and a priori estimates of the corresponding regularized estimators.
",3.0
1476,0,89318,1905.11013,learning to rank with partitioned preference,"Hao Wang, Tong Xu, Qi Liu, Defu Lian, Enhong Chen, Dongfang Du, Han
  Wu, Wen Su","MCNE: An End-to-End Framework for Learning Multiple Conditional Network
  Representations of Social Network","  Recently, the Network Representation Learning (NRL) techniques, which
represent graph structure via low-dimension vectors to support social-oriented
application, have attracted wide attention. Though large efforts have been
made, they may fail to describe the multiple aspects of similarity between
social users, as only a single vector for one unique aspect has been
represented for each node. To that end, in this paper, we propose a novel
end-to-end framework named MCNE to learn multiple conditional network
representations, so that various preferences for multiple behaviors could be
fully captured. Specifically, we first design a binary mask layer to divide the
single vector as conditional embeddings for multiple behaviors. Then, we
introduce the attention network to model interaction relationship among
multiple preferences, and further utilize the adapted message sending and
receiving operation of graph neural network, so that multi-aspect preference
information from high-order neighbors will be captured. Finally, we utilize
Bayesian Personalized Ranking loss function to learn the preference similarity
on each behavior, and jointly learn multiple conditional node embeddings via
multi-task learning framework. Extensive experiments on public datasets
validate that our MCNE framework could significantly outperform several
state-of-the-art baselines, and further support the visualization and transfer
learning tasks with excellent interpretability and robustness.
",0.0
1484,16,100943,1910.09293,activation function in neutral networks,"Ming-Xi Wang, Yang Qu",Approximation capabilities of neural networks on unbounded domains,"  In this paper, we prove that a shallow neural network with a monotone
sigmoid, ReLU, ELU, Softplus, or LeakyReLU activation function can arbitrarily
well approximate any L^p(p>=2) integrable functions defined on R*[0,1]^n. We
also prove that a shallow neural network with a sigmoid, ReLU, ELU, Softplus,
or LeakyReLU activation function expresses no nonzero integrable function
defined on the Euclidean plane. Together with a recent result that the deep
ReLU network can arbitrarily well approximate any integrable function on
Euclidean spaces, we provide a new perspective on the advantage of multiple
hidden layers in the context of ReLU networks. Lastly, we prove that the ReLU
network with depth 3 is a universal approximator in L^p(R^n).
",5.0
1503,19,216234,2211.02001,artificial intelligence for low carbon,"Alexandra Sasha Luccioni, Sylvain Viguier, Anne-Laure Ligozat","Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language
  Model","  Progress in machine learning (ML) comes with a cost to the environment, given
that training ML models requires significant computational resources, energy
and materials. In the present article, we aim to quantify the carbon footprint
of BLOOM, a 176-billion parameter language model, across its life cycle. We
estimate that BLOOM's final training emitted approximately 24.7 tonnes
of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes
if we account for all processes ranging from equipment manufacturing to
energy-based operational consumption. We also study the energy requirements and
carbon emissions of its deployment for inference via an API endpoint receiving
user queries in real-time. We conclude with a discussion regarding the
difficulty of precisely estimating the carbon footprint of ML models and future
research directions that can contribute towards improving carbon emissions
reporting.
",2.0
1526,2,34381,1506.0341,random forests,"Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L.
  Patsolic, Benjamin Falk, Jason Yim, Carey E. Priebe, Randal Burns, Mauro
  Maggioni, Joshua T. Vogelstein",Sparse Projection Oblique Randomer Forests,"  Decision forests, including Random Forests and Gradient Boosting Trees, have
recently demonstrated state-of-the-art performance in a variety of machine
learning settings. Decision forests are typically ensembles of axis-aligned
decision trees; that is, trees that split only along feature dimensions. In
contrast, many recent extensions to decision forests are based on axis-oblique
splits. Unfortunately, these extensions forfeit one or more of the favorable
properties of decision forests based on axis-aligned splits, such as robustness
to many noise dimensions, interpretability, or computational efficiency. We
introduce yet another decision forest, called ""Sparse Projection Oblique
Randomer Forests"" (SPORF). SPORF uses very sparse random projections, i.e.,
linear combinations of a small subset of features. SPORF significantly improves
accuracy over existing state-of-the-art algorithms on a standard benchmark
suite for classification with >100 problems of varying dimension, sample size,
and number of classes. To illustrate how SPORF addresses the limitations of
both axis-aligned and existing oblique decision forest methods, we conduct
extensive simulated experiments. SPORF typically yields improved performance
over existing decision forests, while mitigating computational efficiency and
scalability and maintaining interpretability. SPORF can easily be incorporated
into other ensemble methods such as boosting to obtain potentially similar
gains.
",5.0
1530,8,172948,2110.07654,node embedding for graph,"Sadamori Kojaku, Jisung Yoon, Isabel Constantino, Yong-Yeol Ahn",Residual2Vec: Debiasing graph embedding with random graphs,"  Graph embedding maps a graph into a convenient vector-space representation
for graph analysis and machine learning applications. Many graph embedding
methods hinge on a sampling of context nodes based on random walks. However,
random walks can be a biased sampler due to the structural properties of
graphs. Most notably, random walks are biased by the degree of each node, where
a node is sampled proportionally to its degree. The implication of such biases
has not been clear, particularly in the context of graph representation
learning. Here, we investigate the impact of the random walks' bias on graph
embedding and propose residual2vec, a general graph embedding method that can
debias various structural biases in graphs by using random graphs. We
demonstrate that this debiasing not only improves link prediction and
clustering performance but also allows us to explicitly model salient
structural properties in graph embedding.
",4.0
1537,13,47356,1611.02337,social network analysis with natrual language processing,"Daniel Robins, Fernando Emmanuel Frati, Jonatan Alvarez, Jose Texier","Balotage in Argentina 2015, a sentiment analysis of tweets","  Twitter social network contains a large amount of information generated by
its users. That information is composed of opinions and comments that may
reflect trends in social behavior. There is talk of trend when it is possible
to identify opinions and comments geared towards the same shared by a lot of
people direction. To determine if two or more written opinions share the same
address, techniques Natural Language Processing (NLP) are used. This paper
proposes a methodology for predicting reflected in Twitter from the use of
sentiment analysis functions NLP based on social behaviors. The case study was
selected the 2015 Presidential in Argentina, and a software architecture Big
Data composed Vertica data base with the component called Pulse was used.
Through the analysis it was possible to detect trends in voting intentions with
regard to the presidential candidates, achieving greater accuracy in predicting
that achieved with traditional systems surveys.
",5.0
1544,5,101652,1910.12774,matrix completion,"Wei Ma, George H. Chen","Missing Not at Random in Matrix Completion: The Effectiveness of
  Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption","  Matrix completion is often applied to data with entries missing not at random
(MNAR). For example, consider a recommendation system where users tend to only
reveal ratings for items they like. In this case, a matrix completion method
that relies on entries being revealed at uniformly sampled row and column
indices can yield overly optimistic predictions of unseen user ratings.
Recently, various papers have shown that we can reduce this bias in MNAR matrix
completion if we know the probabilities of different matrix entries being
missing. These probabilities are typically modeled using logistic regression or
naive Bayes, which make strong assumptions and lack guarantees on the accuracy
of the estimated probabilities. In this paper, we suggest a simple approach to
estimating these probabilities that avoids these shortcomings. Our approach
follows from the observation that missingness patterns in real data often
exhibit low nuclear norm structure. We can then estimate the missingness
probabilities by feeding the (always fully-observed) binary matrix specifying
which entries are revealed or missing to an existing nuclear-norm-constrained
matrix completion algorithm by Davenport et al. [2014]. Thus, we tackle MNAR
matrix completion by solving a different matrix completion problem first that
recovers missingness probabilities. We establish finite-sample error bounds for
how accurate these probability estimates are and how well these estimates
debias standard matrix completion losses for the original matrix to be
completed. Our experiments show that the proposed debiasing strategy can
improve a variety of existing matrix completion algorithms, and achieves
downstream matrix completion accuracy at least as good as logistic regression
and naive Bayes debiasing baselines that require additional auxiliary
information.
",4.0
1560,3,218647,cs/0612137,database management system,"Eric Robinson, David DeWitt",Turning Cluster Management into Data Management: A System Overview,"  This paper introduces the CondorJ2 cluster management system. Traditionally,
cluster management systems such as Condor employ a process-oriented approach
with little or no use of modern database system technology. In contrast,
CondorJ2 employs a data-centric, 3-tier web-application architecture for all
system functions (e.g., job submission, monitoring and scheduling; node
configuration, monitoring and management, etc.) except for job execution.
Employing a data-oriented approach allows the core challenge (i.e., managing
and coordinating a large set of distributed computing resources) to be
transformed from a relatively low-level systems problem into a more abstract,
higher-level data management problem. Preliminary results suggest that
CondorJ2's use of standard 3-tier software represents a significant step
forward to the design and implementation of large clusters (1,000 to 10,000
nodes).
",2.0
1576,3,27012,1406.1431,database management system,Ali Mansouri and Youssef Amghar,Int\'egration des r\`egles actives dans des documents,"  The management of technical documentation is an unavoidable activity
interesting for the enterprises. Indeed, the need to manage documents during
all the life cycle is an important issue. For that, the need to enhance the
ability of document management systems is an interesting challenge. Despite
existing systems on market (electronic document management systems), they are
considered as non-flexible systems which are based on data models preventing
any extension or improvement. In addition, those systems do not allow a slight
description of documents elements and propose an insufficient mechanisms for
both links and consistency management. LIRIS laboratory has developed research
in this area and proposed an active system, termed SAGED, whose objectives is
to manage link and consistency using active rules. However SAGED is based on an
approach that split rules (for consistency management) and documents
description. The main drawback is the rigidity of such approach which is
highlighted whenever documents are moved from one server to another or during
exchanges of documents. To contribute to solve this problem, we propose to
develop an approach aiming at improve the document management including
consistency. This approach is based on the introduction of rules with the XML
description of the documents [BoCP01]. In this context we proposed a
XML-oriented storage level allowing the storing of documents and rules
uniformly through a native XML database. We defined an intelligent system
termed SIGED according a client/server architecture built around an intelligent
component for active rules execution. These rules are extracted from XML
document, compiled and executed.
",5.0
1582,2,86759,1904.10142,random forests,"Hemant Rathore, Sanjay K. Sahay, Palash Chaturvedi and Mohit Sewak",Android Malicious Application Classification Using Clustering,"  Android malware have been growing at an exponential pace and becomes a
serious threat to mobile users. It appears that most of the anti-malware still
relies on the signature-based detection system which is generally slow and
often not able to detect advanced obfuscated malware. Hence time-to-time
various authors have proposed different machine learning solutions to identify
sophisticated malware. However, it appears that detection accuracy can be
improved by using the clustering method. Therefore in this paper, we propose a
novel scalable and effective clustering method to improve the detection
accuracy of the malicious android application and obtained a better overall
accuracy (98.34%) by random forest classifier compared to regular method, i.e.,
taking the data altogether to detect the malware. However, as far as true
positive and true negative are concerned, by clustering method, true positive
is best obtained by decision tree (97.59%) and true negative by support vector
machine (99.96%) which is the almost same result obtained by the random forest
true positive (97.30%) and true negative (99.38%) respectively. The reason that
overall accuracy of random forest is high because the true positive of support
vector machine and true negative of the decision tree is significantly less
than the random forest.
",4.0
1583,4,80377,1901.02565,pre-trained language model,"Maxwell Crouse, Achille Fokoue, Maria Chang, Pavan Kapanipathi, Ryan
  Musa, Constantine Nakos, Lingfei Wu, Kenneth Forbus, Michael Witbrock",High-Fidelity Vector Space Models of Structured Data,"  Machine learning systems regularly deal with structured data in real-world
applications. Unfortunately, such data has been difficult to faithfully
represent in a way that most machine learning techniques would expect, i.e. as
a real-valued vector of a fixed, pre-specified size. In this work, we introduce
a novel approach that compiles structured data into a satisfiability problem
which has in its set of solutions at least (and often only) the input data. The
satisfiability problem is constructed from constraints which are generated
automatically a priori from a given signature, thus trivially allowing for a
bag-of-words-esque vector representation of the input to be constructed. The
method is demonstrated in two areas, automated reasoning and natural language
processing, where it is shown to produce vector representations of
natural-language sentences and first-order logic clauses that can be precisely
translated back to their original, structured input forms.
",3.0
1599,10,89714,1905.12607,web archive,"Sawood Alam, Michele C. Weigle, Michael L. Nelson, Fernando Melo,
  Daniel Bicho, Daniel Gomes",MementoMap Framework for Flexible and Adaptive Web Archive Profiling,"  In this work we propose MementoMap, a flexible and adaptive framework to
efficiently summarize holdings of a web archive. We described a simple, yet
extensible, file format suitable for MementoMap. We used the complete index of
the Arquivo.pt comprising 5B mementos (archived web pages/files) to understand
the nature and shape of its holdings. We generated MementoMaps with varying
amount of detail from its HTML pages that have an HTTP status code of 200 OK.
Additionally, we designed a single-pass, memory-efficient, and
parallelization-friendly algorithm to compact a large MementoMap into a small
one and an in-file binary search method for efficient lookup. We analyzed more
than three years of MemGator (a Memento aggregator) logs to understand the
response behavior of 14 public web archives. We evaluated MementoMaps by
measuring their Accuracy using 3.3M unique URIs from MemGator logs. We found
that a MementoMap of less than 1.5% Relative Cost (as compared to the
comprehensive listing of all the unique original URIs) can correctly identify
the presence or absence of 60% of the lookup URIs in the corresponding archive
while maintaining 100% Recall (i.e., zero false negatives).
",5.0
1607,5,13958,1206.647,matrix completion,"Franz Kiraly (TU Berlin), Ryota Tomioka (University of Tokyo)","A Combinatorial Algebraic Approach for the Identifiability of Low-Rank
  Matrix Completion","  In this paper, we review the problem of matrix completion and expose its
intimate relations with algebraic geometry, combinatorics and graph theory. We
present the first necessary and sufficient combinatorial conditions for
matrices of arbitrary rank to be identifiable from a set of matrix entries,
yielding theoretical constraints and new algorithms for the problem of matrix
completion. We conclude by algorithmically evaluating the tightness of the
given conditions and algorithms for practically relevant matrix sizes, showing
that the algebraic-combinatoric approach can lead to improvements over
state-of-the-art matrix completion methods.
",5.0
1618,3,99123,1909.13839,database management system,Sami Alabed,RLCache: Automated Cache Management Using Reinforcement Learning,"  This study investigates the use of reinforcement learning to guide a general
purpose cache manager decisions. Cache managers directly impact the overall
performance of computer systems. They govern decisions about which objects
should be cached, the duration they should be cached for, and decides on which
objects to evict from the cache if it is full. These three decisions impact
both the cache hit rate and size of the storage that is needed to achieve that
cache hit rate. An optimal cache manager will avoid unnecessary operations,
maximise the cache hit rate which results in fewer round trips to a slower
backend storage system, and minimise the size of storage needed to achieve a
high hit-rate.
  This project investigates using reinforcement learning in cache management by
designing three separate agents for each of the cache manager tasks.
Furthermore, the project investigates two advanced reinforcement learning
architectures for multi-decision problems: a single multi-task agent and a
multi-agent. We also introduce a framework to simplify the modelling of
computer systems problems as a reinforcement learning task. The framework
abstracts delayed experiences observations and reward assignment in computer
systems while providing a flexible way to scale to multiple agents.
  Simulation results based on an established database benchmark system show
that reinforcement learning agents can achieve a higher cache hit rate over
heuristic driven algorithms while minimising the needed space. They are also
able to adapt to a changing workload and dynamically adjust their caching
strategy accordingly. The proposed cache manager model is generic and
applicable to other types of caches, such as file system caches. This project
is the first, to our knowledge, to model cache manager decisions as a
multi-task control problem.
",2.0
1645,2,27766,1407.3939,random forests,"Sylvain Arlot (DI-ENS, INRIA Paris - Rocquencourt), Robin Genuer
  (ISPED, INRIA Bordeaux - Sud-Ouest)",Analysis of purely random forests bias,"  Random forests are a very effective and commonly used statistical method, but
their full theoretical analysis is still an open problem. As a first step,
simplified models such as purely random forests have been introduced, in order
to shed light on the good performance of random forests. In this paper, we
study the approximation error (the bias) of some purely random forest models in
a regression framework, focusing in particular on the influence of the number
of trees in the forest. Under some regularity assumptions on the regression
function, we show that the bias of an infinite forest decreases at a faster
rate (with respect to the size of each tree) than a single tree. As a
consequence, infinite forests attain a strictly better risk rate (with respect
to the sample size) than single trees. Furthermore, our results allow to derive
a minimum number of trees sufficient to reach the same rate as an infinite
forest. As a by-product of our analysis, we also show a link between the bias
of purely random forests and the bias of some kernel estimators.
",5.0
1647,14,169232,2109.06896,text summarization model,Chao-Chun Hsu and Chenhao Tan,Decision-Focused Summarization,"  Relevance in summarization is typically defined based on textual information
alone, without incorporating insights about a particular decision. As a result,
to support risk analysis of pancreatic cancer, summaries of medical notes may
include irrelevant information such as a knee injury. We propose a novel
problem, decision-focused summarization, where the goal is to summarize
relevant information for a decision. We leverage a predictive model that makes
the decision based on the full text to provide valuable insights on how a
decision can be inferred from text. To build a summary, we then select
representative sentences that lead to similar model decisions as using the full
text while accounting for textual non-redundancy. To evaluate our method
(DecSum), we build a testbed where the task is to summarize the first ten
reviews of a restaurant in support of predicting its future rating on Yelp.
DecSum substantially outperforms text-only summarization methods and
model-based explanation methods in decision faithfulness and
representativeness. We further demonstrate that DecSum is the only method that
enables humans to outperform random chance in predicting which restaurant will
be better rated in the future.
",4.0
1656,19,189908,2203.10749,artificial intelligence for low carbon,"Wei Zhao, Shiqi Zhang, Bing Zhou, Bei Wang","STCGAT: A Spatio-temporal Causal Graph Attention Network for traffic
  flow prediction in Intelligent Transportation Systems","  Air pollution and carbon emissions caused by modern transportation are
closely related to global climate change. With the help of next-generation
information technology such as Internet of Things (IoT) and Artificial
Intelligence (AI), accurate traffic flow prediction can effectively solve
problems such as traffic congestion and mitigate environmental pollution and
climate change. It further promotes the development of Intelligent
Transportation Systems (ITS) and smart cities. However, the strong spatial and
temporal correlation of traffic data makes the task of accurate traffic
forecasting a significant challenge. Existing methods are usually based on
graph neural networks using predefined spatial adjacency graphs of traffic
networks to model spatial dependencies, ignoring the dynamic correlation of
relationships between road nodes. In addition, they usually use independent
Spatio-temporal components to capture Spatio-temporal dependencies and do not
effectively model global Spatio-temporal dependencies. This paper proposes a
new Spatio-temporal Causal Graph Attention Network (STCGAT) for traffic
prediction to address the above challenges. In STCGAT, we use a node embedding
approach that can adaptively generate spatial adjacency subgraphs at each time
step without a priori geographic knowledge and fine-grained modeling of the
topology of dynamically generated graphs for different time steps. Meanwhile,
we propose an efficient causal temporal correlation component that contains
node adaptive learning, graph convolution, and local and global causal temporal
convolution modules to learn local and global Spatio-temporal dependencies
jointly. Extensive experiments on four real, large traffic datasets show that
our model consistently outperforms all baseline models.
",3.0
1657,16,180739,2112.12078,activation function in neutral networks,Advait Vagerwal,Deeper Learning with CoLU Activation,"  In neural networks, non-linearity is introduced by activation functions. One
commonly used activation function is Rectified Linear Unit (ReLU). ReLU has
been a popular choice as an activation but has flaws. State-of-the-art
functions like Swish and Mish are now gaining attention as a better choice as
they combat many flaws presented by other activation functions. CoLU is an
activation function similar to Swish and Mish in properties. It is defined as
f(x)=x/(1-xe^-(x+e^x)). It is smooth, continuously differentiable, unbounded
above, bounded below, non-saturating, and non-monotonic. Based on experiments
done with CoLU with different activation functions, it is observed that CoLU
usually performs better than other functions on deeper neural networks. While
training different neural networks on MNIST on an incrementally increasing
number of convolutional layers, CoLU retained the highest accuracy for more
layers. On a smaller network with 8 convolutional layers, CoLU had the highest
mean accuracy, closely followed by ReLU. On VGG-13 trained on Fashion-MNIST,
CoLU had a 4.20% higher accuracy than Mish and 3.31% higher accuracy than ReLU.
On ResNet-9 trained on Cifar-10, CoLU had 0.05% higher accuracy than Swish,
0.09% higher accuracy than Mish, and 0.29% higher accuracy than ReLU. It is
observed that activation functions may behave better than other activation
functions based on different factors including the number of layers, types of
layers, number of parameters, learning rate, optimizer, etc. Further research
can be done on these factors and activation functions for more optimal
activation functions and more knowledge on their behavior.
",5.0
1665,11,13384,1206.1494,PageRank for web search,"Georg Singer, Ulrich Norbisrath, Dirk Lewandowski",Impact of Gender and Age on performing Search Tasks Online,"  More and more people use the Internet to work on duties of their daily work
routine. To find the right information online, Web search engines are the tools
of their choice. Apart from finding facts, people use Web search engines to
also execute rather complex and time consuming search tasks. So far search
engines follow the one-for-all approach to serve its users and little is known
about the impact of gender and age on people's Web search behavior. In this
article we present a study that examines (1) how female and male web users
carry out simple and complex search tasks and what are the differences between
the two user groups, and (2) how the age of the users impacts their search
performance. The laboratory study was done with 56 ordinary people each
carrying out 12 search tasks. Our findings confirm that age impacts behavior
and search performance significantly, while gender influences were smaller than
expected.
",0.0
1672,3,3251,906.3112,database management system,"Panagiotis Papadakos, Yannis Theoharis, Yannis Marketakis, Nikos
  Armenatzoglou and Yannis Tzitzikas",Object-Relational Database Representations for Text Indexing,"  One of the distinctive features of Information Retrieval systems comparing to
Database Management systems, is that they offer better compression for posting
lists, resulting in better I/O performance and thus faster query evaluation. In
this paper, we introduce database representations of the index that reduce the
size (and thus the disk I/Os) of the posting lists. This is not achieved by
redesigning the DBMS, but by exploiting the non 1NF features that existing
Object-Relational DBM systems (ORDBMS) already offer. Specifically, four
different database representations are described and detailed experimental
results for one million pages are reported. Three of these representations are
one order of magnitude more space efficient and faster (in query evaluation)
than the plain relational representation.
",5.0
1680,19,131316,2009.08002,artificial intelligence for low carbon,Pushpendra Rana and Lav R Varshney,"Planting trees at the right places: Recommending suitable sites for
  growing trees using algorithm fusion","  Large-scale planting of trees has been proposed as a low-cost natural
solution for carbon mitigation, but is hampered by poor selection of plantation
sites, especially in developing countries. To aid in site selection, we develop
the ePSA (e-Plantation Site Assistant) recommendation system based on algorithm
fusion that combines physics-based/traditional forestry science knowledge with
machine learning. ePSA assists forest range officers by identifying blank
patches inside forest areas and ranking each such patch based on their tree
growth potential. Experiments, user studies, and deployment results
characterize the utility of the recommender system in shaping the long-term
success of tree plantations as a nature climate solution for carbon mitigation
in northern India and beyond.
",5.0
1687,1,18811,1303.3229,advanced search engine,"Radu Dragusin (1 and 2), Paula Petcu (1 and 3), Christina Lioma (1 and
  2), Birger Larsen (4), Henrik L. J{\o}rgensen (5), Ingemar J. Cox (1 and 6),
  Lars Kai Hansen (1), Peter Ingwersen (4), Ole Winther (1) ((1) DTU Compute,
  Technical University of Denmark, Denmark, (2) Department of Computer Science,
  University of Copenhagen, Denmark, (3) Findwise, Copenhagen, Denmark, (4)
  Information Systems and Interaction Design, Royal School of Library and
  Information Science, Copenhagen, Denmark, (5) Department of Clinical
  Biochemistry, Bispebjerg Hospital, Copenhagen, Denmark, (6) Department of
  Computer Science, University College London, London, United Kingdom)",FindZebra: A search engine for rare diseases,"  Background: The web has become a primary information resource about illnesses
and treatments for both medical and non-medical users. Standard web search is
by far the most common interface for such information. It is therefore of
interest to find out how well web search engines work for diagnostic queries
and what factors contribute to successes and failures. Among diseases, rare (or
orphan) diseases represent an especially challenging and thus interesting class
to diagnose as each is rare, diverse in symptoms and usually has scattered
resources associated with it. Methods: We use an evaluation approach for web
search engines for rare disease diagnosis which includes 56 real life
diagnostic cases, state-of-the-art evaluation measures, and curated information
resources. In addition, we introduce FindZebra, a specialized (vertical) rare
disease search engine. FindZebra is powered by open source search technology
and uses curated freely available online medical information. Results:
FindZebra outperforms Google Search in both default setup and customised to the
resources used by FindZebra. We extend FindZebra with specialized
functionalities exploiting medical ontological information and UMLS medical
concepts to demonstrate different ways of displaying the retrieved results to
medical experts. Conclusions: Our results indicate that a specialized search
engine can improve the diagnostic quality without compromising the ease of use
of the currently widely popular web search engines. The proposed evaluation
approach can be valuable for future development and benchmarking. The FindZebra
search engine is available at http://www.findzebra.com/.
",5.0
1693,0,55001,1706.09007,learning to rank with partitioned preference,"Michele Flammini, Gianpiero Monaco, Qiang Zhang","Strategyproof Mechanisms for Additively Separable Hedonic Games and
  Fractional Hedonic Games","  Additively separable hedonic games and fractional hedonic games have received
considerable attention. They are coalition forming games of selfish agents
based on their mutual preferences. Most of the work in the literature
characterizes the existence and structure of stable outcomes (i.e., partitions
in coalitions), assuming that preferences are given. However, there is little
discussion on this assumption. In fact, agents receive different utilities if
they belong to different partitions, and thus it is natural for them to declare
their preferences strategically in order to maximize their benefit. In this
paper we consider strategyproof mechanisms for additively separable hedonic
games and fractional hedonic games, that is, partitioning methods without
payments such that utility maximizing agents have no incentive to lie about
their true preferences. We focus on social welfare maximization and provide
several lower and upper bounds on the performance achievable by strategyproof
mechanisms for general and specific additive functions. In most of the cases we
provide tight or asymptotically tight results. All our mechanisms are simple
and can be computed in polynomial time. Moreover, all the lower bounds are
unconditional, that is, they do not rely on any computational or complexity
assumptions.
",0.0
1696,1,106257,1912.09519,advanced search engine,"Nikitha Rao, Chetan Bansal, Thomas Zimmermann, Ahmed Hassan Awadallah,
  Nachiappan Nagappan",Analyzing Web Search Behavior for Software Engineering Tasks,"  Web search plays an integral role in software engineering (SE) to help with
various tasks such as finding documentation, debugging, installation, etc. In
this work, we present the first large-scale analysis of web search behavior for
SE tasks using the search query logs from Bing, a commercial web search engine.
First, we use distant supervision techniques to build a machine learning
classifier to extract the SE search queries with an F1 score of 93%. We then
perform an analysis on one million search sessions to understand how software
engineering related queries and sessions differ from other queries and
sessions. Subsequently, we propose a taxonomy of intents to identify the
various contexts in which web search is used in software engineering. Lastly,
we analyze millions of SE queries to understand the distribution, search
metrics and trends across these SE search intents. Our analysis shows that SE
related queries form a significant portion of the overall web search traffic.
Additionally, we found that there are six major intent categories for which web
search is used in software engineering. The techniques and insights can not
only help improve existing tools but can also inspire the development of new
tools that aid in finding information for SE related tasks.
",3.0
1705,11,20475,1305.7395,PageRank for web search,"L. Chakhmakhchyan, D. Shepelyansky",PageRank model of opinion formation on Ulam networks,"  We consider a PageRank model of opinion formation on Ulam networks, generated
by the intermittency map and the typical Chirikov map. The Ulam networks
generated by these maps have certain similarities with such scale-free networks
as the World Wide Web (WWW), showing an algebraic decay of the PageRank
probability. We find that the opinion formation process on Ulam networks have
certain similarities but also distinct features comparing to the WWW. We
attribute these distinctions to internal differences in network structure of
the Ulam and WWW networks. We also analyze the process of opinion formation in
the frame of generalized Sznajd model which protects opinion of small
communities.
",5.0
1707,4,175736,2111.0347,pre-trained language model,"Romina Oji, Seyedeh Fatemeh Razavi, Sajjad Abdi Dehsorkh, Alireza
  Hariri, Hadi Asheri, Reshad Hosseini",ParsiNorm: A Persian Toolkit for Speech Processing Normalization,"  In general, speech processing models consist of a language model along with
an acoustic model. Regardless of the language model's complexity and variants,
three critical pre-processing steps are needed in language models: cleaning,
normalization, and tokenization. Among mentioned steps, the normalization step
is so essential to format unification in pure textual applications. However,
for embedded language models in speech processing modules, normalization is not
limited to format unification. Moreover, it has to convert each readable
symbol, number, etc., to how they are pronounced. To the best of our knowledge,
there is no Persian normalization toolkits for embedded language models in
speech processing modules, So in this paper, we propose an open-source
normalization toolkit for text processing in speech applications. Briefly, we
consider different readable Persian text like symbols (common currencies, #, @,
URL, etc.), numbers (date, time, phone number, national code, etc.), and so on.
Comparison with other available Persian textual normalization tools indicates
the superiority of the proposed method in speech processing. Also, comparing
the model's performance for one of the proposed functions (sentence separation)
with other common natural language libraries such as HAZM and Parsivar
indicates the proper performance of the proposed method. Besides, its
evaluation of some Persian Wikipedia data confirms the proper performance of
the proposed method.
",5.0
1711,1,64892,1803.05127,advanced search engine,"Xinzhi Han, Sen Lei","Feature Selection and Model Comparison on Microsoft Learning-to-Rank
  Data Sets","  With the rapid advance of the Internet, search engines (e.g., Google, Bing,
Yahoo!) are used by billions of users for each day. The main function of a
search engine is to locate the most relevant webpages corresponding to what the
user requests. This report focuses on the core problem of information
retrieval: how to learn the relevance between a document (very often webpage)
and a query given by user. Our analysis consists of two parts: 1) we use
standard statistical methods to select important features among 137 candidates
given by information retrieval researchers from Microsoft. We find that not all
the features are useful, and give interpretations on the top-selected features;
2) we give baselines on prediction over the real-world dataset MSLR-WEB by
using various learning algorithms. We find that models of boosting trees,
random forest in general achieve the best performance of prediction. This
agrees with the mainstream opinion in information retrieval community that
tree-based algorithms outperform the other candidates for this problem.
",4.0
1720,14,64466,1803.01465,text summarization model,"Shuming Ma, Xu Sun, Wei Li, Sujian Li, Wenjie Li, Xuancheng Ren","Query and Output: Generating Words by Querying Distributed Word
  Representations for Paraphrase Generation","  Most recent approaches use the sequence-to-sequence model for paraphrase
generation. The existing sequence-to-sequence model tends to memorize the words
and the patterns in the training dataset instead of learning the meaning of the
words. Therefore, the generated sentences are often grammatically correct but
semantically improper. In this work, we introduce a novel model based on the
encoder-decoder framework, called Word Embedding Attention Network (WEAN). Our
proposed model generates the words by querying distributed word representations
(i.e. neural word embeddings), hoping to capturing the meaning of the according
words. Following previous work, we evaluate our model on two
paraphrase-oriented tasks, namely text simplification and short text
abstractive summarization. Experimental results show that our model outperforms
the sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two
English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a
Chinese summarization dataset. Moreover, our model achieves state-of-the-art
performances on these three benchmark datasets.
",2.0
1722,5,167320,2108.11124,matrix completion,"Wei Shen, Chuheng Zhang, Yun Tian, Liang Zeng, Xiaonan He, Wanchun
  Dou, Xiaolong Xu",Inductive Matrix Completion Using Graph Autoencoder,"  Recently, the graph neural network (GNN) has shown great power in matrix
completion by formulating a rating matrix as a bipartite graph and then
predicting the link between the corresponding user and item nodes. The majority
of GNN-based matrix completion methods are based on Graph Autoencoder (GAE),
which considers the one-hot index as input, maps a user (or item) index to a
learnable embedding, applies a GNN to learn the node-specific representations
based on these learnable embeddings and finally aggregates the representations
of the target users and its corresponding item nodes to predict missing links.
However, without node content (i.e., side information) for training, the user
(or item) specific representation can not be learned in the inductive setting,
that is, a model trained on one group of users (or items) cannot adapt to new
users (or items). To this end, we propose an inductive matrix completion method
using GAE (IMC-GAE), which utilizes the GAE to learn both the user-specific (or
item-specific) representation for personalized recommendation and local graph
patterns for inductive matrix completion. Specifically, we design two
informative node features and employ a layer-wise node dropout scheme in GAE to
learn local graph patterns which can be generalized to unseen data. The main
contribution of our paper is the capability to efficiently learn local graph
patterns in GAE, with good scalability and superior expressiveness compared to
previous GNN-based matrix completion methods. Furthermore, extensive
experiments demonstrate that our model achieves state-of-the-art performance on
several matrix completion benchmarks. Our official code is publicly available.
",5.0
1742,14,172615,2110.06263,text summarization model,"Roshan Sharma, Shruti Palaskar, Alan W Black and Florian Metze",Speech Summarization using Restricted Self-Attention,"  Speech summarization is typically performed by using a cascade of speech
recognition and text summarization models. End-to-end modeling of speech
summarization models is challenging due to memory and compute constraints
arising from long input audio sequences. Recent work in document summarization
has inspired methods to reduce the complexity of self-attentions, which enables
transformer models to handle long sequences. In this work, we introduce a
single model optimized end-to-end for speech summarization. We apply the
restricted self-attention technique from text-based models to speech models to
address the memory and compute constraints. We demonstrate that the proposed
model learns to directly summarize speech for the How-2 corpus of instructional
videos. The proposed end-to-end model outperforms the previously proposed
cascaded model by 3 points absolute on ROUGE. Further, we consider the spoken
language understanding task of predicting concepts from speech inputs and show
that the proposed end-to-end model outperforms the cascade model by 4 points
absolute F-1.
",5.0
1751,19,28616,1409.117,artificial intelligence for low carbon,Kamran Latif,"Hybrid Systems Knowledge Representation Using Modelling Environment
  System Techniques Artificial Intelligence","  Knowledge-based or Artificial Intelligence techniques are used increasingly
as alternatives to more classical techniques to model ENVIRONMENTAL SYSTEMS.
Use of Artificial Intelligence (AI) in environmental modelling has increased
with recognition of its potential. In this paper we examine the DIFFERENT
TECHNIQUES of Artificial intelligence with profound examples of human
perception, learning and reasoning to solve complex problems. However with the
increase of complexity better methods are required. Keeping in view of the
above some researchers introduced the idea of hybrid mechanism in which two or
more methods can be combined which seems to be a positive effort for creating a
more complex; advanced and intelligent system which has the capability to in-
cooperate human decisions thus driving the landscape changes.
",4.0
1761,13,2527,901.4784,social network analysis with natural language processing,Fabio G. Guerrero,On the Entropy of Written Spanish,"  This paper reports on results on the entropy of the Spanish language. They
are based on an analysis of natural language for n-word symbols (n = 1 to 18),
trigrams, digrams, and characters. The results obtained in this work are based
on the analysis of twelve different literary works in Spanish, as well as a
279917 word news file provided by the Spanish press agency EFE. Entropy values
are calculated by a direct method using computer processing and the probability
law of large numbers. Three samples of artificial Spanish language produced by
a first-order model software source are also analyzed and compared with natural
Spanish language.
",1.0
1762,13,6724,1010.6242,social network analysis with natural language processing,"Martine Hurault-Plantet (LIMSI), Elie Naulleau, Bernard Jacquemin
  (CREM-EA3476)",GraphDuplex: visualisation simultan\'ee de N r\'eseaux coupl\'es 2 par 2,"  While social network analysis often focuses on graph structure of social
actors, an increasing number of communication networks now provide textual
content within social activity (email, instant messaging, blogging,
collaboration networks). We present an open source visualization software,
GraphDuplex, which brings together social structure and textual content, adding
a semantic dimension to social analysis. GraphDuplex eventually connects any
number of social or semantic graphs together, and through dynamic queries
enables user interaction and exploration across multiple graphs of different
nature.
",1.0
1763,13,11917,1202.5299,social network analysis with natural language processing,"Jianbo Gao, Jing Hu, Xiang Mao, Matjaz Perc","Culturomics meets random fractal theory: Insights into long-range
  correlations of social and natural phenomena over the past two centuries","  Culturomics was recently introduced as the application of high-throughput
data collection and analysis to the study of human culture. Here we make use of
this data by investigating fluctuations in yearly usage frequencies of specific
words that describe social and natural phenomena, as derived from books that
were published over the course of the past two centuries. We show that the
determination of the Hurst parameter by means of fractal analysis provides
fundamental insights into the nature of long-range correlations contained in
the culturomic trajectories, and by doing so, offers new interpretations as to
what might be the main driving forces behind the examined phenomena. Quite
remarkably, we find that social and natural phenomena are governed by
fundamentally different processes. While natural phenomena have properties that
are typical for processes with persistent long-range correlations, social
phenomena are better described as nonstationary, on-off intermittent, or Levy
walk processes.
",0.0
1764,13,13726,1206.4958,social network analysis with natural language processing,"Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall",A Pointillism Approach for Natural Language Processing of Social Media,"  The Chinese language poses challenges for natural language processing based
on the unit of a word even for formal uses of the Chinese language, social
media only makes word segmentation in Chinese even more difficult. In this
document we propose a pointillism approach to natural language processing.
Rather than words that have individual meanings, the basic unit of a
pointillism approach is trigrams of characters. These grams take on meaning in
aggregate when they appear together in a way that is correlated over time.
  Our results from three kinds of experiments show that when words and topics
do have a meme-like trend, they can be reconstructed from only trigrams. For
example, for 4-character idioms that appear at least 99 times in one day in our
data, the unconstrained precision (that is, precision that allows for deviation
from a lexicon when the result is just as correct as the lexicon version of the
word or phrase) is 0.93. For longer words and phrases collected from
Wiktionary, including neologisms, the unconstrained precision is 0.87. We
consider these results to be very promising, because they suggest that it is
feasible for a machine to reconstruct complex idioms, phrases, and neologisms
with good precision without any notion of words. Thus the colorful and baroque
uses of language that typify social media in challenging languages such as
Chinese may in fact be accessible to machines.
",2.0
1765,13,17064,1301.195,social network analysis with natural language processing,Bogdan Patrut,"Syntactic Analysis Based on Morphological Characteristic Features of the
  Romanian Language","  This paper refers to the syntactic analysis of phrases in Romanian, as an
important process of natural language processing. We will suggest a real-time
solution, based on the idea of using some words or groups of words that
indicate grammatical category; and some specific endings of some parts of
sentence. Our idea is based on some characteristics of the Romanian language,
where some prepositions, adverbs or some specific endings can provide a lot of
information about the structure of a complex sentence. Such characteristics can
be found in other languages, too, such as French. Using a special grammar, we
developed a system (DIASEXP) that can perform a dialogue in natural language
with assertive and interogative sentences about a ""story"" (a set of sentences
describing some events from the real life).
",1.0
1766,13,20946,1306.6944,social network analysis with natural language processing,"Ulf Sch\""oneberg and Wolfram Sperber",The DeLiVerMATH project - Text analysis in mathematics,"  A high-quality content analysis is essential for retrieval functionalities
but the manual extraction of key phrases and classification is expensive.
Natural language processing provides a framework to automatize the process.
Here, a machine-based approach for the content analysis of mathematical texts
is described. A prototype for key phrase extraction and classification of
mathematical texts is presented.
",0.0
1767,13,22467,1310.1249,social network analysis with natural language processing,"Andrzej Jarynowski, Amir Rostami",Reading Stockholm Riots 2013 in social media by text-mining,"  The riots in Stockholm in May 2013 were an event that reverberated in the
world media for its dimension of violence that had spread through the Swedish
capital. In this study we have investigated the role of social media in
creating media phenomena via text mining and natural language processing. We
have focused on two channels of communication for our analysis: Twitter and
Poloniainfo.se (Forum of Polish community in Sweden). Our preliminary results
show some hot topics driving discussion related mostly to Swedish Police and
Swedish Politics by counting word usage. Typical features for media
intervention are presented. We have built networks of most popular phrases,
clustered by categories (geography, media institution, etc.). Sentiment
analysis shows negative connotation with Police. The aim of this preliminary
exploratory quantitative study was to generate questions and hypotheses, which
we could carefully follow by deeper more qualitative methods.
",4.0
1768,13,23866,1312.6635,social network analysis with natural language processing,"Shana Dacres, Hamed Haddadi, Matthew Purver","Topic and Sentiment Analysis on OSNs: a Case Study of Advertising
  Strategies on Twitter","  Social media have substantially altered the way brands and businesses
advertise: Online Social Networks provide brands with more versatile and
dynamic channels for advertisement than traditional media (e.g., TV and radio).
Levels of engagement in such media are usually measured in terms of content
adoption (e.g., likes and retweets) and sentiment, around a given topic.
However, sentiment analysis and topic identification are both non-trivial
tasks.
  In this paper, using data collected from Twitter as a case study, we analyze
how engagement and sentiment in promoted content spread over a 10-day period.
We find that promoted tweets lead to higher positive sentiment than promoted
trends; although promoted trends pay off in response volume. We observe that
levels of engagement for the brand and promoted content are highest on the
first day of the campaign, and fall considerably thereafter. However, we show
that these insights depend on the use of robust machine learning and natural
language processing techniques to gather focused, relevant datasets, and to
accurately gauge sentiment, rather than relying on the simple keyword- or
frequency-based metrics sometimes used in social media research.
",4.0
1769,13,28884,1409.5718,social network analysis with natural language processing,"Lili Mou, Ge Li, Lu Zhang, Tao Wang, Zhi Jin","Convolutional Neural Networks over Tree Structures for Programming
  Language Processing","  Programming language processing (similar to natural language processing) is a
hot research topic in the field of software engineering; it has also aroused
growing interest in the artificial intelligence community. However, different
from a natural language sentence, a program contains rich, explicit, and
complicated structural information. Hence, traditional NLP models may be
inappropriate for programs. In this paper, we propose a novel tree-based
convolutional neural network (TBCNN) for programming language processing, in
which a convolution kernel is designed over programs' abstract syntax trees to
capture structural information. TBCNN is a generic architecture for programming
language processing; our experiments show its effectiveness in two different
program analysis tasks: classifying programs according to functionality, and
detecting code snippets of certain patterns. TBCNN outperforms baseline
methods, including several neural models for NLP.
",1.0
1770,13,33522,1505.00989,social network analysis with natural language processing,"Kais Dai, Celia G\'onzalez Nespereira, Ana Fern\'andez Vilas, Rebeca
  P. D\'iaz Redondo","Scraping and Clustering Techniques for the Characterization of Linkedin
  Profiles","  The socialization of the web has undertaken a new dimension after the
emergence of the Online Social Networks (OSN) concept. The fact that each
Internet user becomes a potential content creator entails managing a big amount
of data. This paper explores the most popular professional OSN: LinkedIn. A
scraping technique was implemented to get around 5 Million public profiles. The
application of natural language processing techniques (NLP) to classify the
educational background and to cluster the professional background of the
collected profiles led us to provide some insights about this OSN's users and
to evaluate the relationships between educational degrees and professional
careers.
",4.0
1771,13,34540,1506.05402,social network analysis with natural language processing,"Iana Atanassova, Marc Bertin, Philipp Mayr","Editorial for the First Workshop on Mining Scientific Papers:
  Computational Linguistics and Bibliometrics","  The workshop ""Mining Scientific Papers: Computational Linguistics and
Bibliometrics"" (CLBib 2015), co-located with the 15th International Society of
Scientometrics and Informetrics Conference (ISSI 2015), brought together
researchers in Bibliometrics and Computational Linguistics in order to study
the ways Bibliometrics can benefit from large-scale text analytics and sense
mining of scientific papers, thus exploring the interdisciplinarity of
Bibliometrics and Natural Language Processing (NLP). The goals of the workshop
were to answer questions like: How can we enhance author network analysis and
Bibliometrics using data obtained by text analytics? What insights can NLP
provide on the structure of scientific writing, on citation networks, and on
in-text citation analysis? This workshop is the first step to foster the
reflection on the interdisciplinarity and the benefits that the two disciplines
Bibliometrics and Natural Language Processing can drive from it.
",1.0
1772,13,42399,1605.04122,social network analysis with natural language processing,"Richard Moot (LaBRI), Christian Retor\'e (TEXTE)",Natural Language Semantics and Computability,"  This paper is a reflexion on the computability of natural language semantics.
It does not contain a new model or new results in the formal semantics of
natural language: it is rather a computational analysis of the logical models
and algorithms currently used in natural language semantics, defined as the
mapping of a statement to logical formulas - formulas, because a statement can
be ambiguous. We argue that as long as possible world semantics is left out,
one can compute the semantic representation(s) of a given statement, including
aspects of lexical meaning. We also discuss the algorithmic complexity of this
process.
",1.0
1773,13,46248,1610.00369,social network analysis with natural language processing,"A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad","Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep
  Recurrent models","  Sentiment Analysis (SA) is an action research area in the digital age. With
rapid and constant growth of online social media sites and services, and the
increasing amount of textual data such as - statuses, comments, reviews etc.
available in them, application of automatic SA is on the rise. However, most of
the research works on SA in natural language processing (NLP) are based on
English language. Despite being the sixth most widely spoken language in the
world, Bangla still does not have a large and standard dataset. Because of
this, recent research works in Bangla have failed to produce results that can
be both comparable to works done by others and reusable as stepping stones for
future researchers to progress in this field. Therefore, we first tried to
provide a textual dataset - that includes not just Bangla, but Romanized Bangla
texts as well, is substantial, post-processed and multiple validated, ready to
be used in SA experiments. We tested this dataset in Deep Recurrent model,
specifically, Long Short Term Memory (LSTM), using two types of loss functions
- binary crossentropy and categorical crossentropy, and also did some
experimental pre-training by using data from one validation to pre-train the
other and vice versa. Lastly, we documented the results along with some
analysis on them, which were promising.
",3.0
1774,13,48402,1612.03231,social network analysis with natural language processing,"Yongjun Zhu, Erjia Yan, Il-Yeol Song","A natural language interface to a graph-based bibliographic information
  retrieval system","  With the ever-increasing scientific literature, there is a need on a natural
language interface to bibliographic information retrieval systems to retrieve
related information effectively. In this paper, we propose a natural language
interface, NLI-GIBIR, to a graph-based bibliographic information retrieval
system. In designing NLI-GIBIR, we developed a novel framework that can be
applicable to graph-based bibliographic information retrieval systems. Our
framework integrates algorithms/heuristics for interpreting and analyzing
natural language bibliographic queries. NLI-GIBIR allows users to search for a
variety of bibliographic data through natural language. A series of text- and
linguistic-based techniques are used to analyze and answer natural language
queries, including tokenization, named entity recognition, and syntactic
analysis. We find that our framework can effectively represents and addresses
complex bibliographic information needs. Thus, the contributions of this paper
are as follows: First, to our knowledge, it is the first attempt to propose a
natural language interface to graph-based bibliographic information retrieval.
Second, we propose a novel customized natural language processing framework
that integrates a few original algorithms/heuristics for interpreting and
analyzing natural language bibliographic queries. Third, we show that the
proposed framework and natural language interface provide a practical solution
in building real-world natural language interface-based bibliographic
information retrieval systems. Our experimental results show that the presented
system can correctly answer 39 out of 40 example natural language queries with
varying lengths and complexities.
",1.0
1775,13,53493,1705.06824,social network analysis with natural language processing,"Zhengyang Wang, Shuiwang Ji","Learning Convolutional Text Representations for Visual Question
  Answering","  Visual question answering is a recently proposed artificial intelligence task
that requires a deep understanding of both images and texts. In deep learning,
images are typically modeled through convolutional neural networks, and texts
are typically modeled through recurrent neural networks. While the requirement
for modeling images is similar to traditional computer vision tasks, such as
object recognition and image classification, visual question answering raises a
different need for textual representation as compared to other natural language
processing tasks. In this work, we perform a detailed analysis on natural
language questions in visual question answering. Based on the analysis, we
propose to rely on convolutional neural networks for learning textual
representations. By exploring the various properties of convolutional neural
networks specialized for text data, such as width and depth, we present our
""CNN Inception + Gate"" model. We show that our model improves question
representations and thus the overall accuracy of visual question answering
models. We also show that the text representation requirement in visual
question answering is more complicated and comprehensive than that in
conventional natural language processing tasks, making it a better task to
evaluate textual representation methods. Shallow models like fastText, which
can obtain comparable results with deep learning models in tasks like text
classification, are not suitable in visual question answering.
",1.0
1776,13,67460,1805.05144,social network analysis with natural language processing,"Firoj Alam, Ferda Ofli, Muhammad Imran, Michael Aupetit","A Twitter Tale of Three Hurricanes: Harvey, Irma, and Maria","  People increasingly use microblogging platforms such as Twitter during
natural disasters and emergencies. Research studies have revealed the
usefulness of the data available on Twitter for several disaster response
tasks. However, making sense of social media data is a challenging task due to
several reasons such as limitations of available tools to analyze high-volume
and high-velocity data streams. This work presents an extensive
multidimensional analysis of textual and multimedia content from millions of
tweets shared on Twitter during the three disaster events. Specifically, we
employ various Artificial Intelligence techniques from Natural Language
Processing and Computer Vision fields, which exploit different machine learning
algorithms to process the data generated during the disaster events. Our study
reveals the distributions of various types of useful information that can
inform crisis managers and responders as well as facilitate the development of
future automated systems for disaster management.
",4.0
1777,13,69842,1806.0873,social network analysis with natural language processing,"Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard
  Socher",The Natural Language Decathlon: Multitask Learning as Question Answering,"  Deep learning has improved performance on many natural language processing
(NLP) tasks individually. However, general NLP models cannot emerge within a
paradigm that focuses on the particularities of a single metric, dataset, and
task. We introduce the Natural Language Decathlon (decaNLP), a challenge that
spans ten tasks: question answering, machine translation, summarization,
natural language inference, sentiment analysis, semantic role labeling,
zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and
commonsense pronoun resolution. We cast all tasks as question answering over a
context. Furthermore, we present a new Multitask Question Answering Network
(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or
parameters in the multitask setting. MQAN shows improvements in transfer
learning for machine translation and named entity recognition, domain
adaptation for sentiment analysis and natural language inference, and zero-shot
capabilities for text classification. We demonstrate that the MQAN's
multi-pointer-generator decoder is key to this success and performance further
improves with an anti-curriculum training strategy. Though designed for
decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic
parsing task in the single-task setting. We also release code for procuring and
processing data, training and evaluating models, and reproducing all
experiments for decaNLP.
",1.0
1778,13,71184,1807.07752,social network analysis with natural language processing,Shaunak Joshi and Deepali Deshpande,Twitter Sentiment Analysis System,"  Social media is increasingly used by humans to express their feelings and
opinions in the form of short text messages. Detecting sentiments in the text
has a wide range of applications including identifying anxiety or depression of
individuals and measuring well-being or mood of a community. Sentiments can be
expressed in many ways that can be seen such as facial expression and gestures,
speech and by written text. Sentiment Analysis in text documents is essentially
a content-based classification problem involving concepts from the domains of
Natural Language Processing as well as Machine Learning. In this paper,
sentiment recognition based on textual data and the techniques used in
sentiment analysis are discussed.
",5.0
1816,16,30648,1412.683,activation function in neural networks,"Forest Agostinelli, Matthew Hoffman, Peter Sadowski, Pierre Baldi",Learning Activation Functions to Improve Deep Neural Networks,"  Artificial neural networks typically have a fixed, non-linear activation
function at each neuron. We have designed a novel form of piecewise linear
activation function that is learned independently for each neuron using
gradient descent. With this adaptive activation function, we are able to
improve upon deep neural network architectures composed of static rectified
linear units, achieving state-of-the-art performance on CIFAR-10 (7.51%),
CIFAR-100 (30.83%), and a benchmark from high-energy physics involving Higgs
boson decay modes.
",5.0
1817,16,36989,1510.03528,activation function in neural networks,"Yuchen Zhang, Jason D. Lee, Michael I. Jordan","$\ell_1$-regularized Neural Networks are Improperly Learnable in
  Polynomial Time","  We study the improper learning of multi-layer neural networks. Suppose that
the neural network to be learned has $k$ hidden layers and that the
$\ell_1$-norm of the incoming weights of any neuron is bounded by $L$. We
present a kernel-based method, such that with probability at least $1 -
\delta$, it learns a predictor whose generalization error is at most $\epsilon$
worse than that of the neural network. The sample complexity and the time
complexity of the presented method are polynomial in the input dimension and in
$(1/\epsilon,\log(1/\delta),F(k,L))$, where $F(k,L)$ is a function depending on
$(k,L)$ and on the activation function, independent of the number of neurons.
The algorithm applies to both sigmoid-like activation functions and ReLU-like
activation functions. It implies that any sufficiently sparse neural network is
learnable in polynomial time.
",3.0
1818,16,70356,1807.01194,activation function in neural networks,"Hans-Peter Beise, Steve Dias Da Cruz, Udo Schr\""oder",On decision regions of narrow deep neural networks,"  We show that for neural network functions that have width less or equal to
the input dimension all connected components of decision regions are unbounded.
The result holds for continuous and strictly monotonic activation functions as
well as for the ReLU activation function. This complements recent results on
approximation capabilities by [Hanin 2017 Approximating] and connectivity of
decision regions by [Nguyen 2018 Neural] for such narrow neural networks. Our
results are illustrated by means of numerical experiments.
",5.0
1819,16,76154,1810.12165,activation function in neural networks,"Luana Ruiz, Fernando Gama, Antonio G. Marques, Alejandro Ribeiro",Median activation functions for graph neural networks,"  Graph neural networks (GNNs) have been shown to replicate convolutional
neural networks' (CNNs) superior performance in many problems involving graphs.
By replacing regular convolutions with linear shift-invariant graph filters
(LSI-GFs), GNNs take into account the (irregular) structure of the graph and
provide meaningful representations of network data. However, LSI-GFs fail to
encode local nonlinear graph signal behavior, and so do regular activation
functions, which are nonlinear but pointwise. To address this issue, we propose
median activation functions with support on graph neighborhoods instead of
individual nodes. A GNN architecture with a trainable multirresolution version
of this activation function is then tested on synthetic and real-word datasets,
where we show that median activation functions can improve GNN capacity with
marginal increase in complexity.
",4.0
1820,16,78392,1812.00308,activation function in neural networks,Yongdai Kim and Dongha Kim,On variation of gradients of deep neural networks,"  We provide a theoretical explanation of the role of the number of nodes at
each layer in deep neural networks. We prove that the largest variation of a
deep neural network with ReLU activation function arises when the layer with
the fewest nodes changes its activation pattern. An important implication is
that deep neural network is a useful tool to generate functions most of whose
variations are concentrated on a smaller area of the input space near the
boundaries corresponding to the layer with the fewest nodes. In turn, this
property makes the function more invariant to input transformation. That is,
our theoretical result gives a clue about how to design the architecture of a
deep neural network to increase complexity and transformation invariancy
simultaneously.
",3.0
1821,16,87660,1905.02199,activation function in neural networks,"I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and G. Petrova",Nonlinear Approximation and (Deep) ReLU Networks,"  This article is concerned with the approximation and expressive powers of
deep neural networks. This is an active research area currently producing many
interesting papers. The results most commonly found in the literature prove
that neural networks approximate functions with classical smoothness to the
same accuracy as classical linear methods of approximation, e.g. approximation
by polynomials or by piecewise polynomials on prescribed partitions. However,
approximation by neural networks depending on n parameters is a form of
nonlinear approximation and as such should be compared with other nonlinear
methods such as variable knot splines or n-term approximation from
dictionaries. The performance of neural networks in targeted applications such
as machine learning indicate that they actually possess even greater
approximation power than these traditional methods of nonlinear approximation.
The main results of this article prove that this is indeed the case. This is
done by exhibiting large classes of functions which can be efficiently captured
by neural networks where classical nonlinear methods fall short of the task.
The present article purposefully limits itself to studying the approximation of
univariate functions by ReLU networks. Many generalizations to functions of
several variables and other activation functions can be envisioned. However,
even in this simplest of settings considered here, a theory that completely
quantifies the approximation power of neural networks is still lacking.
",2.0
1822,16,89076,1905.10259,activation function in neural networks,"Ga\""el Letarte and Pascal Germain and Benjamin Guedj and Fran\c{c}ois
  Laviolette","Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural
  Networks","  We present a comprehensive study of multilayer neural networks with binary
activation, relying on the PAC-Bayesian theory. Our contributions are twofold:
(i) we develop an end-to-end framework to train a binary activated deep neural
network, (ii) we provide nonvacuous PAC-Bayesian generalization bounds for
binary activated deep neural networks. Our results are obtained by minimizing
the expected loss of an architecture-dependent aggregation of binary activated
deep neural networks. Our analysis inherently overcomes the fact that binary
activation function is non-differentiable. The performance of our approach is
assessed on a thorough numerical experiment protocol on real-life datasets.
",4.0
1823,16,90478,1906.01975,activation function in neural networks,"Snehanshu Saha, Nithin Nagaraj, Archana Mathur, Rahul Yedida","Evolution of Novel Activation Functions in Neural Network Training with
  Applications to Classification of Exoplanets","  We present analytical exploration of novel activation functions as
consequence of integration of several ideas leading to implementation and
subsequent use in habitability classification of exoplanets. Neural networks,
although a powerful engine in supervised methods, often require expensive
tuning efforts for optimized performance. Habitability classes are hard to
discriminate, especially when attributes used as hard markers of separation are
removed from the data set. The solution is approached from the point of
investigating analytical properties of the proposed activation functions. The
theory of ordinary differential equations and fixed point are exploited to
justify the ""lack of tuning efforts"" to achieve optimal performance compared to
traditional activation functions. Additionally, the relationship between the
proposed activation functions and the more popular ones is established through
extensive analytical and empirical evidence. Finally, the activation functions
have been implemented in plain vanilla feed-forward neural network to classify
exoplanets.
",4.0
1824,16,92232,1906.10654,activation function in neural networks,"Chao Huang, Jiameng Fan, Wenchao Li, Xin Chen and Qi Zhu",ReachNN: Reachability Analysis of Neural-Network Controlled Systems,"  Applying neural networks as controllers in dynamical systems has shown great
promises. However, it is critical yet challenging to verify the safety of such
control systems with neural-network controllers in the loop. Previous methods
for verifying neural network controlled systems are limited to a few specific
activation functions. In this work, we propose a new reachability analysis
approach based on Bernstein polynomials that can verify neural-network
controlled systems with a more general form of activation functions, i.e., as
long as they ensure that the neural networks are Lipschitz continuous.
Specifically, we consider abstracting feedforward neural networks with
Bernstein polynomials for a small subset of inputs. To quantify the error
introduced by abstraction, we provide both theoretical error bound estimation
based on the theory of Bernstein polynomials and more practical sampling based
error bound estimation, following a tight Lipschitz constant estimation
approach based on forward reachability analysis. Compared with previous
methods, our approach addresses a much broader set of neural networks,
including heterogeneous neural networks that contain multiple types of
activation functions. Experiment results on a variety of benchmarks show the
effectiveness of our approach.
",3.0
1825,16,93210,1907.03742,activation function in neural networks,Stella Rose Biderman,Neural Networks on Groups,"  Although neural networks traditionally are typically used to approximate
functions defined over $\mathbb{R}^n$, the successes of graph neural networks,
point-cloud neural networks, and manifold deep learning among other methods
have demonstrated the clear value of leveraging neural networks to approximate
functions defined over more general spaces. The theory of neural networks has
not kept up however,and the relevant theoretical results (when they exist at
all) have been proven on a case-by-case basis without a general theory or
connection to classical work. The process of deriving new theoretical backing
for each new type of network has become a bottleneck to understanding and
validating new approaches.
  In this paper we extend the definition of neural networks to general
topological groups and prove that neural networks with a single hidden layer
and a bounded non-constant activation function can approximate any
$\mathcal{L}^p$ function defined over any locally compact Abelian group. This
framework and universal approximation theorem encompass all of the
aforementioned contexts. We also derive important corollaries and extensions
with minor modification, including the case for approximating continuous
functions on a compact subset, neural networks with ReLU activation functions
on a linearly bi-ordered group, and neural networks with affine transformations
on a vector space. Our work obtains as special cases the recent theorems of Qi
et al. [2017], Sennai et al. [2019], Keriven and Peyre [2019], and Maron et al.
[2019]
",2.0
1826,16,99590,1910.02333,activation function in neural networks,Rahul Parhi and Robert D. Nowak,The Role of Neural Network Activation Functions,"  A wide variety of activation functions have been proposed for neural
networks. The Rectified Linear Unit (ReLU) is especially popular today. There
are many practical reasons that motivate the use of the ReLU. This paper
provides new theoretical characterizations that support the use of the ReLU,
its variants such as the leaky ReLU, as well as other activation functions in
the case of univariate, single-hidden layer feedforward neural networks. Our
results also explain the importance of commonly used strategies in the design
and training of neural networks such as ""weight decay"" and ""path-norm""
regularization, and provide a new justification for the use of ""skip
connections"" in network architectures. These new insights are obtained through
the lens of spline theory. In particular, we show how neural network training
problems are related to infinite-dimensional optimizations posed over Banach
spaces of functions whose solutions are well-known to be fractional and
polynomial splines, where the particular Banach space (which controls the order
of the spline) depends on the choice of activation function.
",5.0
1827,16,102282,1911.01413,activation function in neural networks,"Tian Ding, Dawei Li, Ruoyu Sun","Sub-Optimal Local Minima Exist for Neural Networks with Almost All
  Non-Linear Activations","  Does over-parameterization eliminate sub-optimal local minima for neural
networks? An affirmative answer was given by a classical result in [59] for
1-hidden-layer wide neural networks. A few recent works have extended the
setting to multi-layer neural networks, but none of them has proved every local
minimum is global. Why is this result never extended to deep networks?
  In this paper, we show that the task is impossible because the original
result for 1-hidden-layer network in [59] can not hold. More specifically, we
prove that for any multi-layer network with generic input data and non-linear
activation functions, sub-optimal local minima can exist, no matter how wide
the network is (as long as the last hidden layer has at least two neurons).
While the result of [59] assumes sigmoid activation, our counter-example covers
a large set of activation functions (dense in the set of continuous functions),
indicating that the limitation is not due to the specific activation. Our
result indicates that ""no bad local-min"" may be unable to explain the benefit
of over-parameterization for training neural nets.
",2.0
1828,16,103964,1911.09576,activation function in neural networks,"Gordon MacDonald and Andrew Godbout and Bryn Gillcash and Stephanie
  Cairns",Volume-preserving Neural Networks,"  We propose a novel approach to addressing the vanishing (or exploding)
gradient problem in deep neural networks. We construct a new architecture for
deep neural networks where all layers (except the output layer) of the network
are a combination of rotation, permutation, diagonal, and activation sublayers
which are all volume preserving. Our approach replaces the standard weight
matrix of a neural network with a combination of diagonal, rotational and
permutation matrices, all of which are volume-preserving. We introduce a
coupled activation function allowing us to preserve volume even in the
activation function portion of a neural network layer. This control on the
volume forces the gradient (on average) to maintain equilibrium and not explode
or vanish. To demonstrate our architecture we apply our volume-preserving
neural network model to two standard datasets.
",2.0
1854,16,210328,2209.11395,activation function in neural networks,Yongqiang Cai,Achieve the Minimum Width of Neural Networks for Universal Approximation,"  The universal approximation property (UAP) of neural networks is fundamental
for deep learning, and it is well known that wide neural networks are universal
approximators of continuous functions within both the $L^p$ norm and the
continuous/uniform norm. However, the exact minimum width, $w_{\min}$, for the
UAP has not been studied thoroughly. Recently, using a
decoder-memorizer-encoder scheme, \citet{Park2021Minimum} found that $w_{\min}
= \max(d_x+1,d_y)$ for both the $L^p$-UAP of ReLU networks and the $C$-UAP of
ReLU+STEP networks, where $d_x,d_y$ are the input and output dimensions,
respectively. In this paper, we consider neural networks with an arbitrary set
of activation functions. We prove that both $C$-UAP and $L^p$-UAP for functions
on compact domains share a universal lower bound of the minimal width; that is,
$w^*_{\min} = \max(d_x,d_y)$. In particular, the critical width, $w^*_{\min}$,
for $L^p$-UAP can be achieved by leaky-ReLU networks, provided that the input
or output dimension is larger than one. Our construction is based on the
approximation power of neural ordinary differential equations and the ability
to approximate flow maps by neural networks. The nonmonotone or discontinuous
activation functions case and the one-dimensional case are also discussed.
",3.0
1855,17,51713,1703.08245,robustness of neural networks,"Nicholas Cheney, Martin Schrimpf, Gabriel Kreiman","On the Robustness of Convolutional Neural Networks to Internal
  Architecture and Weight Perturbations","  Deep convolutional neural networks are generally regarded as robust function
approximators. So far, this intuition is based on perturbations to external
stimuli such as the images to be classified. Here we explore the robustness of
convolutional neural networks to perturbations to the internal weights and
architecture of the network itself. We show that convolutional networks are
surprisingly robust to a number of internal perturbations in the higher
convolutional layers but the bottom convolutional layers are much more fragile.
For instance, Alexnet shows less than a 30% decrease in classification
performance when randomly removing over 70% of weight connections in the top
convolutional or dense layers but performance is almost at chance with the same
perturbation in the first convolutional layer. Finally, we suggest further
investigations which could continue to inform the robustness of convolutional
networks to internal perturbations.
",4.0
1856,17,55965,1707.08167,robustness of neural networks,"El Mahdi El Mhamdi, Rachid Guerraoui, Sebastien Rouault",On The Robustness of a Neural Network,"  With the development of neural networks based machine learning and their
usage in mission critical applications, voices are rising against the
\textit{black box} aspect of neural networks as it becomes crucial to
understand their limits and capabilities. With the rise of neuromorphic
hardware, it is even more critical to understand how a neural network, as a
distributed system, tolerates the failures of its computing nodes, neurons, and
its communication channels, synapses. Experimentally assessing the robustness
of neural networks involves the quixotic venture of testing all the possible
failures, on all the possible inputs, which ultimately hits a combinatorial
explosion for the first, and the impossibility to gather all the possible
inputs for the second.
  In this paper, we prove an upper bound on the expected error of the output
when a subset of neurons crashes. This bound involves dependencies on the
network parameters that can be seen as being too pessimistic in the average
case. It involves a polynomial dependency on the Lipschitz coefficient of the
neurons activation function, and an exponential dependency on the depth of the
layer where a failure occurs. We back up our theoretical results with
experiments illustrating the extent to which our prediction matches the
dependencies between the network parameters and robustness. Our results show
that the robustness of neural networks to the average crash can be estimated
without the need to neither test the network on all failure configurations, nor
access the training set used to train the network, both of which are
practically impossible requirements.
",3.0
1857,17,57380,1709.02802,robustness of neural networks,"Guy Katz (Stanford University), Clark Barrett (Stanford University),
  David L. Dill (Stanford University), Kyle Julian (Stanford University), Mykel
  J. Kochenderfer (Stanford University)",Towards Proving the Adversarial Robustness of Deep Neural Networks,"  Autonomous vehicles are highly complex systems, required to function reliably
in a wide variety of situations. Manually crafting software controllers for
these vehicles is difficult, but there has been some success in using deep
neural networks generated using machine-learning. However, deep neural networks
are opaque to human engineers, rendering their correctness very difficult to
prove manually; and existing automated techniques, which were not designed to
operate on neural networks, fail to scale to large systems. This paper focuses
on proving the adversarial robustness of deep neural networks, i.e. proving
that small perturbations to a correctly-classified input to the network cannot
cause it to be misclassified. We describe some of our recent and ongoing work
on verifying the adversarial robustness of networks, and discuss some of the
open questions we have encountered and how they might be addressed.
",5.0
1858,17,62798,1801.10578,robustness of neural networks,"Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng
  Gao, Cho-Jui Hsieh, Luca Daniel","Evaluating the Robustness of Neural Networks: An Extreme Value Theory
  Approach","  The robustness of neural networks to adversarial examples has received great
attention due to security implications. Despite various attack approaches to
crafting visually imperceptible adversarial examples, little has been developed
towards a comprehensive measure of robustness. In this paper, we provide a
theoretical justification for converting robustness analysis into a local
Lipschitz constant estimation problem, and propose to use the Extreme Value
Theory for efficient evaluation. Our analysis yields a novel robustness metric
called CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork
Robustness. The proposed CLEVER score is attack-agnostic and computationally
feasible for large neural networks. Experimental results on various networks,
including ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned
with the robustness indication measured by the $\ell_2$ and $\ell_\infty$ norms
of adversarial examples from powerful attacks, and (ii) defended networks using
defensive distillation or bounded ReLU indeed achieve better CLEVER scores. To
the best of our knowledge, CLEVER is the first attack-independent robustness
metric that can be applied to any neural network classifier.
",4.0
1859,17,70239,1807.0034,robustness of neural networks,"Xinhan Di, Pengqian Yu, Meng Tian","Towards Adversarial Training with Moderate Performance Improvement for
  Neural Network Classification","  It has been demonstrated that deep neural networks are prone to noisy
examples particular adversarial samples during inference process. The gap
between robust deep learning systems in real world applications and vulnerable
neural networks is still large. Current adversarial training strategies improve
the robustness against adversarial samples. However, these methods lead to
accuracy reduction when the input examples are clean thus hinders the
practicability. In this paper, we investigate an approach that protects the
neural network classification from the adversarial samples and improves its
accuracy when the input examples are clean. We demonstrate the versatility and
effectiveness of our proposed approach on a variety of different networks and
datasets.
",4.0
1860,17,71934,1808.02433,robustness of neural networks,Francois Fagan and Garud Iyengar,Robust Implicit Backpropagation,"  Arguably the biggest challenge in applying neural networks is tuning the
hyperparameters, in particular the learning rate. The sensitivity to the
learning rate is due to the reliance on backpropagation to train the network.
In this paper we present the first application of Implicit Stochastic Gradient
Descent (ISGD) to train neural networks, a method known in convex optimization
to be unconditionally stable and robust to the learning rate. Our key
contribution is a novel layer-wise approximation of ISGD which makes its
updates tractable for neural networks. Experiments show that our method is more
robust to high learning rates and generally outperforms standard
backpropagation on a variety of tasks.
",3.0
1861,17,73032,1809.01129,robustness of neural networks,"Zac Cranko, Simon Kornblith, Zhan Shi, Richard Nock",Lipschitz Networks and Distributional Robustness,"  Robust risk minimisation has several advantages: it has been studied with
regards to improving the generalisation properties of models and robustness to
adversarial perturbation. We bound the distributionally robust risk for a model
class rich enough to include deep neural networks by a regularised empirical
risk involving the Lipschitz constant of the model. This allows us to
interpretand quantify the robustness properties of a deep neural network. As an
application we show the distributionally robust risk upperbounds the
adversarial training risk.
",4.0
1862,17,74457,1810.00144,robustness of neural networks,"Fuxun Yu, Chenchen Liu, Yanzhi Wang, Liang Zhao, Xiang Chen","Interpreting Adversarial Robustness: A View from Decision Surface in
  Input Space","  One popular hypothesis of neural network generalization is that the flat
local minima of loss surface in parameter space leads to good generalization.
However, we demonstrate that loss surface in parameter space has no obvious
relationship with generalization, especially under adversarial settings.
Through visualizing decision surfaces in both parameter space and input space,
we instead show that the geometry property of decision surface in input space
correlates well with the adversarial robustness. We then propose an adversarial
robustness indicator, which can evaluate a neural network's intrinsic
robustness property without testing its accuracy under adversarial attacks.
Guided by it, we further propose our robust training method. Without involving
adversarial training, our method could enhance network's intrinsic adversarial
robustness against various adversarial attacks.
",4.0
1863,17,75597,1810.0864,robustness of neural networks,"Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Aurelie Lozano, Cho-Jui Hsieh,
  Luca Daniel","On Extensions of CLEVER: A Neural Network Robustness Evaluation
  Algorithm","  CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme
Value Theory (EVT) based robustness score for large-scale deep neural networks
(DNNs). In this paper, we propose two extensions on this robustness score.
First, we provide a new formal robustness guarantee for classifier functions
that are twice differentiable. We apply extreme value theory on the new formal
robustness guarantee and the estimated robustness is called second-order CLEVER
score. Second, we discuss how to handle gradient masking, a common defensive
technique, using CLEVER with Backward Pass Differentiable Approximation (BPDA).
With BPDA applied, CLEVER can evaluate the intrinsic robustness of neural
networks of a broader class -- networks with non-differentiable input
transformations. We demonstrate the effectiveness of CLEVER with BPDA in
experiments on a 121-layer Densenet model trained on the ImageNet dataset.
",4.0
1864,17,76084,1810.11783,robustness of neural networks,"Huan Zhang, Pengchuan Zhang, Cho-Jui Hsieh","RecurJac: An Efficient Recursive Algorithm for Bounding Jacobian Matrix
  of Neural Networks and Its Applications","  The Jacobian matrix (or the gradient for single-output networks) is directly
related to many important properties of neural networks, such as the function
landscape, stationary points, (local) Lipschitz constants and robustness to
adversarial attacks. In this paper, we propose a recursive algorithm, RecurJac,
to compute both upper and lower bounds for each element in the Jacobian matrix
of a neural network with respect to network's input, and the network can
contain a wide range of activation functions. As a byproduct, we can
efficiently obtain a (local) Lipschitz constant, which plays a crucial role in
neural network robustness verification, as well as the training stability of
GANs. Experiments show that (local) Lipschitz constants produced by our method
is of better quality than previous approaches, thus providing better robustness
verification results. Our algorithm has polynomial time complexity, and its
computation time is reasonable even for relatively large networks.
Additionally, we use our bounds of Jacobian matrix to characterize the
landscape of the neural network, for example, to determine whether there exist
stationary points in a local neighborhood. Source code available at
\url{http://github.com/huanzhang12/RecurJac-Jacobian-bounds}.
",2.0
1865,17,78105,1811.11373,robustness of neural networks,Panagiotis Kouvaros and Alessio Lomuscio,Formal Verification of CNN-based Perception Systems,"  We address the problem of verifying neural-based perception systems
implemented by convolutional neural networks. We define a notion of local
robustness based on affine and photometric transformations. We show the notion
cannot be captured by previously employed notions of robustness. The method
proposed is based on reachability analysis for feed-forward neural networks and
relies on MILP encodings of both the CNNs and transformations under question.
We present an implementation and discuss the experimental results obtained for
a CNN trained from the MNIST data set.
",3.0
1866,17,81569,1901.10371,robustness of neural networks,"Peter Langenberg, Emilio Rafael Balda, Arash Behboodi, Rudolf Mathar","On the Effect of Low-Rank Weights on Adversarial Robustness of Neural
  Networks","  Recently, there has been an abundance of works on designing Deep Neural
Networks (DNNs) that are robust to adversarial examples. In particular, a
central question is which features of DNNs influence adversarial robustness
and, therefore, can be to used to design robust DNNs. In this work, this
problem is studied through the lens of compression which is captured by the
low-rank structure of weight matrices. It is first shown that adversarial
training tends to promote simultaneously low-rank and sparse structure in the
weight matrices of neural networks. This is measured through the notions of
effective rank and effective sparsity. In the reverse direction, when the low
rank structure is promoted by nuclear norm regularization and combined with
sparsity inducing regularizations, neural networks show significantly improved
adversarial robustness. The effect of nuclear norm regularization on
adversarial robustness is paramount when it is applied to convolutional neural
networks. Although still not competing with adversarial training, this result
contributes to understanding the key properties of robust classifiers.
",4.0
1867,17,81864,1902.00577,robustness of neural networks,Sascha Saralajew and Lars Holdijk and Maike Rees and Thomas Villmann,"Robustness of Generalized Learning Vector Quantization Models against
  Adversarial Attacks","  Adversarial attacks and the development of (deep) neural networks robust
against them are currently two widely researched topics. The robustness of
Learning Vector Quantization (LVQ) models against adversarial attacks has
however not yet been studied to the same extent. We therefore present an
extensive evaluation of three LVQ models: Generalized LVQ, Generalized Matrix
LVQ and Generalized Tangent LVQ. The evaluation suggests that both Generalized
LVQ and Generalized Tangent LVQ have a high base robustness, on par with the
current state-of-the-art in robust neural network methods. In contrast to this,
Generalized Matrix LVQ shows a high susceptibility to adversarial attacks,
scoring consistently behind all other models. Additionally, our numerical
evaluation indicates that increasing the number of prototypes per class
improves the robustness of the models.
",5.0
1868,17,84856,1903.10033,robustness of neural networks,"Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli,
  Sanjit A. Seshia",A Formalization of Robustness for Deep Neural Networks,"  Deep neural networks have been shown to lack robustness to small input
perturbations. The process of generating the perturbations that expose the lack
of robustness of neural networks is known as adversarial input generation. This
process depends on the goals and capabilities of the adversary, In this paper,
we propose a unifying formalization of the adversarial input generation process
from a formal methods perspective. We provide a definition of robustness that
is general enough to capture different formulations. The expressiveness of our
formalization is shown by modeling and comparing a variety of adversarial
attack techniques.
",3.0
1869,17,84925,1903.10672,robustness of neural networks,"Abhishek Murthy, Himel Das, Md Ariful Islam",Robustness of Neural Networks to Parameter Quantization,"  Quantization, a commonly used technique to reduce the memory footprint of a
neural network for edge computing, entails reducing the precision of the
floating-point representation used for the parameters of the network. The
impact of such rounding-off errors on the overall performance of the neural
network is estimated using testing, which is not exhaustive and thus cannot be
used to guarantee the safety of the model. We present a framework based on
Satisfiability Modulo Theory (SMT) solvers to quantify the robustness of neural
networks to parameter perturbation. To this end, we introduce notions of local
and global robustness that capture the deviation in the confidence of class
assignments due to parameter quantization. The robustness notions are then cast
as instances of SMT problems and solved automatically using solvers, such as
dReal. We demonstrate our framework on two simple Multi-Layer Perceptrons (MLP)
that perform binary classification on a two-dimensional input. In addition to
quantifying the robustness, we also show that Rectified Linear Unit activation
results in higher robustness than linear activations for our MLPs.
",4.0
1870,17,87003,1904.11803,robustness of neural networks,Francesco Ranzato and Marco Zanella,Robustness Verification of Support Vector Machines,"  We study the problem of formally verifying the robustness to adversarial
examples of support vector machines (SVMs), a major machine learning model for
classification and regression tasks. Following a recent stream of works on
formal robustness verification of (deep) neural networks, our approach relies
on a sound abstract version of a given SVM classifier to be used for checking
its robustness. This methodology is parametric on a given numerical abstraction
of real values and, analogously to the case of neural networks, needs neither
abstract least upper bounds nor widening operators on this abstraction. The
standard interval domain provides a simple instantiation of our abstraction
technique, which is enhanced with the domain of reduced affine forms, which is
an efficient abstraction of the zonotope abstract domain. This robustness
verification technique has been fully implemented and experimentally evaluated
on SVMs based on linear and nonlinear (polynomial and radial basis function)
kernels, which have been trained on the popular MNIST dataset of images and on
the recent and more challenging Fashion-MNIST dataset. The experimental results
of our prototype SVM robustness verifier appear to be encouraging: this
automated verification is fast, scalable and shows significantly high
percentages of provable robustness on the test set of MNIST, in particular
compared to the analogous provable robustness of neural networks.
",1.0
1871,17,87986,1905.0427,robustness of neural networks,"Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang
  Chen",Interpreting and Evaluating Neural Network Robustness,"  Recently, adversarial deception becomes one of the most considerable threats
to deep neural networks. However, compared to extensive research in new designs
of various adversarial attacks and defenses, the neural networks' intrinsic
robustness property is still lack of thorough investigation. This work aims to
qualitatively interpret the adversarial attack and defense mechanism through
loss visualization, and establish a quantitative metric to evaluate the neural
network model's intrinsic robustness. The proposed robustness metric identifies
the upper bound of a model's prediction divergence in the given domain and thus
indicates whether the model can maintain a stable prediction. With extensive
experiments, our metric demonstrates several advantages over conventional
adversarial testing accuracy based robustness estimation: (1) it provides a
uniformed evaluation to models with different structures and parameter scales;
(2) it over-performs conventional accuracy based robustness estimation and
provides a more reliable evaluation that is invariant to different test
settings; (3) it can be fast generated without considerable testing cost.
",4.0
1878,17,100102,1910.05018,robustness of neural networks,"Nathana\""el Fijalkow, Mohit Kumar Gupta","Verification of Neural Networks: Specifying Global Robustness using
  Generative Models","  The success of neural networks across most machine learning tasks and the
persistence of adversarial examples have made the verification of such models
an important quest. Several techniques have been successfully developed to
verify robustness, and are now able to evaluate neural networks with thousands
of nodes. The main weakness of this approach is in the specification:
robustness is asserted on a validation set consisting of a finite set of
examples, i.e. locally.
  We propose a notion of global robustness based on generative models, which
asserts the robustness on a very large and representative set of examples. We
show how this can be used for verifying neural networks. In this paper we
experimentally explore the merits of this approach, and show how it can be used
to construct realistic adversarial examples.
",5.0
1936,17,196514,2205.10122,robustness of neural networks,"Egor Manuylovich, Diego Arg\""uello Ron, Morteza Kamalian-Kopae, Sergei
  Turitsyn",Stochastic resonance neurons in artificial neural networks,"  Many modern applications of the artificial neural networks ensue large number
of layers making traditional digital implementations increasingly complex.
Optical neural networks offer parallel processing at high bandwidth, but have
the challenge of noise accumulation. We propose here a new type of neural
networks using stochastic resonances as an inherent part of the architecture
and demonstrate a possibility of significant reduction of the required number
of neurons for a given performance accuracy. We also show that such a neural
network is more robust against the impact of noise.
",3.0
1937,17,197552,2205.13863,robustness of neural networks,"Binghui Li, Jikai Jin, Han Zhong, John E. Hopcroft, Liwei Wang","Why Robust Generalization in Deep Learning is Difficult: Perspective of
  Expressive Power","  It is well-known that modern neural networks are vulnerable to adversarial
examples. To mitigate this problem, a series of robust learning algorithms have
been proposed. However, although the robust training error can be near zero via
some methods, all existing algorithms lead to a high robust generalization
error. In this paper, we provide a theoretical understanding of this puzzling
phenomenon from the perspective of expressive power for deep neural networks.
Specifically, for binary classification problems with well-separated data, we
show that, for ReLU networks, while mild over-parameterization is sufficient
for high robust training accuracy, there exists a constant robust
generalization gap unless the size of the neural network is exponential in the
data dimension $d$. This result holds even if the data is linear separable
(which means achieving standard generalization is easy), and more generally for
any parameterized function classes as long as their VC dimension is at most
polynomial in the number of parameters. Moreover, we establish an improved
upper bound of $\exp({\mathcal{O}}(k))$ for the network size to achieve low
robust generalization error when the data lies on a manifold with intrinsic
dimension $k$ ($k \ll d$). Nonetheless, we also have a lower bound that grows
exponentially with respect to $k$ -- the curse of dimensionality is inevitable.
By demonstrating an exponential separation between the network size for
achieving low robust training and generalization error, our results reveal that
the hardness of robust generalization may stem from the expressive power of
practical models.
",5.0
1938,17,198287,2206.00513,robustness of neural networks,"Thulasi Tholeti, Sheetal Kalyani",The robust way to stack and bag: the local Lipschitz way,"  Recent research has established that the local Lipschitz constant of a neural
network directly influences its adversarial robustness. We exploit this
relationship to construct an ensemble of neural networks which not only
improves the accuracy, but also provides increased adversarial robustness. The
local Lipschitz constants for two different ensemble methods - bagging and
stacking - are derived and the architectures best suited for ensuring
adversarial robustness are deduced. The proposed ensemble architectures are
tested on MNIST and CIFAR-10 datasets in the presence of white-box attacks,
FGSM and PGD. The proposed architecture is found to be more robust than a) a
single network and b) traditional ensemble methods.
",4.0
1939,17,200052,2206.07311,robustness of neural networks,"Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang",Can pruning improve certified robustness of neural networks?,"  With the rapid development of deep learning, the sizes of neural networks
become larger and larger so that the training and inference often overwhelm the
hardware resources. Given the fact that neural networks are often
over-parameterized, one effective way to reduce such computational overhead is
neural network pruning, by removing redundant parameters from trained neural
networks. It has been recently observed that pruning can not only reduce
computational overhead but also can improve empirical robustness of deep neural
networks (NNs), potentially owing to removing spurious correlations while
preserving the predictive accuracies. This paper for the first time
demonstrates that pruning can generally improve certified robustness for
ReLU-based NNs under the complete verification setting. Using the popular
Branch-and-Bound (BaB) framework, we find that pruning can enhance the
estimated bound tightness of certified robustness verification, by alleviating
linear relaxation and sub-domain split problems. We empirically verify our
findings with off-the-shelf pruning methods and further present a new
stability-based pruning method tailored for reducing neuron instability, that
outperforms existing pruning methods in enhancing certified robustness. Our
experiments show that by appropriately pruning an NN, its certified accuracy
can be boosted up to 8.2% under standard training, and up to 24.5% under
adversarial training on the CIFAR10 dataset. We additionally observe the
existence of certified lottery tickets that can match both standard and
certified robust accuracies of the original dense models across different
datasets. Our findings offer a new angle to study the intriguing interaction
between sparsity and robustness, i.e. interpreting the interaction of sparsity
and certified robustness via neuron stability. Codes are available at:
https://github.com/VITA-Group/CertifiedPruning.
",4.0
1940,17,200754,2206.09868,robustness of neural networks,"Christian Cianfarani, Arjun Nitin Bhagoji, Vikash Sehwag, Ben Y. Zhao,
  Prateek Mittal, Haitao Zheng","Understanding Robust Learning through the Lens of Representation
  Similarities","  Representation learning, i.e. the generation of representations useful for
downstream applications, is a task of fundamental importance that underlies
much of the success of deep neural networks (DNNs). Recently, robustness to
adversarial examples has emerged as a desirable property for DNNs, spurring the
development of robust training methods that account for adversarial examples.
In this paper, we aim to understand how the properties of representations
learned by robust training differ from those obtained from standard, non-robust
training. This is critical to diagnosing numerous salient pitfalls in robust
networks, such as, degradation of performance on benign inputs, poor
generalization of robustness, and increase in over-fitting. We utilize a
powerful set of tools known as representation similarity metrics, across three
vision datasets, to obtain layer-wise comparisons between robust and non-robust
DNNs with different training procedures, architectural parameters and
adversarial constraints. Our experiments highlight hitherto unseen properties
of robust representations that we posit underlie the behavioral differences of
robust networks. We discover a lack of specialization in robust networks'
representations along with a disappearance of `block structure'. We also find
overfitting during robust training largely impacts deeper layers. These, along
with other findings, suggest ways forward for the design and training of better
robust networks.
",3.0
1941,17,201310,2206.12227,robustness of neural networks,"Mark Huasong Meng, Guangdong Bai, Sin Gee Teo, Zhe Hou, Yan Xiao, Yun
  Lin, Jin Song Dong","Adversarial Robustness of Deep Neural Networks: A Survey from a Formal
  Verification Perspective","  Neural networks have been widely applied in security applications such as
spam and phishing detection, intrusion prevention, and malware detection. This
black-box method, however, often has uncertainty and poor explainability in
applications. Furthermore, neural networks themselves are often vulnerable to
adversarial attacks. For those reasons, there is a high demand for trustworthy
and rigorous methods to verify the robustness of neural network models.
Adversarial robustness, which concerns the reliability of a neural network when
dealing with maliciously manipulated inputs, is one of the hottest topics in
security and machine learning. In this work, we survey existing literature in
adversarial robustness verification for neural networks and collect 39
diversified research works across machine learning, security, and software
engineering domains. We systematically analyze their approaches, including how
robustness is formulated, what verification techniques are used, and the
strengths and limitations of each technique. We provide a taxonomy from a
formal verification perspective for a comprehensive understanding of this
topic. We classify the existing techniques based on property specification,
problem reduction, and reasoning strategies. We also demonstrate representative
techniques that have been applied in existing studies with a sample model.
Finally, we discuss open questions for future research.
",5.0
1942,17,202997,2207.04227,robustness of neural networks,"Morgane Ayle, Bertrand Charpentier, John Rachwan, Daniel Z\""ugner,
  Simon Geisler, Stephan G\""unnemann",On the Robustness and Anomaly Detection of Sparse Neural Networks,"  The robustness and anomaly detection capability of neural networks are
crucial topics for their safe adoption in the real-world. Moreover, the
over-parameterization of recent networks comes with high computational costs
and raises questions about its influence on robustness and anomaly detection.
In this work, we show that sparsity can make networks more robust and better
anomaly detectors. To motivate this even further, we show that a pre-trained
neural network contains, within its parameter space, sparse subnetworks that
are better at these tasks without any further training. We also show that
structured sparsity greatly helps in reducing the complexity of expensive
robustness and detection methods, while maintaining or even improving their
results on these tasks. Finally, we introduce a new method, SensNorm, which
uses the sensitivity of weights derived from an appropriate pruning method to
detect anomalous samples in the input.
",3.0
1943,17,204562,2207.11727,robustness of neural networks,"Nikolaos Tsilivis, Jingtong Su, Julia Kempe",Can we achieve robustness from data alone?,"  Adversarial training and its variants have come to be the prevailing methods
to achieve adversarially robust classification using neural networks. However,
its increased computational cost together with the significant gap between
standard and robust performance hinder progress and beg the question of whether
we can do better. In this work, we take a step back and ask: Can models achieve
robustness via standard training on a suitably optimized set? To this end, we
devise a meta-learning method for robust classification, that optimizes the
dataset prior to its deployment in a principled way, and aims to effectively
remove the non-robust parts of the data. We cast our optimization method as a
multi-step PGD procedure on kernel regression, with a class of kernels that
describe infinitely wide neural nets (Neural Tangent Kernels - NTKs).
Experiments on MNIST and CIFAR-10 demonstrate that the datasets we produce
enjoy very high robustness against PGD attacks, when deployed in both kernel
regression classifiers and neural networks. However, this robustness is
somewhat fallacious, as alternative attacks manage to fool the models, which we
find to be the case for previous similar works in the literature as well. We
discuss potential reasons for this and outline further avenues of research.
",3.0
1944,17,205857,2208.03889,robustness of neural networks,"Saber Jafarpour and Alexander Davydov and Matthew Abate and Francesco
  Bullo and Samuel Coogan","Robust Training and Verification of Implicit Neural Networks: A
  Non-Euclidean Contractive Approach","  This paper proposes a theoretical and computational framework for training
and robustness verification of implicit neural networks based upon
non-Euclidean contraction theory. The basic idea is to cast the robustness
analysis of a neural network as a reachability problem and use (i) the
$\ell_{\infty}$-norm input-output Lipschitz constant and (ii) the tight
inclusion function of the network to over-approximate its reachable sets.
First, for a given implicit neural network, we use $\ell_{\infty}$-matrix
measures to propose sufficient conditions for its well-posedness, design an
iterative algorithm to compute its fixed points, and provide upper bounds for
its $\ell_\infty$-norm input-output Lipschitz constant. Second, we introduce a
related embedded network and show that the embedded network can be used to
provide an $\ell_\infty$-norm box over-approximation of the reachable sets of
the original network. Moreover, we use the embedded network to design an
iterative algorithm for computing the upper bounds of the original system's
tight inclusion function. Third, we use the upper bounds of the Lipschitz
constants and the upper bounds of the tight inclusion functions to design two
algorithms for the training and robustness verification of implicit neural
networks. Finally, we apply our algorithms to train implicit neural networks on
the MNIST dataset and compare the robustness of our models with the models
trained via existing approaches in the literature.
",3.0
1945,17,209560,2209.07754,robustness of neural networks,"Yang Song, Qiyu Kang, Sijie Wang, Zhao Kai, Wee Peng Tay",On the Robustness of Graph Neural Diffusion to Topology Perturbations,"  Neural diffusion on graphs is a novel class of graph neural networks that has
attracted increasing attention recently. The capability of graph neural partial
differential equations (PDEs) in addressing common hurdles of graph neural
networks (GNNs), such as the problems of over-smoothing and bottlenecks, has
been investigated but not their robustness to adversarial attacks. In this
work, we explore the robustness properties of graph neural PDEs. We empirically
demonstrate that graph neural PDEs are intrinsically more robust against
topology perturbation as compared to other GNNs. We provide insights into this
phenomenon by exploiting the stability of the heat semigroup under graph
topology perturbations. We discuss various graph diffusion operators and relate
them to existing graph neural PDEs. Furthermore, we propose a general graph
neural PDE framework based on which a new class of robust GNNs can be defined.
We verify that the new model achieves comparable state-of-the-art performance
on several benchmark datasets.
",5.0
1946,17,210036,2209.09996,robustness of neural networks,"Jiaqi Xue, Lei Xu, Lin Chen, Weidong Shi, Kaidi Xu, Qian Lou","Audit and Improve Robustness of Private Neural Networks on Encrypted
  Data","  Performing neural network inference on encrypted data without decryption is
one popular method to enable privacy-preserving neural networks (PNet) as a
service. Compared with regular neural networks deployed for
machine-learning-as-a-service, PNet requires additional encoding, e.g.,
quantized-precision numbers, and polynomial activation. Encrypted input also
introduces novel challenges such as adversarial robustness and security. To the
best of our knowledge, we are the first to study questions including (i)
Whether PNet is more robust against adversarial inputs than regular neural
networks? (ii) How to design a robust PNet given the encrypted input without
decryption? We propose PNet-Attack to generate black-box adversarial examples
that can successfully attack PNet in both target and untarget manners. The
attack results show that PNet robustness against adversarial inputs needs to be
improved. This is not a trivial task because the PNet model owner does not have
access to the plaintext of the input values, which prevents the application of
existing detection and defense methods such as input tuning, model
normalization, and adversarial training. To tackle this challenge, we propose a
new fast and accurate noise insertion method, called RPNet, to design Robust
and Private Neural Networks. Our comprehensive experiments show that
PNet-Attack reduces at least $2.5\times$ queries than prior works. We
theoretically analyze our RPNet methods and demonstrate that RPNet can decrease
$\sim 91.88\%$ attack success rate.
",4.0
1947,17,212698,2210.05577,robustness of neural networks,"Nikolaos Tsilivis, Julia Kempe",What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?,"  The adversarial vulnerability of neural nets, and subsequent techniques to
create robust models have attracted significant attention; yet we still lack a
full understanding of this phenomenon. Here, we study adversarial examples of
trained neural networks through analytical tools afforded by recent theory
advances connecting neural networks and kernel methods, namely the Neural
Tangent Kernel (NTK), following a growing body of work that leverages the NTK
approximation to successfully analyze important deep learning phenomena and
design algorithms for new applications. We show how NTKs allow to generate
adversarial examples in a ``training-free'' fashion, and demonstrate that they
transfer to fool their finite-width neural net counterparts in the ``lazy''
regime. We leverage this connection to provide an alternative view on robust
and non-robust features, which have been suggested to underlie the adversarial
brittleness of neural nets. Specifically, we define and study features induced
by the eigendecomposition of the kernel to better understand the role of robust
and non-robust features, the reliance on both for standard classification and
the robustness-accuracy trade-off. We find that such features are surprisingly
consistent across architectures, and that robust features tend to correspond to
the largest eigenvalues of the model, and thus are learned early during
training. Our framework allows us to identify and visualize non-robust yet
useful features. Finally, we shed light on the robustness mechanism underlying
adversarial training of neural nets used in practice: quantifying the evolution
of the associated empirical NTK, we demonstrate that its dynamics falls much
earlier into the ``lazy'' regime and manifests a much stronger form of the well
known bias to prioritize learning features within the top eigenspaces of the
kernel, compared to standard training.
",5.0
1948,18,7182,1012.5208,information retrieval time complexity,"Nadia Baaziz, Omar Abahmane and Rokia Missaoui","Texture feature extraction in the spatial-frequency domain for
  content-based image retrieval","  The advent of large scale multimedia databases has led to great challenges in
content-based image retrieval (CBIR). Even though CBIR is considered an
emerging field of research, however it constitutes a strong background for new
methodologies and systems implementations. Therefore, many research
contributions are focusing on techniques enabling higher image retrieval
accuracy while preserving low level of computational complexity. Image
retrieval based on texture features is receiving special attention because of
the omnipresence of this visual feature in most real-world images. This paper
highlights the state-of-the-art and current progress relevant to texture-based
image retrieval and spatial-frequency image representations. In particular, it
gives an overview of statistical methodologies and techniques employed for
texture feature extraction using most popular spatial-frequency image
transforms, namely discrete wavelets, Gabor wavelets, dual-tree complex wavelet
and contourlets. Indications are also given about used similarity measurement
functions and most important achieved results.
",2.0
1949,18,28575,1409.0749,information retrieval time complexity,Vikas Verma,Image Retrieval And Classification Using Local Feature Vectors,"  Content Based Image Retrieval(CBIR) is one of the important subfield in the
field of Information Retrieval. The goal of a CBIR algorithm is to retrieve
semantically similar images in response to a query image submitted by the end
user. CBIR is a hard problem because of the phenomenon known as $\textit
{semantic gap}$.
  In this thesis, we aim at analyzing the performance of a CBIR system build
using local feature vectors and Intermediate Matching Kernel. We also propose a
Two-Step Matching process for reducing the response time of the CBIR systems.
Further, we develop a Meta-Learning framework for improving the retrieval
performance of these systems. Our results show that the Two-Step Matching
process significantly reduces response time and the Meta-Learning Framework
improves the retrieval performance by more than two fold. We also analyze the
performance of various image classification systems that use different image
representations constructed from the local feature vectors.
",3.0
1950,18,45997,1609.07027,information retrieval time complexity,"Simon R. Blackburn, Tuvi Etzion, Maura B. Paterson",PIR schemes with small download complexity and low storage requirements,"  In the classical model for (information theoretically secure) Private
Information Retrieval (PIR), a user wishes to retrieve one bit of a database
that is stored on a set of $n$ servers, in such a way that no individual server
gains information about which bit the user is interested in. The aim is to
design schemes that minimise communication between the user and the servers.
More recently, there have been moves to consider more realistic models where
the total storage of the set of servers, or the per server storage, should be
minimised (possibly using techniques from distributed storage), and where the
database is divided into $R$-bit records with $R>1$, and the user wishes to
retrieve one record rather than one bit. When $R$ is large, downloads from the
servers to the user dominate the communication complexity and so the aim is to
minimise the total number of downloaded bits. Shah, Rashmi and Ramchandran show
that at least $R+1$ bits must be downloaded from servers in the worst case, and
provide PIR schemes meeting this bound. Sun and Jafar determine the best
asymptotic download cost of a PIR scheme (as $R\rightarrow\infty$), where this
cost is defined as the ratio of the message length $R$ and the total number of
bits downloaded.
  This paper provides various bounds on the download complexity of a PIR
scheme, generalising those of Shah et al. to the case when the number $n$ of
servers is bounded, and providing links with classical techniques due to Chor
et al. The paper also provides a range of constructions for PIR schemes that
are either simpler or perform better than previously known schemes, including
explicit schemes that achieve the best asymptotic download complexity of Sun
and Jafar with significantly lower upload complexity, and general techniques
for constructing a scheme with good worst case download complexity from a
scheme with good download complexity on average.
",3.0
1951,18,116834,2004.12303,information retrieval time complexity,"Xinyue Zheng, Peng Wang, Qigang Wang, Zhongchao Shi","Challenge Closed-book Science Exam: A Meta-learning Based Question
  Answering System","  Prior work in standardized science exams requires support from large text
corpus, such as targeted science corpus fromWikipedia or SimpleWikipedia.
However, retrieving knowledge from the large corpus is time-consuming and
questions embedded in complex semantic representation may interfere with
retrieval. Inspired by the dual process theory in cognitive science, we propose
a MetaQA framework, where system 1 is an intuitive meta-classifier and system 2
is a reasoning module. Specifically, our method based on meta-learning method
and large language model BERT, which can efficiently solve science problems by
learning from related example questions without relying on external knowledge
bases. We evaluate our method on AI2 Reasoning Challenge (ARC), and the
experimental results show that meta-classifier yields considerable
classification performance on emerging question types. The information provided
by meta-classifier significantly improves the accuracy of reasoning module from
46.6% to 64.2%, which has a competitive advantage over retrieval-based QA
methods.
",0.0
